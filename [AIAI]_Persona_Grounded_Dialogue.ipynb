{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 준비 사항"
      ],
      "metadata": {
        "id": "a7xnPx5hF97A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "I1uHdeJTtPKs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ef6d4a7-1877-4518-d014-f04ad56e181b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jan 12 05:25:08 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q parlai==1.6.0"
      ],
      "metadata": {
        "id": "i7uiTt8f17k6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q subword_nmt # extra requirement we need for this tutorial"
      ],
      "metadata": {
        "id": "unrVp4KHGLoj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/NVIDIA/apex\n",
        "%cd apex\n",
        "!python setup.py install\n",
        "%cd /content/"
      ],
      "metadata": {
        "id": "azb86MKlT8Jo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5c7e7e0-d9b2-40fe-e0c1-dc63f945e346"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'apex' already exists and is not an empty directory.\n",
            "/content/apex\n",
            "\n",
            "\n",
            "torch.__version__  = 1.13.1+cu116\n",
            "\n",
            "\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing apex.egg-info/PKG-INFO\n",
            "writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "writing requirements to apex.egg-info/requires.txt\n",
            "writing top-level names to apex.egg-info/top_level.txt\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/apex\n",
            "creating build/bdist.linux-x86_64/egg/apex/transformer\n",
            "creating build/bdist.linux-x86_64/egg/apex/transformer/testing\n",
            "copying build/lib/apex/transformer/testing/arguments.py -> build/bdist.linux-x86_64/egg/apex/transformer/testing\n",
            "copying build/lib/apex/transformer/testing/global_vars.py -> build/bdist.linux-x86_64/egg/apex/transformer/testing\n",
            "copying build/lib/apex/transformer/testing/commons.py -> build/bdist.linux-x86_64/egg/apex/transformer/testing\n",
            "copying build/lib/apex/transformer/testing/standalone_bert.py -> build/bdist.linux-x86_64/egg/apex/transformer/testing\n",
            "copying build/lib/apex/transformer/testing/distributed_test_base.py -> build/bdist.linux-x86_64/egg/apex/transformer/testing\n",
            "copying build/lib/apex/transformer/testing/standalone_gpt.py -> build/bdist.linux-x86_64/egg/apex/transformer/testing\n",
            "copying build/lib/apex/transformer/testing/standalone_transformer_lm.py -> build/bdist.linux-x86_64/egg/apex/transformer/testing\n",
            "copying build/lib/apex/transformer/testing/__init__.py -> build/bdist.linux-x86_64/egg/apex/transformer/testing\n",
            "creating build/bdist.linux-x86_64/egg/apex/transformer/_data\n",
            "copying build/lib/apex/transformer/_data/__init__.py -> build/bdist.linux-x86_64/egg/apex/transformer/_data\n",
            "copying build/lib/apex/transformer/_data/_batchsampler.py -> build/bdist.linux-x86_64/egg/apex/transformer/_data\n",
            "copying build/lib/apex/transformer/_ucc_util.py -> build/bdist.linux-x86_64/egg/apex/transformer\n",
            "copying build/lib/apex/transformer/log_util.py -> build/bdist.linux-x86_64/egg/apex/transformer\n",
            "creating build/bdist.linux-x86_64/egg/apex/transformer/functional\n",
            "copying build/lib/apex/transformer/functional/fused_softmax.py -> build/bdist.linux-x86_64/egg/apex/transformer/functional\n",
            "copying build/lib/apex/transformer/functional/__init__.py -> build/bdist.linux-x86_64/egg/apex/transformer/functional\n",
            "copying build/lib/apex/transformer/enums.py -> build/bdist.linux-x86_64/egg/apex/transformer\n",
            "creating build/bdist.linux-x86_64/egg/apex/transformer/amp\n",
            "copying build/lib/apex/transformer/amp/grad_scaler.py -> build/bdist.linux-x86_64/egg/apex/transformer/amp\n",
            "copying build/lib/apex/transformer/amp/__init__.py -> build/bdist.linux-x86_64/egg/apex/transformer/amp\n",
            "copying build/lib/apex/transformer/utils.py -> build/bdist.linux-x86_64/egg/apex/transformer\n",
            "copying build/lib/apex/transformer/microbatches.py -> build/bdist.linux-x86_64/egg/apex/transformer\n",
            "creating build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel\n",
            "copying build/lib/apex/transformer/tensor_parallel/mappings.py -> build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel\n",
            "copying build/lib/apex/transformer/tensor_parallel/random.py -> build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel\n",
            "copying build/lib/apex/transformer/tensor_parallel/layers.py -> build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel\n",
            "copying build/lib/apex/transformer/tensor_parallel/utils.py -> build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel\n",
            "copying build/lib/apex/transformer/tensor_parallel/data.py -> build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel\n",
            "copying build/lib/apex/transformer/tensor_parallel/memory.py -> build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel\n",
            "copying build/lib/apex/transformer/tensor_parallel/cross_entropy.py -> build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel\n",
            "copying build/lib/apex/transformer/tensor_parallel/__init__.py -> build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel\n",
            "copying build/lib/apex/transformer/__init__.py -> build/bdist.linux-x86_64/egg/apex/transformer\n",
            "creating build/bdist.linux-x86_64/egg/apex/transformer/layers\n",
            "copying build/lib/apex/transformer/layers/layer_norm.py -> build/bdist.linux-x86_64/egg/apex/transformer/layers\n",
            "copying build/lib/apex/transformer/layers/__init__.py -> build/bdist.linux-x86_64/egg/apex/transformer/layers\n",
            "creating build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel\n",
            "copying build/lib/apex/transformer/pipeline_parallel/_timers.py -> build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel\n",
            "copying build/lib/apex/transformer/pipeline_parallel/utils.py -> build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel\n",
            "copying build/lib/apex/transformer/pipeline_parallel/p2p_communication.py -> build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel\n",
            "creating build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/schedules\n",
            "copying build/lib/apex/transformer/pipeline_parallel/schedules/common.py -> build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/schedules\n",
            "copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/schedules\n",
            "copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/schedules\n",
            "copying build/lib/apex/transformer/pipeline_parallel/schedules/__init__.py -> build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/schedules\n",
            "copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/schedules\n",
            "copying build/lib/apex/transformer/pipeline_parallel/__init__.py -> build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel\n",
            "copying build/lib/apex/transformer/parallel_state.py -> build/bdist.linux-x86_64/egg/apex/transformer\n",
            "creating build/bdist.linux-x86_64/egg/apex/fused_dense\n",
            "copying build/lib/apex/fused_dense/fused_dense.py -> build/bdist.linux-x86_64/egg/apex/fused_dense\n",
            "copying build/lib/apex/fused_dense/__init__.py -> build/bdist.linux-x86_64/egg/apex/fused_dense\n",
            "creating build/bdist.linux-x86_64/egg/apex/normalization\n",
            "copying build/lib/apex/normalization/fused_layer_norm.py -> build/bdist.linux-x86_64/egg/apex/normalization\n",
            "copying build/lib/apex/normalization/__init__.py -> build/bdist.linux-x86_64/egg/apex/normalization\n",
            "creating build/bdist.linux-x86_64/egg/apex/parallel\n",
            "copying build/lib/apex/parallel/multiproc.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
            "copying build/lib/apex/parallel/sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
            "copying build/lib/apex/parallel/optimized_sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
            "copying build/lib/apex/parallel/sync_batchnorm.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
            "copying build/lib/apex/parallel/optimized_sync_batchnorm.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
            "copying build/lib/apex/parallel/distributed.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
            "copying build/lib/apex/parallel/LARC.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
            "copying build/lib/apex/parallel/__init__.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/cudnn_gbn\n",
            "copying build/lib/apex/contrib/cudnn_gbn/batch_norm.py -> build/bdist.linux-x86_64/egg/apex/contrib/cudnn_gbn\n",
            "copying build/lib/apex/contrib/cudnn_gbn/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/cudnn_gbn\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/focal_loss\n",
            "copying build/lib/apex/contrib/focal_loss/focal_loss.py -> build/bdist.linux-x86_64/egg/apex/contrib/focal_loss\n",
            "copying build/lib/apex/contrib/focal_loss/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/focal_loss\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/groupbn\n",
            "copying build/lib/apex/contrib/groupbn/batch_norm.py -> build/bdist.linux-x86_64/egg/apex/contrib/groupbn\n",
            "copying build/lib/apex/contrib/groupbn/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/groupbn\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/clip_grad\n",
            "copying build/lib/apex/contrib/clip_grad/clip_grad.py -> build/bdist.linux-x86_64/egg/apex/contrib/clip_grad\n",
            "copying build/lib/apex/contrib/clip_grad/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/clip_grad\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib/apex/contrib/multihead_attn/self_multihead_attn.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib/apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib/apex/contrib/multihead_attn/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/sparsity\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_search_kernels\n",
            "copying build/lib/apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_search_kernels\n",
            "copying build/lib/apex/contrib/sparsity/permutation_search_kernels/channel_swap.py -> build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_search_kernels\n",
            "copying build/lib/apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_search_kernels\n",
            "copying build/lib/apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_search_kernels\n",
            "copying build/lib/apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_search_kernels\n",
            "copying build/lib/apex/contrib/sparsity/permutation_lib.py -> build/bdist.linux-x86_64/egg/apex/contrib/sparsity\n",
            "copying build/lib/apex/contrib/sparsity/asp.py -> build/bdist.linux-x86_64/egg/apex/contrib/sparsity\n",
            "copying build/lib/apex/contrib/sparsity/sparse_masklib.py -> build/bdist.linux-x86_64/egg/apex/contrib/sparsity\n",
            "copying build/lib/apex/contrib/sparsity/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/sparsity\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/transducer\n",
            "copying build/lib/apex/contrib/transducer/transducer.py -> build/bdist.linux-x86_64/egg/apex/contrib/transducer\n",
            "copying build/lib/apex/contrib/transducer/_transducer_ref.py -> build/bdist.linux-x86_64/egg/apex/contrib/transducer\n",
            "copying build/lib/apex/contrib/transducer/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/transducer\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/index_mul_2d\n",
            "copying build/lib/apex/contrib/index_mul_2d/index_mul_2d.py -> build/bdist.linux-x86_64/egg/apex/contrib/index_mul_2d\n",
            "copying build/lib/apex/contrib/index_mul_2d/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/index_mul_2d\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/layer_norm\n",
            "copying build/lib/apex/contrib/layer_norm/layer_norm.py -> build/bdist.linux-x86_64/egg/apex/contrib/layer_norm\n",
            "copying build/lib/apex/contrib/layer_norm/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/layer_norm\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
            "copying build/lib/apex/contrib/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
            "copying build/lib/apex/contrib/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
            "copying build/lib/apex/contrib/optimizers/distributed_fused_lamb.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
            "copying build/lib/apex/contrib/optimizers/fused_adam.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
            "copying build/lib/apex/contrib/optimizers/fp16_optimizer.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
            "copying build/lib/apex/contrib/optimizers/distributed_fused_adam.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
            "copying build/lib/apex/contrib/optimizers/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/xentropy\n",
            "copying build/lib/apex/contrib/xentropy/softmax_xentropy.py -> build/bdist.linux-x86_64/egg/apex/contrib/xentropy\n",
            "copying build/lib/apex/contrib/xentropy/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/xentropy\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/test\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/test/cudnn_gbn\n",
            "copying build/lib/apex/contrib/test/cudnn_gbn/test_cudnn_gbn_with_two_gpus.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/cudnn_gbn\n",
            "copying build/lib/apex/contrib/test/cudnn_gbn/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/cudnn_gbn\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/test/focal_loss\n",
            "copying build/lib/apex/contrib/test/focal_loss/test_focal_loss.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/focal_loss\n",
            "copying build/lib/apex/contrib/test/focal_loss/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/focal_loss\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/test/clip_grad\n",
            "copying build/lib/apex/contrib/test/clip_grad/test_clip_grad.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/clip_grad\n",
            "copying build/lib/apex/contrib/test/clip_grad/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/clip_grad\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn\n",
            "copying build/lib/apex/contrib/test/multihead_attn/test_encdec_multihead_attn.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn\n",
            "copying build/lib/apex/contrib/test/multihead_attn/test_fast_self_multihead_attn_bias.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn\n",
            "copying build/lib/apex/contrib/test/multihead_attn/test_mha_fused_softmax.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn\n",
            "copying build/lib/apex/contrib/test/multihead_attn/test_encdec_multihead_attn_norm_add.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn\n",
            "copying build/lib/apex/contrib/test/multihead_attn/test_self_multihead_attn_norm_add.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn\n",
            "copying build/lib/apex/contrib/test/multihead_attn/test_self_multihead_attn.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn\n",
            "copying build/lib/apex/contrib/test/multihead_attn/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/test/transducer\n",
            "copying build/lib/apex/contrib/test/transducer/test_transducer_loss.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/transducer\n",
            "copying build/lib/apex/contrib/test/transducer/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/transducer\n",
            "copying build/lib/apex/contrib/test/transducer/test_transducer_joint.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/transducer\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/test/index_mul_2d\n",
            "copying build/lib/apex/contrib/test/index_mul_2d/test_index_mul_2d.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/index_mul_2d\n",
            "copying build/lib/apex/contrib/test/index_mul_2d/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/index_mul_2d\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/test/layer_norm\n",
            "copying build/lib/apex/contrib/test/layer_norm/test_fast_layer_norm.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/layer_norm\n",
            "copying build/lib/apex/contrib/test/layer_norm/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/layer_norm\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/test/optimizers\n",
            "copying build/lib/apex/contrib/test/optimizers/test_distributed_fused_lamb.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/optimizers\n",
            "copying build/lib/apex/contrib/test/optimizers/test_dist_adam.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/optimizers\n",
            "copying build/lib/apex/contrib/test/optimizers/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/optimizers\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/test/xentropy\n",
            "copying build/lib/apex/contrib/test/xentropy/test_label_smoothing.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/xentropy\n",
            "copying build/lib/apex/contrib/test/xentropy/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/xentropy\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/test/peer_memory\n",
            "copying build/lib/apex/contrib/test/peer_memory/test_peer_halo_exchange_module.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/peer_memory\n",
            "copying build/lib/apex/contrib/test/peer_memory/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/peer_memory\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/test/bottleneck\n",
            "copying build/lib/apex/contrib/test/bottleneck/test_bottleneck_module.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/bottleneck\n",
            "copying build/lib/apex/contrib/test/bottleneck/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/bottleneck\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/test/conv_bias_relu\n",
            "copying build/lib/apex/contrib/test/conv_bias_relu/test_conv_bias_relu.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/conv_bias_relu\n",
            "copying build/lib/apex/contrib/test/conv_bias_relu/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/conv_bias_relu\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/test/fmha\n",
            "copying build/lib/apex/contrib/test/fmha/test_fmha.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/fmha\n",
            "copying build/lib/apex/contrib/test/fmha/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/test/fmha\n",
            "copying build/lib/apex/contrib/test/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/test\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/peer_memory\n",
            "copying build/lib/apex/contrib/peer_memory/peer_memory.py -> build/bdist.linux-x86_64/egg/apex/contrib/peer_memory\n",
            "copying build/lib/apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/bdist.linux-x86_64/egg/apex/contrib/peer_memory\n",
            "copying build/lib/apex/contrib/peer_memory/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/peer_memory\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/bottleneck\n",
            "copying build/lib/apex/contrib/bottleneck/bottleneck.py -> build/bdist.linux-x86_64/egg/apex/contrib/bottleneck\n",
            "copying build/lib/apex/contrib/bottleneck/halo_exchangers.py -> build/bdist.linux-x86_64/egg/apex/contrib/bottleneck\n",
            "copying build/lib/apex/contrib/bottleneck/test.py -> build/bdist.linux-x86_64/egg/apex/contrib/bottleneck\n",
            "copying build/lib/apex/contrib/bottleneck/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/bottleneck\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/conv_bias_relu\n",
            "copying build/lib/apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/bdist.linux-x86_64/egg/apex/contrib/conv_bias_relu\n",
            "copying build/lib/apex/contrib/conv_bias_relu/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/conv_bias_relu\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/fmha\n",
            "copying build/lib/apex/contrib/fmha/fmha.py -> build/bdist.linux-x86_64/egg/apex/contrib/fmha\n",
            "copying build/lib/apex/contrib/fmha/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/fmha\n",
            "copying build/lib/apex/contrib/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib\n",
            "creating build/bdist.linux-x86_64/egg/apex/RNN\n",
            "copying build/lib/apex/RNN/models.py -> build/bdist.linux-x86_64/egg/apex/RNN\n",
            "copying build/lib/apex/RNN/cells.py -> build/bdist.linux-x86_64/egg/apex/RNN\n",
            "copying build/lib/apex/RNN/RNNBackend.py -> build/bdist.linux-x86_64/egg/apex/RNN\n",
            "copying build/lib/apex/RNN/__init__.py -> build/bdist.linux-x86_64/egg/apex/RNN\n",
            "creating build/bdist.linux-x86_64/egg/apex/optimizers\n",
            "copying build/lib/apex/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n",
            "copying build/lib/apex/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n",
            "copying build/lib/apex/optimizers/fused_adam.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n",
            "copying build/lib/apex/optimizers/fused_novograd.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n",
            "copying build/lib/apex/optimizers/fused_adagrad.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n",
            "copying build/lib/apex/optimizers/fused_mixed_precision_lamb.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n",
            "copying build/lib/apex/optimizers/__init__.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n",
            "creating build/bdist.linux-x86_64/egg/apex/mlp\n",
            "copying build/lib/apex/mlp/mlp.py -> build/bdist.linux-x86_64/egg/apex/mlp\n",
            "copying build/lib/apex/mlp/__init__.py -> build/bdist.linux-x86_64/egg/apex/mlp\n",
            "creating build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib/apex/amp/scaler.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib/apex/amp/opt.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib/apex/amp/handle.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib/apex/amp/_amp_state.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib/apex/amp/utils.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib/apex/amp/_initialize.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib/apex/amp/_process_optimizer.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib/apex/amp/__version__.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "creating build/bdist.linux-x86_64/egg/apex/amp/lists\n",
            "copying build/lib/apex/amp/lists/torch_overrides.py -> build/bdist.linux-x86_64/egg/apex/amp/lists\n",
            "copying build/lib/apex/amp/lists/tensor_overrides.py -> build/bdist.linux-x86_64/egg/apex/amp/lists\n",
            "copying build/lib/apex/amp/lists/__init__.py -> build/bdist.linux-x86_64/egg/apex/amp/lists\n",
            "copying build/lib/apex/amp/lists/functional_overrides.py -> build/bdist.linux-x86_64/egg/apex/amp/lists\n",
            "copying build/lib/apex/amp/frontend.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib/apex/amp/amp.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib/apex/amp/compat.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib/apex/amp/rnn_compat.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib/apex/amp/__init__.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib/apex/amp/wrap.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "creating build/bdist.linux-x86_64/egg/apex/fp16_utils\n",
            "copying build/lib/apex/fp16_utils/fp16util.py -> build/bdist.linux-x86_64/egg/apex/fp16_utils\n",
            "copying build/lib/apex/fp16_utils/fp16_optimizer.py -> build/bdist.linux-x86_64/egg/apex/fp16_utils\n",
            "copying build/lib/apex/fp16_utils/loss_scaler.py -> build/bdist.linux-x86_64/egg/apex/fp16_utils\n",
            "copying build/lib/apex/fp16_utils/__init__.py -> build/bdist.linux-x86_64/egg/apex/fp16_utils\n",
            "creating build/bdist.linux-x86_64/egg/apex/multi_tensor_apply\n",
            "copying build/lib/apex/multi_tensor_apply/multi_tensor_apply.py -> build/bdist.linux-x86_64/egg/apex/multi_tensor_apply\n",
            "copying build/lib/apex/multi_tensor_apply/__init__.py -> build/bdist.linux-x86_64/egg/apex/multi_tensor_apply\n",
            "copying build/lib/apex/_autocast_utils.py -> build/bdist.linux-x86_64/egg/apex\n",
            "copying build/lib/apex/__init__.py -> build/bdist.linux-x86_64/egg/apex\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/testing/arguments.py to arguments.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/testing/global_vars.py to global_vars.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/testing/commons.py to commons.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/testing/standalone_bert.py to standalone_bert.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/testing/distributed_test_base.py to distributed_test_base.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/testing/standalone_gpt.py to standalone_gpt.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/testing/standalone_transformer_lm.py to standalone_transformer_lm.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/testing/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/_data/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/_data/_batchsampler.py to _batchsampler.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/_ucc_util.py to _ucc_util.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/log_util.py to log_util.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/functional/fused_softmax.py to fused_softmax.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/functional/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/enums.py to enums.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/amp/grad_scaler.py to grad_scaler.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/amp/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/utils.py to utils.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/microbatches.py to microbatches.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel/mappings.py to mappings.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel/random.py to random.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel/layers.py to layers.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel/utils.py to utils.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel/data.py to data.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel/memory.py to memory.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel/cross_entropy.py to cross_entropy.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/layers/layer_norm.py to layer_norm.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/layers/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/_timers.py to _timers.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/utils.py to utils.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/p2p_communication.py to p2p_communication.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/schedules/common.py to common.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py to fwd_bwd_no_pipelining.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py to fwd_bwd_pipelining_without_interleaving.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/schedules/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py to fwd_bwd_pipelining_with_interleaving.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/parallel_state.py to parallel_state.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/fused_dense/fused_dense.py to fused_dense.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/fused_dense/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/normalization/fused_layer_norm.py to fused_layer_norm.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/normalization/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/multiproc.py to multiproc.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/sync_batchnorm_kernel.py to sync_batchnorm_kernel.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/optimized_sync_batchnorm_kernel.py to optimized_sync_batchnorm_kernel.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/sync_batchnorm.py to sync_batchnorm.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/optimized_sync_batchnorm.py to optimized_sync_batchnorm.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/distributed.py to distributed.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/LARC.py to LARC.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/cudnn_gbn/batch_norm.py to batch_norm.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/cudnn_gbn/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/focal_loss/focal_loss.py to focal_loss.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/focal_loss/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/groupbn/batch_norm.py to batch_norm.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/groupbn/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/clip_grad/clip_grad.py to clip_grad.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/clip_grad/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py to fast_self_multihead_attn_func.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/self_multihead_attn.py to self_multihead_attn.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/mask_softmax_dropout_func.py to mask_softmax_dropout_func.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/self_multihead_attn_func.py to self_multihead_attn_func.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py to fast_self_multihead_attn_norm_add_func.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py to fast_encdec_multihead_attn_norm_add_func.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py to fast_encdec_multihead_attn_func.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/encdec_multihead_attn_func.py to encdec_multihead_attn_func.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/encdec_multihead_attn.py to encdec_multihead_attn.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py to call_permutation_search_kernels.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_search_kernels/channel_swap.py to channel_swap.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py to exhaustive_search.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py to permutation_utilities.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_search_kernels/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_lib.py to permutation_lib.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/sparsity/asp.py to asp.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/sparsity/sparse_masklib.py to sparse_masklib.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/sparsity/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/transducer/transducer.py to transducer.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/transducer/_transducer_ref.py to _transducer_ref.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/transducer/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/index_mul_2d/index_mul_2d.py to index_mul_2d.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/index_mul_2d/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/layer_norm/layer_norm.py to layer_norm.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/layer_norm/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/fused_lamb.py to fused_lamb.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/fused_sgd.py to fused_sgd.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/distributed_fused_lamb.py to distributed_fused_lamb.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/fused_adam.py to fused_adam.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/fp16_optimizer.py to fp16_optimizer.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/distributed_fused_adam.py to distributed_fused_adam.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/xentropy/softmax_xentropy.py to softmax_xentropy.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/xentropy/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/cudnn_gbn/test_cudnn_gbn_with_two_gpus.py to test_cudnn_gbn_with_two_gpus.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/cudnn_gbn/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/focal_loss/test_focal_loss.py to test_focal_loss.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/focal_loss/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/clip_grad/test_clip_grad.py to test_clip_grad.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/clip_grad/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn/test_encdec_multihead_attn.py to test_encdec_multihead_attn.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn/test_fast_self_multihead_attn_bias.py to test_fast_self_multihead_attn_bias.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn/test_mha_fused_softmax.py to test_mha_fused_softmax.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn/test_encdec_multihead_attn_norm_add.py to test_encdec_multihead_attn_norm_add.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn/test_self_multihead_attn_norm_add.py to test_self_multihead_attn_norm_add.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn/test_self_multihead_attn.py to test_self_multihead_attn.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/multihead_attn/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/transducer/test_transducer_loss.py to test_transducer_loss.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/transducer/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/transducer/test_transducer_joint.py to test_transducer_joint.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/index_mul_2d/test_index_mul_2d.py to test_index_mul_2d.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/index_mul_2d/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/layer_norm/test_fast_layer_norm.py to test_fast_layer_norm.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/layer_norm/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/optimizers/test_distributed_fused_lamb.py to test_distributed_fused_lamb.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/optimizers/test_dist_adam.py to test_dist_adam.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/optimizers/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/xentropy/test_label_smoothing.py to test_label_smoothing.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/xentropy/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/peer_memory/test_peer_halo_exchange_module.py to test_peer_halo_exchange_module.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/peer_memory/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/bottleneck/test_bottleneck_module.py to test_bottleneck_module.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/bottleneck/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/conv_bias_relu/test_conv_bias_relu.py to test_conv_bias_relu.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/conv_bias_relu/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/fmha/test_fmha.py to test_fmha.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/fmha/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/test/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/peer_memory/peer_memory.py to peer_memory.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/peer_memory/peer_halo_exchanger_1d.py to peer_halo_exchanger_1d.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/peer_memory/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/bottleneck/bottleneck.py to bottleneck.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/bottleneck/halo_exchangers.py to halo_exchangers.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/bottleneck/test.py to test.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/bottleneck/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/conv_bias_relu/conv_bias_relu.py to conv_bias_relu.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/conv_bias_relu/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/fmha/fmha.py to fmha.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/fmha/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/RNN/models.py to models.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/RNN/cells.py to cells.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/RNN/RNNBackend.py to RNNBackend.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/RNN/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/fused_lamb.py to fused_lamb.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/fused_sgd.py to fused_sgd.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/fused_adam.py to fused_adam.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/fused_novograd.py to fused_novograd.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/fused_adagrad.py to fused_adagrad.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/fused_mixed_precision_lamb.py to fused_mixed_precision_lamb.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/mlp/mlp.py to mlp.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/mlp/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/scaler.py to scaler.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/opt.py to opt.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/handle.py to handle.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/_amp_state.py to _amp_state.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/utils.py to utils.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/_initialize.py to _initialize.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/_process_optimizer.py to _process_optimizer.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/__version__.py to __version__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/lists/torch_overrides.py to torch_overrides.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/lists/tensor_overrides.py to tensor_overrides.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/lists/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/lists/functional_overrides.py to functional_overrides.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/frontend.py to frontend.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/amp.py to amp.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/compat.py to compat.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/rnn_compat.py to rnn_compat.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/wrap.py to wrap.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/fp16_utils/fp16util.py to fp16util.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/fp16_utils/fp16_optimizer.py to fp16_optimizer.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/fp16_utils/loss_scaler.py to loss_scaler.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/fp16_utils/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/multi_tensor_apply/multi_tensor_apply.py to multi_tensor_apply.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/multi_tensor_apply/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/_autocast_utils.py to _autocast_utils.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/__init__.py to __init__.cpython-38.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying apex.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying apex.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying apex.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying apex.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying apex.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating 'dist/apex-0.1-py3.8.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing apex-0.1-py3.8.egg\n",
            "Removing /usr/local/lib/python3.8/dist-packages/apex-0.1-py3.8.egg\n",
            "Copying apex-0.1-py3.8.egg to /usr/local/lib/python3.8/dist-packages\n",
            "apex 0.1 is already the active version in easy-install.pth\n",
            "\n",
            "Installed /usr/local/lib/python3.8/dist-packages/apex-0.1-py3.8.egg\n",
            "Processing dependencies for apex==0.1\n",
            "Searching for packaging==21.3\n",
            "Best match: packaging 21.3\n",
            "Adding packaging 21.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.8/dist-packages\n",
            "Searching for pyparsing==3.0.9\n",
            "Best match: pyparsing 3.0.9\n",
            "Adding pyparsing 3.0.9 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.8/dist-packages\n",
            "Finished processing dependencies for apex==0.1\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 여기서 import가 안된다면 런타임을 다시 시작해주세요\n",
        "import apex.amp"
      ],
      "metadata": {
        "id": "wZr_ClN6IWlE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!parlai display_data -t personachat\n",
        "!parlai display_data -t empathetic_dialogues\n",
        "!parlai display_data -t dailydialog\n",
        "!parlai display_model -t dailydialog -mf zoo:tutorial_transformer_generator/model --optimizer adam\n",
        "!parlai display_data -t personachat -mf zoo:pretrained_transformers/bi_model_huge_reddit/model --optimizer adam"
      ],
      "metadata": {
        "id": "-5cJkVoU9WH_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a06b694e-4faf-4ce6-c4dc-159ead3e731b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "05:26:03 | Opt:\n",
            "05:26:03 |     allow_missing_init_opts: False\n",
            "05:26:03 |     batchsize: 1\n",
            "05:26:03 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
            "05:26:03 |     datatype: train:ordered\n",
            "05:26:03 |     dict_class: None\n",
            "05:26:03 |     display_add_fields: \n",
            "05:26:03 |     download_path: None\n",
            "05:26:03 |     dynamic_batching: None\n",
            "05:26:03 |     hide_labels: False\n",
            "05:26:03 |     ignore_agent_reply: True\n",
            "05:26:03 |     image_cropsize: 224\n",
            "05:26:03 |     image_mode: raw\n",
            "05:26:03 |     image_size: 256\n",
            "05:26:03 |     init_model: None\n",
            "05:26:03 |     init_opt: None\n",
            "05:26:03 |     is_debug: False\n",
            "05:26:03 |     loglevel: info\n",
            "05:26:03 |     max_display_len: 1000\n",
            "05:26:03 |     model: None\n",
            "05:26:03 |     model_file: None\n",
            "05:26:03 |     multitask_weights: [1]\n",
            "05:26:03 |     mutators: None\n",
            "05:26:03 |     num_examples: 10\n",
            "05:26:03 |     override: {}\n",
            "05:26:03 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
            "05:26:03 |     starttime: Jan12_05-26\n",
            "05:26:03 |     task: personachat\n",
            "05:26:03 |     verbose: False\n",
            "05:26:03 | creating task(s): personachat\n",
            "[building data: /usr/local/lib/python3.8/dist-packages/data/Persona-Chat]\n",
            "05:26:03 | Downloading http://parl.ai/downloads/personachat/personachat.tgz to /usr/local/lib/python3.8/dist-packages/data/Persona-Chat/personachat.tgz\n",
            "Downloading personachat.tgz: 100% 223M/223M [00:05<00:00, 40.5MB/s]\n",
            "05:26:18 | loading fbdialog data: /usr/local/lib/python3.8/dist-packages/data/Persona-Chat/personachat/train_self_original.txt\n",
            "\u001b[1;31m- - - NEW EPISODE: personachat - - -\u001b[0;0m\n",
            "\u001b[0myour persona: i like to remodel homes.\n",
            "your persona: i like to go hunting.\n",
            "your persona: i like to shoot a bow.\n",
            "your persona: my favorite holiday is halloween.\n",
            "hi , how are you doing ? i am getting ready to do some cheetah chasing to stay in shape .\u001b[0;0m\n",
            "   \u001b[1;94myou must be very fast . hunting is one of my favorite hobbies .\u001b[0;0m\n",
            "\u001b[0mi am ! for my hobby i like to do canning or some whittling .\u001b[0;0m\n",
            "   \u001b[1;94mi also remodel homes when i am not out bow hunting .\u001b[0;0m\n",
            "\u001b[0mthat is neat . when i was in high school i placed 6th in 100m dash !\u001b[0;0m\n",
            "   \u001b[1;94mthat is awesome . do you have a favorite season or time of year ?\u001b[0;0m\n",
            "\u001b[0mi do not . but i do have a favorite meat since that is all i eat exclusively .\u001b[0;0m\n",
            "   \u001b[1;94mwhat is your favorite meat to eat ?\u001b[0;0m\n",
            "\u001b[0mi would have to say its prime rib . do you have any favorite foods ?\u001b[0;0m\n",
            "   \u001b[1;94mi like chicken or macaroni and cheese .\u001b[0;0m\n",
            "\u001b[0mdo you have anything planned for today ? i think i am going to do some canning .\u001b[0;0m\n",
            "   \u001b[1;94mi am going to watch football . what are you canning ?\u001b[0;0m\n",
            "\u001b[0mi think i will can some jam . do you also play footfall for fun ?\u001b[0;0m\n",
            "   \u001b[1;94mif i have time outside of hunting and remodeling homes . which is not much !\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: personachat - - -\u001b[0;0m\n",
            "\u001b[0myour persona: my mom is my best friend.\n",
            "your persona: i have four sisters.\n",
            "your persona: i believe that mermaids are real.\n",
            "your persona: i love iced tea.\n",
            "hi , how are you doing today ?\u001b[0;0m\n",
            "   \u001b[1;94mi am spending time with my 4 sisters what are you up to\u001b[0;0m\n",
            "\u001b[0mwow , four sisters . just watching game of thrones .\u001b[0;0m\n",
            "   \u001b[1;94mthat is a good show i watch that while drinking iced tea\u001b[0;0m\n",
            "\u001b[0mi agree . what do you do for a living ?\u001b[0;0m\n",
            "   \u001b[1;94mi am a researcher i am researching the fact that mermaids are real\u001b[0;0m\n",
            "05:26:19 | loaded 8939 episodes with a total of 65719 examples\n",
            "05:26:29 | Opt:\n",
            "05:26:29 |     allow_missing_init_opts: False\n",
            "05:26:29 |     batchsize: 1\n",
            "05:26:29 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
            "05:26:29 |     datatype: train:ordered\n",
            "05:26:29 |     dict_class: None\n",
            "05:26:29 |     display_add_fields: \n",
            "05:26:29 |     download_path: None\n",
            "05:26:29 |     dynamic_batching: None\n",
            "05:26:29 |     hide_labels: False\n",
            "05:26:29 |     ignore_agent_reply: True\n",
            "05:26:29 |     image_cropsize: 224\n",
            "05:26:29 |     image_mode: raw\n",
            "05:26:29 |     image_size: 256\n",
            "05:26:29 |     init_model: None\n",
            "05:26:29 |     init_opt: None\n",
            "05:26:29 |     is_debug: False\n",
            "05:26:29 |     loglevel: info\n",
            "05:26:29 |     max_display_len: 1000\n",
            "05:26:29 |     model: None\n",
            "05:26:29 |     model_file: None\n",
            "05:26:29 |     multitask_weights: [1]\n",
            "05:26:29 |     mutators: None\n",
            "05:26:29 |     num_examples: 10\n",
            "05:26:29 |     override: {}\n",
            "05:26:29 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
            "05:26:29 |     starttime: Jan12_05-26\n",
            "05:26:29 |     task: empathetic_dialogues\n",
            "05:26:29 |     train_experiencer_only: False\n",
            "05:26:29 |     verbose: False\n",
            "05:26:30 | creating task(s): empathetic_dialogues\n",
            "[building data: /usr/local/lib/python3.8/dist-packages/data/empatheticdialogues]\n",
            "05:26:30 | Downloading http://parl.ai/downloads/empatheticdialogues/empatheticdialogues.tar.gz to /usr/local/lib/python3.8/dist-packages/data/empatheticdialogues/empatheticdialogues.tar.gz\n",
            "Downloading empatheticdialogues.tar.gz: 100% 28.0M/28.0M [00:01<00:00, 19.0MB/s]\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues - - -\u001b[0;0m\n",
            "\u001b[0mI remember going to see the fireworks with my best friend. It was the first time we ever spent time alone together. Although there was a lot of people, we felt like the only people in the world.\u001b[0;0m\n",
            "   \u001b[1;94mWas this a friend you were in love with, or just a best friend?\u001b[0;0m\n",
            "\u001b[0mThis was a best friend. I miss her.\u001b[0;0m\n",
            "   \u001b[1;94mWhere has she gone?\u001b[0;0m\n",
            "\u001b[0mWe no longer talk.\u001b[0;0m\n",
            "   \u001b[1;94mOh was this something that happened because of an argument?\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues - - -\u001b[0;0m\n",
            "\u001b[0mWas this a friend you were in love with, or just a best friend?\u001b[0;0m\n",
            "   \u001b[1;94mThis was a best friend. I miss her.\u001b[0;0m\n",
            "\u001b[0mWhere has she gone?\u001b[0;0m\n",
            "   \u001b[1;94mWe no longer talk.\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues - - -\u001b[0;0m\n",
            "\u001b[0m it feels like hitting to blank wall when i see the darkness\u001b[0;0m\n",
            "   \u001b[1;94mOh ya? I don't really see how\u001b[0;0m\n",
            "\u001b[0mdont you feel so.. its a wonder \u001b[0;0m\n",
            "   \u001b[1;94mI do actually hit blank walls a lot of times but i get by\u001b[0;0m\n",
            "\u001b[0m i virtually thought so.. and i used to get sweatings\u001b[0;0m\n",
            "   \u001b[1;94mWait what are sweatings\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues - - -\u001b[0;0m\n",
            "\u001b[0mOh ya? I don't really see how\u001b[0;0m\n",
            "   \u001b[1;94mdont you feel so.. its a wonder\u001b[0;0m\n",
            "\u001b[0mI do actually hit blank walls a lot of times but i get by\u001b[0;0m\n",
            "   \u001b[1;94mi virtually thought so.. and i used to get sweatings\u001b[0;0m\n",
            "05:26:33 | loaded 39057 episodes with a total of 64636 examples\n",
            "05:26:43 | Opt:\n",
            "05:26:43 |     allow_missing_init_opts: False\n",
            "05:26:43 |     batchsize: 1\n",
            "05:26:43 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
            "05:26:43 |     datatype: train:ordered\n",
            "05:26:43 |     dict_class: None\n",
            "05:26:43 |     display_add_fields: \n",
            "05:26:43 |     download_path: None\n",
            "05:26:43 |     dynamic_batching: None\n",
            "05:26:43 |     hide_labels: False\n",
            "05:26:43 |     ignore_agent_reply: True\n",
            "05:26:43 |     image_cropsize: 224\n",
            "05:26:43 |     image_mode: raw\n",
            "05:26:43 |     image_size: 256\n",
            "05:26:43 |     init_model: None\n",
            "05:26:43 |     init_opt: None\n",
            "05:26:43 |     is_debug: False\n",
            "05:26:43 |     loglevel: info\n",
            "05:26:43 |     max_display_len: 1000\n",
            "05:26:43 |     model: None\n",
            "05:26:43 |     model_file: None\n",
            "05:26:43 |     multitask_weights: [1]\n",
            "05:26:43 |     mutators: None\n",
            "05:26:43 |     num_examples: 10\n",
            "05:26:43 |     override: {}\n",
            "05:26:43 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
            "05:26:43 |     starttime: Jan12_05-26\n",
            "05:26:43 |     task: dailydialog\n",
            "05:26:43 |     verbose: False\n",
            "05:26:43 | creating task(s): dailydialog\n",
            "[building data: /usr/local/lib/python3.8/dist-packages/data/dailydialog]\n",
            "05:26:43 | Downloading http://parl.ai/downloads/dailydialog/dailydialog.tar.gz to /usr/local/lib/python3.8/dist-packages/data/dailydialog/dailydialog.tar.gz\n",
            "Downloading dailydialog.tar.gz: 100% 2.72M/2.72M [00:01<00:00, 2.48MB/s]\n",
            "\u001b[1;31m- - - NEW EPISODE: dailydialog - - -\u001b[0;0m\n",
            "\u001b[0m__SILENCE__\u001b[0;0m\n",
            "   \u001b[1;94mSay , Jim , how about going for a few beers after dinner ?\u001b[0;0m\n",
            "\u001b[0mYou know that is tempting but is really not good for our fitness .\u001b[0;0m\n",
            "   \u001b[1;94mWhat do you mean ? It will help us to relax .\u001b[0;0m\n",
            "\u001b[0mDo you really think so ? I don't . It will just make us fat and act silly . Remember last time ?\u001b[0;0m\n",
            "   \u001b[1;94mI guess you are right . But what shall we do ? I don't feel like sitting at home .\u001b[0;0m\n",
            "\u001b[0mI suggest a walk over to the gym where we can play singsong and meet some of our friends .\u001b[0;0m\n",
            "   \u001b[1;94mThat's a good idea . I hear Mary and Sally often go there to play pingpong . Perhaps we can make a foursome with them .\u001b[0;0m\n",
            "\u001b[0mSounds great to me ! If they are willing , we could ask them to go dancing with us . That is excellent exercise and fun , too .\u001b[0;0m\n",
            "   \u001b[1;94mGood . Let ' s go now .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: dailydialog - - -\u001b[0;0m\n",
            "\u001b[0mSay , Jim , how about going for a few beers after dinner ?\u001b[0;0m\n",
            "   \u001b[1;94mYou know that is tempting but is really not good for our fitness .\u001b[0;0m\n",
            "\u001b[0mWhat do you mean ? It will help us to relax .\u001b[0;0m\n",
            "   \u001b[1;94mDo you really think so ? I don't . It will just make us fat and act silly . Remember last time ?\u001b[0;0m\n",
            "\u001b[0mI guess you are right . But what shall we do ? I don't feel like sitting at home .\u001b[0;0m\n",
            "   \u001b[1;94mI suggest a walk over to the gym where we can play singsong and meet some of our friends .\u001b[0;0m\n",
            "\u001b[0mThat's a good idea . I hear Mary and Sally often go there to play pingpong . Perhaps we can make a foursome with them .\u001b[0;0m\n",
            "   \u001b[1;94mSounds great to me ! If they are willing , we could ask them to go dancing with us . That is excellent exercise and fun , too .\u001b[0;0m\n",
            "\u001b[0mGood . Let ' s go now .\u001b[0;0m\n",
            "   \u001b[1;94mAll right .\u001b[0;0m\n",
            "05:26:45 | loaded 22236 episodes with a total of 87170 examples\n",
            "05:26:54 | building data: /usr/local/lib/python3.8/dist-packages/data/models/tutorial_transformer_generator/tutorial_transformer_generator_v1.tar.gz\n",
            "05:26:54 | Downloading http://parl.ai/downloads/_models/tutorial_transformer_generator/tutorial_transformer_generator_v1.tar.gz to /usr/local/lib/python3.8/dist-packages/data/models/tutorial_transformer_generator/tutorial_transformer_generator_v1.tar.gz\n",
            "Downloading tutorial_transformer_generator_v1.tar.gz: 100% 1.12G/1.12G [00:24<00:00, 45.8MB/s]\n",
            "05:27:37 | \u001b[33mOverriding opt[\"optimizer\"] to adam (previously: fused_adam)\u001b[0m\n",
            "05:27:37 | \u001b[33mLoading model with `--beam-block-full-context false`\u001b[0m\n",
            "05:27:37 | Using CUDA\n",
            "05:27:37 | loading dictionary from /usr/local/lib/python3.8/dist-packages/data/models/tutorial_transformer_generator/model.dict\n",
            "05:27:37 | num words = 54944\n",
            "05:27:38 | \u001b[33mDEPRECATED: XLM should only be used for backwards compatibility, as it involves a less-stable layernorm operation.\u001b[0m\n",
            "05:27:39 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "05:27:39 | Loading existing model params from /usr/local/lib/python3.8/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "05:27:41 | \u001b[33mWARNING: not loading optim state since model params changed.\u001b[0m\n",
            "05:27:41 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n",
            "05:27:41 | \u001b[33m--lr-scheduler invsqrt requires a value for --invsqrt-lr-decay-gamma. Defaulting to set gamma to --warmup-updates value for backwards compatibility.\u001b[0m\n",
            "05:27:41 | creating task(s): dailydialog\n",
            "05:27:41 | Opt:\n",
            "05:27:41 |     activation: gelu\n",
            "05:27:41 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "05:27:41 |     adam_eps: 1e-06\n",
            "05:27:41 |     add_p1_after_newln: False\n",
            "05:27:41 |     aggregate_micro: False\n",
            "05:27:41 |     allow_missing_init_opts: False\n",
            "05:27:41 |     attention_dropout: 0.0\n",
            "05:27:41 |     batch_length_range: 5\n",
            "05:27:41 |     batch_sort_cache_type: pop\n",
            "05:27:41 |     batch_sort_field: text\n",
            "05:27:41 |     batchsize: 48\n",
            "05:27:41 |     beam_block_full_context: False\n",
            "05:27:41 |     beam_block_list_filename: None\n",
            "05:27:41 |     beam_block_ngram: 3\n",
            "05:27:41 |     beam_context_block_ngram: 3\n",
            "05:27:41 |     beam_delay: 30\n",
            "05:27:41 |     beam_length_penalty: 0.65\n",
            "05:27:41 |     beam_min_length: 10\n",
            "05:27:41 |     beam_min_n_best: 3\n",
            "05:27:41 |     beam_size: 8\n",
            "05:27:41 |     betas: '[0.9, 0.98]'\n",
            "05:27:41 |     bpe_add_prefix_space: None\n",
            "05:27:41 |     bpe_debug: False\n",
            "05:27:41 |     bpe_dropout: None\n",
            "05:27:41 |     bpe_merge: None\n",
            "05:27:41 |     bpe_vocab: None\n",
            "05:27:41 |     checkpoint_activations: False\n",
            "05:27:41 |     compute_tokenized_bleu: False\n",
            "05:27:41 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
            "05:27:41 |     datatype: train:stream\n",
            "05:27:41 |     delimiter: '\\n'\n",
            "05:27:41 |     dict_build_first: True\n",
            "05:27:41 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "05:27:41 |     dict_endtoken: __end__\n",
            "05:27:41 |     dict_file: /usr/local/lib/python3.8/dist-packages/data/models/tutorial_transformer_generator/model.dict\n",
            "05:27:41 |     dict_include_test: False\n",
            "05:27:41 |     dict_include_valid: False\n",
            "05:27:41 |     dict_initpath: None\n",
            "05:27:41 |     dict_language: english\n",
            "05:27:41 |     dict_loaded: True\n",
            "05:27:41 |     dict_lower: True\n",
            "05:27:41 |     dict_max_ngram_size: -1\n",
            "05:27:41 |     dict_maxexs: -1\n",
            "05:27:41 |     dict_maxtokens: -1\n",
            "05:27:41 |     dict_minfreq: 0\n",
            "05:27:41 |     dict_nulltoken: __null__\n",
            "05:27:41 |     dict_starttoken: __start__\n",
            "05:27:41 |     dict_textfields: text,labels\n",
            "05:27:41 |     dict_tokenizer: bpe\n",
            "05:27:41 |     dict_unktoken: __unk__\n",
            "05:27:41 |     display_add_fields: \n",
            "05:27:41 |     display_examples: False\n",
            "05:27:41 |     distributed_world_size: 64\n",
            "05:27:41 |     download_path: None\n",
            "05:27:41 |     dropout: 0.1\n",
            "05:27:41 |     dynamic_batching: None\n",
            "05:27:41 |     embedding_projection: random\n",
            "05:27:41 |     embedding_size: 512\n",
            "05:27:41 |     embedding_type: random\n",
            "05:27:41 |     embeddings_scale: True\n",
            "05:27:41 |     eval_batchsize: None\n",
            "05:27:41 |     evaltask: None\n",
            "05:27:41 |     ffn_size: 2048\n",
            "05:27:41 |     force_fp16_tokens: True\n",
            "05:27:41 |     fp16: True\n",
            "05:27:41 |     fp16_impl: safe\n",
            "05:27:41 |     gpu: 0\n",
            "05:27:41 |     gradient_clip: 10.0\n",
            "05:27:41 |     hide_labels: False\n",
            "05:27:41 |     history_add_global_end_token: None\n",
            "05:27:41 |     history_reversed: False\n",
            "05:27:41 |     history_size: -1\n",
            "05:27:41 |     image_cropsize: 224\n",
            "05:27:41 |     image_mode: raw\n",
            "05:27:41 |     image_size: 256\n",
            "05:27:41 |     inference: beam\n",
            "05:27:41 |     init_model: None\n",
            "05:27:41 |     init_opt: None\n",
            "05:27:41 |     interactive_mode: False\n",
            "05:27:41 |     invsqrt_lr_decay_gamma: -1\n",
            "05:27:41 |     is_debug: False\n",
            "05:27:41 |     label_truncate: 128\n",
            "05:27:41 |     learn_positional_embeddings: True\n",
            "05:27:41 |     learningrate: 0.0005\n",
            "05:27:41 |     log_every_n_secs: 30.0\n",
            "05:27:41 |     loglevel: info\n",
            "05:27:41 |     lr_scheduler: invsqrt\n",
            "05:27:41 |     lr_scheduler_decay: 0.5\n",
            "05:27:41 |     lr_scheduler_patience: 3\n",
            "05:27:41 |     max_train_time: -1\n",
            "05:27:41 |     metrics: default\n",
            "05:27:41 |     model: transformer/generator\n",
            "05:27:41 |     model_file: /usr/local/lib/python3.8/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "05:27:41 |     model_parallel: False\n",
            "05:27:41 |     momentum: 0\n",
            "05:27:41 |     multitask_weights: [1]\n",
            "05:27:41 |     mutators: None\n",
            "05:27:41 |     n_decoder_layers: -1\n",
            "05:27:41 |     n_encoder_layers: -1\n",
            "05:27:41 |     n_heads: 16\n",
            "05:27:41 |     n_layers: 8\n",
            "05:27:41 |     n_positions: 512\n",
            "05:27:41 |     n_segments: 0\n",
            "05:27:41 |     nesterov: True\n",
            "05:27:41 |     no_cuda: False\n",
            "05:27:41 |     num_epochs: 5.0\n",
            "05:27:41 |     num_examples: 10\n",
            "05:27:41 |     numthreads: 1\n",
            "05:27:41 |     numworkers: 4\n",
            "05:27:41 |     nus: [0.7]\n",
            "05:27:41 |     optimizer: adam\n",
            "05:27:41 |     output_scaling: 1.0\n",
            "05:27:41 |     override: \"{'optimizer': 'adam'}\"\n",
            "05:27:41 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
            "05:27:41 |     person_tokens: False\n",
            "05:27:41 |     port: 61337\n",
            "05:27:41 |     pytorch_context_length: -1\n",
            "05:27:41 |     pytorch_datapath: None\n",
            "05:27:41 |     pytorch_include_labels: True\n",
            "05:27:41 |     pytorch_preprocess: False\n",
            "05:27:41 |     pytorch_teacher_batch_sort: False\n",
            "05:27:41 |     pytorch_teacher_dataset: None\n",
            "05:27:41 |     pytorch_teacher_task: None\n",
            "05:27:41 |     rank_candidates: False\n",
            "05:27:41 |     relu_dropout: 0.0\n",
            "05:27:41 |     save_after_valid: True\n",
            "05:27:41 |     save_every_n_secs: -1\n",
            "05:27:41 |     share_word_embeddings: True\n",
            "05:27:41 |     short_final_eval: True\n",
            "05:27:41 |     show_advanced_args: False\n",
            "05:27:41 |     shuffle: False\n",
            "05:27:41 |     skip_generation: False\n",
            "05:27:41 |     special_tok_lst: None\n",
            "05:27:41 |     split_lines: False\n",
            "05:27:41 |     starttime: Jan12_05-27\n",
            "05:27:41 |     task: internal:new_reddit:presorted\n",
            "05:27:41 |     temperature: 1.0\n",
            "05:27:41 |     tensorboard_log: False\n",
            "05:27:41 |     text_truncate: 512\n",
            "05:27:41 |     topk: 10\n",
            "05:27:41 |     topp: 0.9\n",
            "05:27:41 |     truncate: -1\n",
            "05:27:41 |     update_freq: 1\n",
            "05:27:41 |     use_reply: label\n",
            "05:27:41 |     validation_cutoff: 1.0\n",
            "05:27:41 |     validation_every_n_epochs: -1\n",
            "05:27:41 |     validation_every_n_secs: 1800.0\n",
            "05:27:41 |     validation_max_exs: 9920\n",
            "05:27:41 |     validation_metric: ppl\n",
            "05:27:41 |     validation_metric_mode: min\n",
            "05:27:41 |     validation_patience: 0\n",
            "05:27:41 |     validation_share_agent: False\n",
            "05:27:41 |     variant: xlm\n",
            "05:27:41 |     verbose: False\n",
            "05:27:41 |     warmup_rate: 0.0001\n",
            "05:27:41 |     warmup_updates: 20000\n",
            "05:27:41 |     weight_decay: 0.01\n",
            "\u001b[1;31m- - - NEW EPISODE: dailydialog- - -\u001b[0;0m\n",
            "\u001b[0m__SILENCE__\u001b[0;0m\n",
            "\u001b[1;94m    labels: Good morning , sir . Is there a bank near here ?\u001b[0;0m\n",
            "\u001b[0;95m     model: i don ' t know what you ' re talking about .\u001b[0;0m\n",
            "\u001b[0mThere is one . 5 blocks away from here ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: Well , that's too far . Can you change some money for me ?\u001b[0;0m\n",
            "\u001b[0;95m     model: i ' ll have to check it out . thanks !\u001b[0;0m\n",
            "\u001b[0mSurely , of course . What kind of currency have you got ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: RIB .\u001b[0;0m\n",
            "\u001b[0;95m     model: i don ' t know , but i ' ll take it .\u001b[0;0m\n",
            "\u001b[0mHow much would you like to change ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: 1000 Yuan . Here you are .\u001b[0;0m\n",
            "\u001b[0;95m     model: i don ' t know . how much do you want ?\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: dailydialog- - -\u001b[0;0m\n",
            "\u001b[0mGood morning , sir . Is there a bank near here ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: There is one . 5 blocks away from here ?\u001b[0;0m\n",
            "\u001b[0;95m     model: i don ' t think so , but i ' ll check .\u001b[0;0m\n",
            "\u001b[0mWell , that's too far . Can you change some money for me ?\u001b[0;0m\n",
            "\u001b[1;94m    labels: Surely , of course . What kind of currency have you got ?\u001b[0;0m\n",
            "\u001b[0;95m     model: i ' m sorry , i don ' t have any money .\u001b[0;0m\n",
            "\u001b[0mRIB .\u001b[0;0m\n",
            "\u001b[1;94m    labels: How much would you like to change ?\u001b[0;0m\n",
            "\u001b[0;95m     model: i ' m sorry , i don ' t have that kind of money .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: dailydialog- - -\u001b[0;0m\n",
            "\u001b[0m__SILENCE__\u001b[0;0m\n",
            "\u001b[1;94m    labels: Good afternoon . This is Michelle Li speaking , calling on behalf of IBA . Is Mr Meng available at all ?\u001b[0;0m\n",
            "\u001b[0;95m     model: i don ' t know what you ' re talking about .\u001b[0;0m\n",
            "\u001b[0mThis is Mr Meng speaking , Michelle .\u001b[0;0m\n",
            "\u001b[1;94m    labels: Oh , hello ! Sorry about that . I'm just calling to say that we've received your new Corporate Credit Card from HQ .\u001b[0;0m\n",
            "\u001b[0;95m     model: this is mrs . meng talking , michelle !\u001b[0;0m\n",
            "\u001b[0mThat was quick ! I wasn't expecting it until later this week .\u001b[0;0m\n",
            "\u001b[1;94m    labels: Yes , our application procedures have speeded up since we started using the new fast-track system .\u001b[0;0m\n",
            "\u001b[0;95m     model: that ' s what she said . . . : )\u001b[0;0m\n",
            "05:28:00 | building data: /usr/local/lib/python3.8/dist-packages/data/models/pretrained_transformers/pretrained_transformers.tgz\n",
            "05:28:00 | Downloading http://parl.ai/downloads/_models/pretrained_transformers/pretrained_transformers.tgz to /usr/local/lib/python3.8/dist-packages/data/models/pretrained_transformers/pretrained_transformers.tgz\n",
            "Downloading pretrained_transformers.tgz: 100% 4.22G/4.22G [01:29<00:00, 47.0MB/s]\n",
            "05:31:46 | Opt:\n",
            "05:31:46 |     activation: relu\n",
            "05:31:46 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "05:31:46 |     adam_eps: 1e-08\n",
            "05:31:46 |     add_p1_after_newln: False\n",
            "05:31:46 |     allow_missing_init_opts: False\n",
            "05:31:46 |     attention_dropout: 0.0\n",
            "05:31:46 |     batchsize: 1\n",
            "05:31:46 |     betas: '(0.9, 0.999)'\n",
            "05:31:46 |     bpe_add_prefix_space: None\n",
            "05:31:46 |     bpe_debug: False\n",
            "05:31:46 |     bpe_dropout: None\n",
            "05:31:46 |     bpe_merge: None\n",
            "05:31:46 |     bpe_vocab: None\n",
            "05:31:46 |     candidates: inline\n",
            "05:31:46 |     cap_num_predictions: 100\n",
            "05:31:46 |     checkpoint_activations: False\n",
            "05:31:46 |     data_parallel: False\n",
            "05:31:46 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
            "05:31:46 |     datatype: train:ordered\n",
            "05:31:46 |     delimiter: '\\n'\n",
            "05:31:46 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "05:31:46 |     dict_endtoken: __end__\n",
            "05:31:46 |     dict_file: None\n",
            "05:31:46 |     dict_initpath: None\n",
            "05:31:46 |     dict_language: english\n",
            "05:31:46 |     dict_lower: False\n",
            "05:31:46 |     dict_max_ngram_size: -1\n",
            "05:31:46 |     dict_maxtokens: -1\n",
            "05:31:46 |     dict_minfreq: 0\n",
            "05:31:46 |     dict_nulltoken: __null__\n",
            "05:31:46 |     dict_starttoken: __start__\n",
            "05:31:46 |     dict_textfields: text,labels\n",
            "05:31:46 |     dict_tokenizer: re\n",
            "05:31:46 |     dict_unktoken: __unk__\n",
            "05:31:46 |     display_add_fields: \n",
            "05:31:46 |     download_path: None\n",
            "05:31:46 |     dropout: 0.0\n",
            "05:31:46 |     dynamic_batching: None\n",
            "05:31:46 |     embedding_projection: random\n",
            "05:31:46 |     embedding_size: 300\n",
            "05:31:46 |     embedding_type: random\n",
            "05:31:46 |     embeddings_scale: True\n",
            "05:31:46 |     encode_candidate_vecs: True\n",
            "05:31:46 |     encode_candidate_vecs_batchsize: 256\n",
            "05:31:46 |     eval_candidates: inline\n",
            "05:31:46 |     ffn_size: 300\n",
            "05:31:46 |     fixed_candidate_vecs: reuse\n",
            "05:31:46 |     fixed_candidates_path: None\n",
            "05:31:46 |     force_fp16_tokens: False\n",
            "05:31:46 |     fp16: False\n",
            "05:31:46 |     fp16_impl: safe\n",
            "05:31:46 |     gpu: -1\n",
            "05:31:46 |     gradient_clip: 0.1\n",
            "05:31:46 |     hide_labels: False\n",
            "05:31:46 |     history_add_global_end_token: None\n",
            "05:31:46 |     history_reversed: False\n",
            "05:31:46 |     history_size: -1\n",
            "05:31:46 |     ignore_agent_reply: True\n",
            "05:31:46 |     ignore_bad_candidates: False\n",
            "05:31:46 |     image_cropsize: 224\n",
            "05:31:46 |     image_mode: raw\n",
            "05:31:46 |     image_size: 256\n",
            "05:31:46 |     inference: max\n",
            "05:31:46 |     init_model: None\n",
            "05:31:46 |     init_opt: None\n",
            "05:31:46 |     interactive_candidates: fixed\n",
            "05:31:46 |     interactive_mode: False\n",
            "05:31:46 |     invsqrt_lr_decay_gamma: -1\n",
            "05:31:46 |     is_debug: False\n",
            "05:31:46 |     label_truncate: None\n",
            "05:31:46 |     learn_embeddings: True\n",
            "05:31:46 |     learn_positional_embeddings: False\n",
            "05:31:46 |     learningrate: 0.0001\n",
            "05:31:46 |     loglevel: info\n",
            "05:31:46 |     lr_scheduler: reduceonplateau\n",
            "05:31:46 |     lr_scheduler_decay: 0.5\n",
            "05:31:46 |     lr_scheduler_patience: 3\n",
            "05:31:46 |     max_display_len: 1000\n",
            "05:31:46 |     memory_attention: sqrt\n",
            "05:31:46 |     model: None\n",
            "05:31:46 |     model_file: /usr/local/lib/python3.8/dist-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n",
            "05:31:46 |     model_parallel: False\n",
            "05:31:46 |     momentum: 0\n",
            "05:31:46 |     multitask_weights: [1]\n",
            "05:31:46 |     mutators: None\n",
            "05:31:46 |     n_decoder_layers: -1\n",
            "05:31:46 |     n_encoder_layers: -1\n",
            "05:31:46 |     n_heads: 2\n",
            "05:31:46 |     n_layers: 2\n",
            "05:31:46 |     n_positions: None\n",
            "05:31:46 |     n_segments: 0\n",
            "05:31:46 |     nesterov: True\n",
            "05:31:46 |     no_cuda: False\n",
            "05:31:46 |     normalize_sent_emb: False\n",
            "05:31:46 |     num_examples: 10\n",
            "05:31:46 |     nus: (0.7,)\n",
            "05:31:46 |     optimizer: adam\n",
            "05:31:46 |     output_scaling: 1.0\n",
            "05:31:46 |     override: \"{'optimizer': 'adam'}\"\n",
            "05:31:46 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
            "05:31:46 |     person_tokens: False\n",
            "05:31:46 |     rank_candidates: False\n",
            "05:31:46 |     rank_top_k: -1\n",
            "05:31:46 |     reduction_type: mean\n",
            "05:31:46 |     relu_dropout: 0.0\n",
            "05:31:46 |     repeat_blocking_heuristic: True\n",
            "05:31:46 |     return_cand_scores: False\n",
            "05:31:46 |     share_encoders: True\n",
            "05:31:46 |     share_word_embeddings: True\n",
            "05:31:46 |     special_tok_lst: None\n",
            "05:31:46 |     split_lines: False\n",
            "05:31:46 |     starttime: Jan12_05-31\n",
            "05:31:46 |     task: personachat\n",
            "05:31:46 |     text_truncate: None\n",
            "05:31:46 |     topk: 5\n",
            "05:31:46 |     train_predict: False\n",
            "05:31:46 |     truncate: 1024\n",
            "05:31:46 |     update_freq: 1\n",
            "05:31:46 |     use_memories: False\n",
            "05:31:46 |     use_reply: label\n",
            "05:31:46 |     variant: aiayn\n",
            "05:31:46 |     verbose: False\n",
            "05:31:46 |     warmup_rate: 0.0001\n",
            "05:31:46 |     warmup_updates: -1\n",
            "05:31:46 |     weight_decay: None\n",
            "05:31:46 |     wrap_memory_encoder: False\n",
            "05:31:47 | creating task(s): personachat\n",
            "05:31:47 | loading fbdialog data: /usr/local/lib/python3.8/dist-packages/data/Persona-Chat/personachat/train_self_original.txt\n",
            "\u001b[1;31m- - - NEW EPISODE: personachat - - -\u001b[0;0m\n",
            "\u001b[0myour persona: i like to remodel homes.\n",
            "your persona: i like to go hunting.\n",
            "your persona: i like to shoot a bow.\n",
            "your persona: my favorite holiday is halloween.\n",
            "hi , how are you doing ? i am getting ready to do some cheetah chasing to stay in shape .\u001b[0;0m\n",
            "   \u001b[1;94myou must be very fast . hunting is one of my favorite hobbies .\u001b[0;0m\n",
            "\u001b[0mi am ! for my hobby i like to do canning or some whittling .\u001b[0;0m\n",
            "   \u001b[1;94mi also remodel homes when i am not out bow hunting .\u001b[0;0m\n",
            "\u001b[0mthat is neat . when i was in high school i placed 6th in 100m dash !\u001b[0;0m\n",
            "   \u001b[1;94mthat is awesome . do you have a favorite season or time of year ?\u001b[0;0m\n",
            "\u001b[0mi do not . but i do have a favorite meat since that is all i eat exclusively .\u001b[0;0m\n",
            "   \u001b[1;94mwhat is your favorite meat to eat ?\u001b[0;0m\n",
            "\u001b[0mi would have to say its prime rib . do you have any favorite foods ?\u001b[0;0m\n",
            "   \u001b[1;94mi like chicken or macaroni and cheese .\u001b[0;0m\n",
            "\u001b[0mdo you have anything planned for today ? i think i am going to do some canning .\u001b[0;0m\n",
            "   \u001b[1;94mi am going to watch football . what are you canning ?\u001b[0;0m\n",
            "\u001b[0mi think i will can some jam . do you also play footfall for fun ?\u001b[0;0m\n",
            "   \u001b[1;94mif i have time outside of hunting and remodeling homes . which is not much !\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: personachat - - -\u001b[0;0m\n",
            "\u001b[0myour persona: my mom is my best friend.\n",
            "your persona: i have four sisters.\n",
            "your persona: i believe that mermaids are real.\n",
            "your persona: i love iced tea.\n",
            "hi , how are you doing today ?\u001b[0;0m\n",
            "   \u001b[1;94mi am spending time with my 4 sisters what are you up to\u001b[0;0m\n",
            "\u001b[0mwow , four sisters . just watching game of thrones .\u001b[0;0m\n",
            "   \u001b[1;94mthat is a good show i watch that while drinking iced tea\u001b[0;0m\n",
            "\u001b[0mi agree . what do you do for a living ?\u001b[0;0m\n",
            "   \u001b[1;94mi am a researcher i am researching the fact that mermaids are real\u001b[0;0m\n",
            "05:31:48 | loaded 8939 episodes with a total of 65719 examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 복습"
      ],
      "metadata": {
        "id": "bRXS5_RRtlCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://parl.ai/docs/index.html"
      ],
      "metadata": {
        "id": "lxI1Unaex1lK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!parlai --help"
      ],
      "metadata": {
        "id": "MDVfZcMaw7A8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33140d41-6c7a-4a7f-d865-d24e2c7b6e70"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: parlai [--help] [--helpall] [--version] COMMAND ...\n",
            "\n",
            "\u001b[0;31m       _\u001b[0;0m\n",
            "\u001b[0;31m      /\u001b[0;0m\u001b[0;90m\"\u001b[0;0m\u001b[0;93m)\u001b[0;0m\n",
            "\u001b[0;31m     //\u001b[0;0m\u001b[0;93m)\u001b[0;0m\n",
            "\u001b[0;32m  ==\u001b[0;0m\u001b[0;34m/\u001b[0;0m\u001b[0;31m/\u001b[0;0m\u001b[0;93m'\u001b[0;0m\u001b[0;32m===\u001b[0;0m ParlAI\n",
            "\u001b[0;34m   /\u001b[0;0m\n",
            "\n",
            "optional arguments:\n",
            "  --help, --h                show this help message and exit\n",
            "  --helpall                  List all commands, including advanced ones.\n",
            "  --version                  Prints version info and exit.\n",
            "\n",
            "Commands:\n",
            "  \n",
            "  eval_model (em, eval)      Evaluate a model\n",
            "  display_data (dd)          Display data from a task\n",
            "  display_model (dm)         Display model predictions.\n",
            "  tod_world_script           World for chatting with the TOD conversation structure\n",
            "  train_model (tm, train)    Train a model\n",
            "  generate_model_card (gmc)  Evaluate a model\n",
            "  interactive (i)            Interactive chat with a model on the command line\n",
            "  safe_interactive           Like interactive, but adds a safety filter\n",
            "  self_chat                  Generate self-chats of a model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터 불러오기"
      ],
      "metadata": {
        "id": "njgE0wBFttaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from parlai.scripts.display_data import DisplayData\n",
        "DisplayData.main(task='empathetic_dialogues', num_examples=5)"
      ],
      "metadata": {
        "id": "0koirRvVtnLR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "747ffab1-d942-48b5-a902-10e0ab404789"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "05:32:15 | Opt:\n",
            "05:32:15 |     allow_missing_init_opts: False\n",
            "05:32:15 |     batchsize: 1\n",
            "05:32:15 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
            "05:32:15 |     datatype: train:ordered\n",
            "05:32:15 |     dict_class: None\n",
            "05:32:15 |     display_add_fields: \n",
            "05:32:15 |     download_path: None\n",
            "05:32:15 |     dynamic_batching: None\n",
            "05:32:15 |     hide_labels: False\n",
            "05:32:15 |     ignore_agent_reply: True\n",
            "05:32:15 |     image_cropsize: 224\n",
            "05:32:15 |     image_mode: raw\n",
            "05:32:15 |     image_size: 256\n",
            "05:32:15 |     init_model: None\n",
            "05:32:15 |     init_opt: None\n",
            "05:32:15 |     is_debug: False\n",
            "05:32:15 |     loglevel: info\n",
            "05:32:15 |     max_display_len: 1000\n",
            "05:32:15 |     model: None\n",
            "05:32:15 |     model_file: None\n",
            "05:32:15 |     multitask_weights: [1]\n",
            "05:32:15 |     mutators: None\n",
            "05:32:15 |     num_examples: 5\n",
            "05:32:15 |     override: \"{'task': 'empathetic_dialogues', 'num_examples': 5}\"\n",
            "05:32:15 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
            "05:32:15 |     starttime: Jan12_05-32\n",
            "05:32:15 |     task: empathetic_dialogues\n",
            "05:32:15 |     train_experiencer_only: False\n",
            "05:32:15 |     verbose: False\n",
            "05:32:16 | creating task(s): empathetic_dialogues\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues - - -\u001b[0;0m\n",
            "\u001b[0mI remember going to see the fireworks with my best friend. It was the first time we ever spent time alone together. Although there was a lot of people, we felt like the only people in the world.\u001b[0;0m\n",
            "   \u001b[1;94mWas this a friend you were in love with, or just a best friend?\u001b[0;0m\n",
            "\u001b[0mThis was a best friend. I miss her.\u001b[0;0m\n",
            "   \u001b[1;94mWhere has she gone?\u001b[0;0m\n",
            "\u001b[0mWe no longer talk.\u001b[0;0m\n",
            "   \u001b[1;94mOh was this something that happened because of an argument?\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues - - -\u001b[0;0m\n",
            "\u001b[0mWas this a friend you were in love with, or just a best friend?\u001b[0;0m\n",
            "   \u001b[1;94mThis was a best friend. I miss her.\u001b[0;0m\n",
            "\u001b[0mWhere has she gone?\u001b[0;0m\n",
            "   \u001b[1;94mWe no longer talk.\u001b[0;0m\n",
            "05:32:16 | loaded 39057 episodes with a total of 64636 examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!parlai display_data --task empathetic_dialogues --num-examples 5"
      ],
      "metadata": {
        "id": "brRu3q47tun4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceaf7cc9-4eec-444b-882a-0efce304c9f4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "05:32:29 | Opt:\n",
            "05:32:29 |     allow_missing_init_opts: False\n",
            "05:32:29 |     batchsize: 1\n",
            "05:32:29 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
            "05:32:29 |     datatype: train:ordered\n",
            "05:32:29 |     dict_class: None\n",
            "05:32:29 |     display_add_fields: \n",
            "05:32:29 |     download_path: None\n",
            "05:32:29 |     dynamic_batching: None\n",
            "05:32:29 |     hide_labels: False\n",
            "05:32:29 |     ignore_agent_reply: True\n",
            "05:32:29 |     image_cropsize: 224\n",
            "05:32:29 |     image_mode: raw\n",
            "05:32:29 |     image_size: 256\n",
            "05:32:29 |     init_model: None\n",
            "05:32:29 |     init_opt: None\n",
            "05:32:29 |     is_debug: False\n",
            "05:32:29 |     loglevel: info\n",
            "05:32:29 |     max_display_len: 1000\n",
            "05:32:29 |     model: None\n",
            "05:32:29 |     model_file: None\n",
            "05:32:29 |     multitask_weights: [1]\n",
            "05:32:29 |     mutators: None\n",
            "05:32:29 |     num_examples: 5\n",
            "05:32:29 |     override: \"{'task': 'empathetic_dialogues', 'num_examples': 5}\"\n",
            "05:32:29 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
            "05:32:29 |     starttime: Jan12_05-32\n",
            "05:32:29 |     task: empathetic_dialogues\n",
            "05:32:29 |     train_experiencer_only: False\n",
            "05:32:29 |     verbose: False\n",
            "05:32:30 | creating task(s): empathetic_dialogues\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues - - -\u001b[0;0m\n",
            "\u001b[0mI remember going to see the fireworks with my best friend. It was the first time we ever spent time alone together. Although there was a lot of people, we felt like the only people in the world.\u001b[0;0m\n",
            "   \u001b[1;94mWas this a friend you were in love with, or just a best friend?\u001b[0;0m\n",
            "\u001b[0mThis was a best friend. I miss her.\u001b[0;0m\n",
            "   \u001b[1;94mWhere has she gone?\u001b[0;0m\n",
            "\u001b[0mWe no longer talk.\u001b[0;0m\n",
            "   \u001b[1;94mOh was this something that happened because of an argument?\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues - - -\u001b[0;0m\n",
            "\u001b[0mWas this a friend you were in love with, or just a best friend?\u001b[0;0m\n",
            "   \u001b[1;94mThis was a best friend. I miss her.\u001b[0;0m\n",
            "\u001b[0mWhere has she gone?\u001b[0;0m\n",
            "   \u001b[1;94mWe no longer talk.\u001b[0;0m\n",
            "05:32:30 | loaded 39057 episodes with a total of 64636 examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델 학습하기 1"
      ],
      "metadata": {
        "id": "3GsQUET7t3Xk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf from_scratch_empathetic_dialogues\n",
        "!mkdir -p from_scratch_empathetic_dialogues\n",
        "\n",
        "from parlai.scripts.train_model import TrainModel\n",
        "TrainModel.main(\n",
        "    model_file='from_scratch_empathetic_dialogues/model',\n",
        "    task='empathetic_dialogues',\n",
        "    max_train_time=60,\n",
        "    batchsize=16,\n",
        "    #모델옵션\n",
        "    model='seq2seq',\n",
        "    attention='dot',\n",
        "    lookuptable='all',\n",
        "    truncate=64,\n",
        "    #skip gerneration을 안할 경우 시간이 더 오래걸림\n",
        ")"
      ],
      "metadata": {
        "id": "FJ3b3FghtySw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b1bb8ed-8f79-4d90-fdae-e89bad14cd8d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "05:32:41 | building dictionary first...\n",
            "05:32:41 | Opt:\n",
            "05:32:41 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "05:32:41 |     adam_eps: 1e-08\n",
            "05:32:41 |     add_p1_after_newln: False\n",
            "05:32:41 |     aggregate_micro: False\n",
            "05:32:41 |     allow_missing_init_opts: False\n",
            "05:32:41 |     attention: dot\n",
            "05:32:41 |     attention_length: 48\n",
            "05:32:41 |     attention_time: post\n",
            "05:32:41 |     batchsize: 1\n",
            "05:32:41 |     beam_block_full_context: True\n",
            "05:32:41 |     beam_block_list_filename: None\n",
            "05:32:41 |     beam_block_ngram: -1\n",
            "05:32:41 |     beam_context_block_ngram: -1\n",
            "05:32:41 |     beam_delay: 30\n",
            "05:32:41 |     beam_length_penalty: 0.65\n",
            "05:32:41 |     beam_min_length: 1\n",
            "05:32:41 |     beam_size: 1\n",
            "05:32:41 |     betas: '(0.9, 0.999)'\n",
            "05:32:41 |     bidirectional: False\n",
            "05:32:41 |     bpe_add_prefix_space: None\n",
            "05:32:41 |     bpe_debug: False\n",
            "05:32:41 |     bpe_dropout: None\n",
            "05:32:41 |     bpe_merge: None\n",
            "05:32:41 |     bpe_vocab: None\n",
            "05:32:41 |     compute_tokenized_bleu: False\n",
            "05:32:41 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
            "05:32:41 |     datatype: train\n",
            "05:32:41 |     decoder: same\n",
            "05:32:41 |     delimiter: '\\n'\n",
            "05:32:41 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "05:32:41 |     dict_endtoken: __end__\n",
            "05:32:41 |     dict_file: from_scratch_empathetic_dialogues/model.dict\n",
            "05:32:41 |     dict_include_test: False\n",
            "05:32:41 |     dict_include_valid: False\n",
            "05:32:41 |     dict_initpath: None\n",
            "05:32:41 |     dict_language: english\n",
            "05:32:41 |     dict_loaded: False\n",
            "05:32:41 |     dict_lower: False\n",
            "05:32:41 |     dict_max_ngram_size: -1\n",
            "05:32:41 |     dict_maxexs: -1\n",
            "05:32:41 |     dict_maxtokens: -1\n",
            "05:32:41 |     dict_minfreq: 0\n",
            "05:32:41 |     dict_nulltoken: __null__\n",
            "05:32:41 |     dict_starttoken: __start__\n",
            "05:32:41 |     dict_textfields: text,labels\n",
            "05:32:41 |     dict_tokenizer: re\n",
            "05:32:41 |     dict_unktoken: __unk__\n",
            "05:32:41 |     display_examples: False\n",
            "05:32:41 |     download_path: None\n",
            "05:32:41 |     dropout: 0.1\n",
            "05:32:41 |     dynamic_batching: None\n",
            "05:32:41 |     embedding_projection: random\n",
            "05:32:41 |     embedding_type: random\n",
            "05:32:41 |     embeddingsize: 128\n",
            "05:32:41 |     eval_batchsize: None\n",
            "05:32:41 |     eval_dynamic_batching: None\n",
            "05:32:41 |     evaltask: None\n",
            "05:32:41 |     final_extra_opt: \n",
            "05:32:41 |     force_fp16_tokens: False\n",
            "05:32:41 |     fp16: False\n",
            "05:32:41 |     fp16_impl: safe\n",
            "05:32:41 |     gpu: -1\n",
            "05:32:41 |     gradient_clip: 0.1\n",
            "05:32:41 |     hiddensize: 128\n",
            "05:32:41 |     hide_labels: False\n",
            "05:32:41 |     history_add_global_end_token: None\n",
            "05:32:41 |     history_reversed: False\n",
            "05:32:41 |     history_size: -1\n",
            "05:32:41 |     image_cropsize: 224\n",
            "05:32:41 |     image_mode: no_image_model\n",
            "05:32:41 |     image_size: 256\n",
            "05:32:41 |     inference: greedy\n",
            "05:32:41 |     init_model: None\n",
            "05:32:41 |     init_opt: None\n",
            "05:32:41 |     input_dropout: 0.0\n",
            "05:32:41 |     interactive_mode: False\n",
            "05:32:41 |     invsqrt_lr_decay_gamma: -1\n",
            "05:32:41 |     is_debug: False\n",
            "05:32:41 |     label_truncate: None\n",
            "05:32:41 |     learningrate: 1\n",
            "05:32:41 |     load_from_checkpoint: True\n",
            "05:32:41 |     log_every_n_secs: -1\n",
            "05:32:41 |     log_every_n_steps: 50\n",
            "05:32:41 |     log_keep_fields: all\n",
            "05:32:41 |     loglevel: info\n",
            "05:32:41 |     lookuptable: all\n",
            "05:32:41 |     lr_scheduler: reduceonplateau\n",
            "05:32:41 |     lr_scheduler_decay: 0.5\n",
            "05:32:41 |     lr_scheduler_patience: 3\n",
            "05:32:41 |     max_train_steps: -1\n",
            "05:32:41 |     max_train_time: 60.0\n",
            "05:32:41 |     metrics: default\n",
            "05:32:41 |     model: seq2seq\n",
            "05:32:41 |     model_file: from_scratch_empathetic_dialogues/model\n",
            "05:32:41 |     momentum: 0\n",
            "05:32:41 |     multitask_weights: [1]\n",
            "05:32:41 |     mutators: None\n",
            "05:32:41 |     nesterov: True\n",
            "05:32:41 |     no_cuda: False\n",
            "05:32:41 |     num_epochs: -1\n",
            "05:32:41 |     num_workers: 0\n",
            "05:32:41 |     numlayers: 2\n",
            "05:32:41 |     numsoftmax: 1\n",
            "05:32:41 |     nus: (0.7,)\n",
            "05:32:41 |     optimizer: sgd\n",
            "05:32:41 |     override: \"{'model_file': 'from_scratch_empathetic_dialogues/model', 'task': 'empathetic_dialogues', 'max_train_time': 60.0, 'batchsize': 16, 'model': 'seq2seq', 'attention': 'dot', 'lookuptable': 'all', 'truncate': 64}\"\n",
            "05:32:41 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
            "05:32:41 |     person_tokens: False\n",
            "05:32:41 |     rank_candidates: False\n",
            "05:32:41 |     rnn_class: lstm\n",
            "05:32:41 |     save_after_valid: False\n",
            "05:32:41 |     save_every_n_secs: -1\n",
            "05:32:41 |     save_format: conversations\n",
            "05:32:41 |     short_final_eval: False\n",
            "05:32:41 |     skip_generation: False\n",
            "05:32:41 |     special_tok_lst: None\n",
            "05:32:41 |     split_lines: False\n",
            "05:32:41 |     starttime: Jan12_05-32\n",
            "05:32:41 |     task: empathetic_dialogues\n",
            "05:32:41 |     temperature: 1.0\n",
            "05:32:41 |     tensorboard_log: False\n",
            "05:32:41 |     tensorboard_logdir: None\n",
            "05:32:41 |     text_truncate: None\n",
            "05:32:41 |     topk: 10\n",
            "05:32:41 |     topp: 0.9\n",
            "05:32:41 |     train_experiencer_only: False\n",
            "05:32:41 |     truncate: 64\n",
            "05:32:41 |     update_freq: 1\n",
            "05:32:41 |     use_reply: label\n",
            "05:32:41 |     validation_cutoff: 1.0\n",
            "05:32:41 |     validation_every_n_epochs: -1\n",
            "05:32:41 |     validation_every_n_secs: -1\n",
            "05:32:41 |     validation_every_n_steps: -1\n",
            "05:32:41 |     validation_max_exs: -1\n",
            "05:32:41 |     validation_metric: accuracy\n",
            "05:32:41 |     validation_metric_mode: None\n",
            "05:32:41 |     validation_patience: 10\n",
            "05:32:41 |     validation_share_agent: False\n",
            "05:32:41 |     verbose: False\n",
            "05:32:41 |     wandb_entity: None\n",
            "05:32:41 |     wandb_log: False\n",
            "05:32:41 |     wandb_name: None\n",
            "05:32:41 |     wandb_project: None\n",
            "05:32:41 |     warmup_rate: 0.0001\n",
            "05:32:41 |     warmup_updates: -1\n",
            "05:32:41 |     weight_decay: None\n",
            "05:32:41 |     world_logs: \n",
            "05:32:41 | creating task(s): empathetic_dialogues\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building dictionary: 100%|██████████| 64.6k/64.6k [00:03<00:00, 19.8kex/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "05:32:45 | Saving dictionary to from_scratch_empathetic_dialogues/model.dict\n",
            "05:32:45 | dictionary built with 22419 tokens in 0.0s\n",
            "05:32:45 | No model with opt yet at: from_scratch_empathetic_dialogues/model(.opt)\n",
            "05:32:45 | Using CUDA\n",
            "05:32:45 | loading dictionary from from_scratch_empathetic_dialogues/model.dict\n",
            "05:32:45 | num words = 22419\n",
            "05:32:50 | Total parameters: 3,453,203 (3,453,203 trainable)\n",
            "05:32:50 | Opt:\n",
            "05:32:50 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "05:32:50 |     adam_eps: 1e-08\n",
            "05:32:50 |     add_p1_after_newln: False\n",
            "05:32:50 |     aggregate_micro: False\n",
            "05:32:50 |     allow_missing_init_opts: False\n",
            "05:32:50 |     attention: dot\n",
            "05:32:50 |     attention_length: 48\n",
            "05:32:50 |     attention_time: post\n",
            "05:32:50 |     batchsize: 16\n",
            "05:32:50 |     beam_block_full_context: True\n",
            "05:32:50 |     beam_block_list_filename: None\n",
            "05:32:50 |     beam_block_ngram: -1\n",
            "05:32:50 |     beam_context_block_ngram: -1\n",
            "05:32:50 |     beam_delay: 30\n",
            "05:32:50 |     beam_length_penalty: 0.65\n",
            "05:32:50 |     beam_min_length: 1\n",
            "05:32:50 |     beam_size: 1\n",
            "05:32:50 |     betas: '(0.9, 0.999)'\n",
            "05:32:50 |     bidirectional: False\n",
            "05:32:50 |     bpe_add_prefix_space: None\n",
            "05:32:50 |     bpe_debug: False\n",
            "05:32:50 |     bpe_dropout: None\n",
            "05:32:50 |     bpe_merge: None\n",
            "05:32:50 |     bpe_vocab: None\n",
            "05:32:50 |     compute_tokenized_bleu: False\n",
            "05:32:50 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
            "05:32:50 |     datatype: train\n",
            "05:32:50 |     decoder: same\n",
            "05:32:50 |     delimiter: '\\n'\n",
            "05:32:50 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "05:32:50 |     dict_endtoken: __end__\n",
            "05:32:50 |     dict_file: from_scratch_empathetic_dialogues/model.dict\n",
            "05:32:50 |     dict_include_test: False\n",
            "05:32:50 |     dict_include_valid: False\n",
            "05:32:50 |     dict_initpath: None\n",
            "05:32:50 |     dict_language: english\n",
            "05:32:50 |     dict_loaded: True\n",
            "05:32:50 |     dict_lower: False\n",
            "05:32:50 |     dict_max_ngram_size: -1\n",
            "05:32:50 |     dict_maxexs: -1\n",
            "05:32:50 |     dict_maxtokens: -1\n",
            "05:32:50 |     dict_minfreq: 0\n",
            "05:32:50 |     dict_nulltoken: __null__\n",
            "05:32:50 |     dict_starttoken: __start__\n",
            "05:32:50 |     dict_textfields: text,labels\n",
            "05:32:50 |     dict_tokenizer: re\n",
            "05:32:50 |     dict_unktoken: __unk__\n",
            "05:32:50 |     display_examples: False\n",
            "05:32:50 |     download_path: None\n",
            "05:32:50 |     dropout: 0.1\n",
            "05:32:50 |     dynamic_batching: None\n",
            "05:32:50 |     embedding_projection: random\n",
            "05:32:50 |     embedding_type: random\n",
            "05:32:50 |     embeddingsize: 128\n",
            "05:32:50 |     eval_batchsize: None\n",
            "05:32:50 |     eval_dynamic_batching: None\n",
            "05:32:50 |     evaltask: None\n",
            "05:32:50 |     final_extra_opt: \n",
            "05:32:50 |     force_fp16_tokens: False\n",
            "05:32:50 |     fp16: False\n",
            "05:32:50 |     fp16_impl: safe\n",
            "05:32:50 |     gpu: -1\n",
            "05:32:50 |     gradient_clip: 0.1\n",
            "05:32:50 |     hiddensize: 128\n",
            "05:32:50 |     hide_labels: False\n",
            "05:32:50 |     history_add_global_end_token: None\n",
            "05:32:50 |     history_reversed: False\n",
            "05:32:50 |     history_size: -1\n",
            "05:32:50 |     image_cropsize: 224\n",
            "05:32:50 |     image_mode: raw\n",
            "05:32:50 |     image_size: 256\n",
            "05:32:50 |     inference: greedy\n",
            "05:32:50 |     init_model: None\n",
            "05:32:50 |     init_opt: None\n",
            "05:32:50 |     input_dropout: 0.0\n",
            "05:32:50 |     interactive_mode: False\n",
            "05:32:50 |     invsqrt_lr_decay_gamma: -1\n",
            "05:32:50 |     is_debug: False\n",
            "05:32:50 |     label_truncate: None\n",
            "05:32:50 |     learningrate: 1\n",
            "05:32:50 |     load_from_checkpoint: True\n",
            "05:32:50 |     log_every_n_secs: -1\n",
            "05:32:50 |     log_every_n_steps: 50\n",
            "05:32:50 |     log_keep_fields: all\n",
            "05:32:50 |     loglevel: info\n",
            "05:32:50 |     lookuptable: all\n",
            "05:32:50 |     lr_scheduler: reduceonplateau\n",
            "05:32:50 |     lr_scheduler_decay: 0.5\n",
            "05:32:50 |     lr_scheduler_patience: 3\n",
            "05:32:50 |     max_train_steps: -1\n",
            "05:32:50 |     max_train_time: 60.0\n",
            "05:32:50 |     metrics: default\n",
            "05:32:50 |     model: seq2seq\n",
            "05:32:50 |     model_file: from_scratch_empathetic_dialogues/model\n",
            "05:32:50 |     momentum: 0\n",
            "05:32:50 |     multitask_weights: [1]\n",
            "05:32:50 |     mutators: None\n",
            "05:32:50 |     nesterov: True\n",
            "05:32:50 |     no_cuda: False\n",
            "05:32:50 |     num_epochs: -1\n",
            "05:32:50 |     num_workers: 0\n",
            "05:32:50 |     numlayers: 2\n",
            "05:32:50 |     numsoftmax: 1\n",
            "05:32:50 |     nus: (0.7,)\n",
            "05:32:50 |     optimizer: sgd\n",
            "05:32:50 |     override: \"{'model_file': 'from_scratch_empathetic_dialogues/model', 'task': 'empathetic_dialogues', 'max_train_time': 60.0, 'batchsize': 16, 'model': 'seq2seq', 'attention': 'dot', 'lookuptable': 'all', 'truncate': 64}\"\n",
            "05:32:50 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
            "05:32:50 |     person_tokens: False\n",
            "05:32:50 |     rank_candidates: False\n",
            "05:32:50 |     rnn_class: lstm\n",
            "05:32:50 |     save_after_valid: False\n",
            "05:32:50 |     save_every_n_secs: -1\n",
            "05:32:50 |     save_format: conversations\n",
            "05:32:50 |     short_final_eval: False\n",
            "05:32:50 |     skip_generation: False\n",
            "05:32:50 |     special_tok_lst: None\n",
            "05:32:50 |     split_lines: False\n",
            "05:32:50 |     starttime: Jan12_05-32\n",
            "05:32:50 |     task: empathetic_dialogues\n",
            "05:32:50 |     temperature: 1.0\n",
            "05:32:50 |     tensorboard_log: False\n",
            "05:32:50 |     tensorboard_logdir: None\n",
            "05:32:50 |     text_truncate: None\n",
            "05:32:50 |     topk: 10\n",
            "05:32:50 |     topp: 0.9\n",
            "05:32:50 |     train_experiencer_only: False\n",
            "05:32:50 |     truncate: 64\n",
            "05:32:50 |     update_freq: 1\n",
            "05:32:50 |     use_reply: label\n",
            "05:32:50 |     validation_cutoff: 1.0\n",
            "05:32:50 |     validation_every_n_epochs: -1\n",
            "05:32:50 |     validation_every_n_secs: -1\n",
            "05:32:50 |     validation_every_n_steps: -1\n",
            "05:32:50 |     validation_max_exs: -1\n",
            "05:32:50 |     validation_metric: accuracy\n",
            "05:32:50 |     validation_metric_mode: None\n",
            "05:32:50 |     validation_patience: 10\n",
            "05:32:50 |     validation_share_agent: False\n",
            "05:32:50 |     verbose: False\n",
            "05:32:50 |     wandb_entity: None\n",
            "05:32:50 |     wandb_log: False\n",
            "05:32:50 |     wandb_name: None\n",
            "05:32:50 |     wandb_project: None\n",
            "05:32:50 |     warmup_rate: 0.0001\n",
            "05:32:50 |     warmup_updates: -1\n",
            "05:32:50 |     weight_decay: None\n",
            "05:32:50 |     world_logs: \n",
            "05:32:50 | creating task(s): empathetic_dialogues\n",
            "05:32:51 | training...\n",
            "05:32:56 | time:4s total_exs:800 total_steps:50 epochs:0.01\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   30.21     1   445  5306  .09625      2.401 190.3  800  1.002   .02029  16.1 9.304   1 256.7  3061  .00375     .06125 10984   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "       .06872         0                   50 701.6 8367 11.93\n",
            "\n",
            "05:32:59 | time:8s total_exs:1600 total_steps:100 epochs:0.02\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   30.35     1 452.5  6952   .0925      2.069 245.8  800  1.006   .02029  16.1 8.925   1 254.6  3912   .0050      .1850 7518   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps   ups  \n",
            "       .09174         0                  100 707.1 10864 15.37\n",
            "\n",
            "05:33:02 | time:11s total_exs:2400 total_steps:150 epochs:0.04\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   30.26     1 452.9  7020  .08375      1.956   248  800  1.065   .02034 16.18 8.647   1 258.4  4005   .0025      .0275 5694   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps   ups  \n",
            "        .1136         0                  150 711.3 11025 15.51\n",
            "\n",
            "05:33:06 | time:14s total_exs:3200 total_steps:200 epochs:0.05\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   31.57     1 450.2  6668   .1100      3.434   237  800  1.092   .02027 16.51  8.45   1 263.6  3904   .0050      .0400 4675   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps   ups  \n",
            "        .1301         0                  200 713.7 10572 14.82\n",
            "\n",
            "05:33:09 | time:18s total_exs:4000 total_steps:250 epochs:0.06\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   29.55     1 448.1  6893  .07625      1.539 246.1  800  1.106   .02031 16.76 8.335   1 267.9  4120   .0025     .02125 4169   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "        .1350         0                  250  716 11014 15.39\n",
            "\n",
            "05:33:12 | time:21s total_exs:4800 total_steps:300 epochs:0.07\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "    29.7     1 449.1  6819  .08875      1.627 242.9  800  1.135    .0203 16.33 8.176   1 260.8  3959  .00375     .03375 3554   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps   ups  \n",
            "        .1407         0                  300 709.9 10778 15.19\n",
            "\n",
            "05:33:15 | time:24s total_exs:5600 total_steps:350 epochs:0.09\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   29.81     1   450  6885   .0725      1.685 244.8  800  1.154   .02031 15.73 8.051   1   251  3840  .00375      .0450 3136   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "        .1454         0                  350  701 10724 15.3\n",
            "\n",
            "05:33:19 | time:28s total_exs:6400 total_steps:400 epochs:0.10\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   28.98     1 438.6  6630   .0850      1.569 241.8  800  1.154   .02025  16.2 7.969   1 258.2  3903   .0025     .05875 2890   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps   ups  \n",
            "        .1444         0                  400 696.9 10533 15.12\n",
            "\n",
            "05:33:22 | time:31s total_exs:7200 total_steps:450 epochs:0.11\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "    29.2     1 438.8  6807  .08625      1.778 248.2  800  1.174   .01962 15.86 7.862   1 253.7  3937       0          0 2596   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps   ups  \n",
            "        .1476         0                  450 692.5 10744 15.52\n",
            "\n",
            "05:33:25 | time:34s total_exs:8000 total_steps:500 epochs:0.12\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   29.55     1 445.8  6754  .09875      1.683 242.4  800  1.184   .02031 16.12 7.761   1 257.6  3902  .00125      .0275 2347   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps   ups  \n",
            "        .1531         0                  500 703.4 10656 15.15\n",
            "\n",
            "05:33:29 | time:37s total_exs:8800 total_steps:550 epochs:0.14\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "    30.6     1 446.5  6946   .0925      2.692 248.9  800  1.217   .02026 15.69 7.606   1 251.1  3906  .00125     .00125 2010   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps   ups  \n",
            "        .1656         0                  550 697.6 10852 15.56\n",
            "\n",
            "05:33:32 | time:41s total_exs:9600 total_steps:600 epochs:0.15\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   31.07     1 459.7  6933   .0950      2.336 241.3  800  1.247   .02029 15.99 7.641   1 255.6  3855  .00125      .0125 2081   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps   ups  \n",
            "        .1537         0                  600 715.4 10788 15.09\n",
            "\n",
            "05:33:35 | time:44s total_exs:10400 total_steps:650 epochs:0.16\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   29.93     1 446.2  6972   .0950      2.047   250  800  1.206   .02035 16.51 7.481   1 262.8  4106  .00375      .0825 1774   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "        .1674         0                  650  709 11078 15.63\n",
            "\n",
            "05:33:38 | time:47s total_exs:11200 total_steps:700 epochs:0.17\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   28.84     1 433.1  6878   .0825      1.774   254  800  1.251   .02026 15.93 7.435   1 254.3  4038   .0025     .03375 1694   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps   ups  \n",
            "        .1674         0                  700 687.4 10915 15.88\n",
            "\n",
            "05:33:41 | time:50s total_exs:12000 total_steps:750 epochs:0.19\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   30.01     1 451.2  7322   .0925      1.806 259.6  800  1.225   .02036 15.89 7.425   1 253.8  4118   .0025      .0325 1678   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "        .1682         0                  750  705 11440 16.23\n",
            "\n",
            "05:33:44 | time:53s total_exs:12800 total_steps:800 epochs:0.20\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   29.12     1 439.9  7024  .09375      1.631 255.5  800  1.269   .02032 15.75 7.338   1 251.9  4022   .0025      .0100 1538   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps   ups  \n",
            "        .1803         0                  800 691.7 11046 15.97\n",
            "\n",
            "05:33:48 | time:57s total_exs:13600 total_steps:850 epochs:0.21\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "      29     1 434.5  6916   .0900      1.849 254.7  800  1.255   .02032 15.91 7.349   1 254.3  4048  .00375     .01875 1554   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps   ups  \n",
            "        .1683         0                  850 688.8 10963 15.92\n",
            "\n",
            "05:33:51 | time:60s total_exs:14400 total_steps:900 epochs:0.22\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   30.76     1 457.2  7344   .0925      2.183   257  800  1.281   .02031 16.08 7.218   1   256  4113  .00625     .08125 1364   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps   ups  \n",
            "        .1751         0                  900 713.2 11457 16.07\n",
            "\n",
            "05:33:51 | max_train_time elapsed:60.03460907936096s\n",
            "05:33:51 | Using CUDA\n",
            "05:33:51 | loading dictionary from from_scratch_empathetic_dialogues/model.dict\n",
            "05:33:51 | num words = 22419\n",
            "05:33:51 | Total parameters: 3,453,203 (3,453,203 trainable)\n",
            "05:33:51 | Loading existing model params from from_scratch_empathetic_dialogues/model\n",
            "05:33:51 | creating task(s): empathetic_dialogues\n",
            "05:33:52 | running eval: valid\n",
            "05:34:45 | eval completed in 53.02s\n",
            "05:34:45 | \u001b[1mvalid:\n",
            "    accuracy    bleu-4  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  gen_n_toks  gpu_mem  llen  loss  lr  ltpb  ltps  \\\n",
            "           0 1.181e-07 39.52 572.6  3889   .1682      3.592 108.3 5738 .1034       13.95 .0009751 15.65 6.967   1 249.1  1692   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps  \n",
            "   .002091     .02457 1061      .1888         0                  906 821.7 5581\n",
            "\u001b[0m\n",
            "05:34:45 | creating task(s): empathetic_dialogues\n",
            "05:34:45 | running eval: test\n",
            "05:35:33 | eval completed in 47.17s\n",
            "05:35:33 | \u001b[1mtest:\n",
            "    accuracy    bleu-4  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  gen_n_toks  gpu_mem  llen  loss  lr  ltpb  ltps  \\\n",
            "           0 8.871e-09 42.71 604.5  4218   .1960      4.891 111.5 5259 .1027       13.59 .0009814 15.85 6.977   1 252.6  1763   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps  \n",
            "   .003613     .04982 1072      .1863         0                  906 857.1 5981\n",
            "\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'exs': SumMetric(5738),\n",
              "  'accuracy': ExactMatchMetric(0),\n",
              "  'f1': F1Metric(0.1034),\n",
              "  'bleu-4': BleuMetric(1.181e-07),\n",
              "  'clen': AverageMetric(39.52),\n",
              "  'ctrunc': AverageMetric(0.1682),\n",
              "  'ctrunclen': AverageMetric(3.592),\n",
              "  'llen': AverageMetric(15.65),\n",
              "  'ltrunc': AverageMetric(0.002091),\n",
              "  'ltrunclen': AverageMetric(0.02457),\n",
              "  'loss': AverageMetric(6.967),\n",
              "  'ppl': PPLMetric(1061),\n",
              "  'token_acc': AverageMetric(0.1888),\n",
              "  'token_em': AverageMetric(0),\n",
              "  'gen_n_toks': AverageMetric(13.95),\n",
              "  'exps': GlobalTimerMetric(108.3),\n",
              "  'ltpb': GlobalAverageMetric(249.1),\n",
              "  'ltps': GlobalTimerMetric(1692),\n",
              "  'ctpb': GlobalAverageMetric(572.6),\n",
              "  'ctps': GlobalTimerMetric(3889),\n",
              "  'tpb': GlobalAverageMetric(821.7),\n",
              "  'tps': GlobalTimerMetric(5581),\n",
              "  'lr': GlobalAverageMetric(1),\n",
              "  'gpu_mem': GlobalAverageMetric(0.0009751),\n",
              "  'total_train_updates': GlobalFixedMetric(906)},\n",
              " {'exs': SumMetric(5259),\n",
              "  'accuracy': ExactMatchMetric(0),\n",
              "  'f1': F1Metric(0.1027),\n",
              "  'bleu-4': BleuMetric(8.871e-09),\n",
              "  'clen': AverageMetric(42.71),\n",
              "  'ctrunc': AverageMetric(0.196),\n",
              "  'ctrunclen': AverageMetric(4.891),\n",
              "  'llen': AverageMetric(15.85),\n",
              "  'ltrunc': AverageMetric(0.003613),\n",
              "  'ltrunclen': AverageMetric(0.04982),\n",
              "  'loss': AverageMetric(6.977),\n",
              "  'ppl': PPLMetric(1072),\n",
              "  'token_acc': AverageMetric(0.1863),\n",
              "  'token_em': AverageMetric(0),\n",
              "  'gen_n_toks': AverageMetric(13.59),\n",
              "  'exps': GlobalTimerMetric(111.5),\n",
              "  'ltpb': GlobalAverageMetric(252.6),\n",
              "  'ltps': GlobalTimerMetric(1763),\n",
              "  'ctpb': GlobalAverageMetric(604.5),\n",
              "  'ctps': GlobalTimerMetric(4218),\n",
              "  'tpb': GlobalAverageMetric(857.1),\n",
              "  'tps': GlobalTimerMetric(5981),\n",
              "  'lr': GlobalAverageMetric(1),\n",
              "  'gpu_mem': GlobalAverageMetric(0.0009814),\n",
              "  'total_train_updates': GlobalFixedMetric(906)})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델 학습하기 2"
      ],
      "metadata": {
        "id": "d1hji6jKt5Vn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#finetunning 하여 학습시키는 것이 보편적,미리 학습된 모델에 fine tunning하여 진행\n",
        "!rm -rf from_pretrained\n",
        "!mkdir -p from_pretrained\n",
        "from parlai.scripts.train_model import TrainModel\n",
        "\n",
        "TrainModel.main(\n",
        "    task='empathetic_dialogues', \n",
        "    model='transformer/generator',\n",
        "    model_file='from_pretrained/model2',\n",
        "    \n",
        "    init_model='zoo:tutorial_transformer_generator/model',#모델경로\n",
        "    \n",
        "    n_heads=16, n_layers=8, n_positions=512, text_truncate=512,\n",
        "    label_truncate=128, ffn_size=2048, embedding_size=512,\n",
        "    activation='gelu', variant='xlm',\n",
        "    dict_lower=True, dict_tokenizer='bpe',\n",
        "    dict_file='zoo:tutorial_transformer_generator/model.dict',\n",
        "    learn_positional_embeddings=True,\n",
        "    \n",
        "    lr=1e-5, optimizer='adam',#많이 쓰이는 값을 사용\n",
        "    warmup_updates=100,\n",
        "    validation_metric='ppl', # 어떤 기준으로 학습이 잘 되고 있는지를 확인할건지\n",
        "    max_train_time=60, validation_every_n_epochs=0.25, #중간중간 validation 확인하여 모델 학습정도 확인\n",
        "    \n",
        "    batchsize=12, fp16=True, fp16_impl='mem_efficient',\n",
        "    skip_generation=True, \n",
        "    dynamic_batching='full',\n",
        ")"
      ],
      "metadata": {
        "id": "MK5h3KT-t5C2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2aec9f1-5da4-4b50-844d-9aa9b2cc316e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "05:41:18 | building dictionary first...\n",
            "05:41:18 | No model with opt yet at: from_pretrained/model2(.opt)\n",
            "05:41:18 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: full,verbose: False,is_debug: False,datapath: /usr/local/lib/python3.8/dist-packages/data,final_extra_opt: ,eval_dynamic_batching: None,num_workers: 0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_steps: -1,load_from_checkpoint: True,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,mutators: None,train_experiencer_only: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,beam_block_full_context: True,beam_length_penalty: 0.65,topk: 10,topp: 0.9,beam_delay: 30,beam_block_list_filename: None,temperature: 1.0,compute_tokenized_bleu: False,interactive_mode: False,fp16_impl: mem_efficient,force_fp16_tokens: False,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,parlai_home: /usr/local/lib/python3.8/dist-packages\u001b[0m\n",
            "05:41:18 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "--show-advanced-args False --task internal:new_reddit:presorted --datatype train:stream --numthreads 1 --batchsize 48 --num-epochs 5.0 --max-train-time -1 --validation-every-n-secs 1800.0 --save-after-valid True --validation-every-n-epochs -1 --validation-max-exs 9920 --short-final-eval True --validation-patience 0 --validation-metric-mode min --dict-build-first True --numworkers 4 --pytorch-preprocess False --pytorch-teacher-batch-sort False --batch-sort-cache-type pop --batch-length-range 5 --shuffle False --batch-sort-field text --pytorch-context-length -1 --pytorch-include-labels True --log-every-n-secs 30.0 --distributed-world-size 64 --port 61337 --dropout 0.1 --beam-size 8 --beam-min-n-best 3 --beam-min-length 10 --skip-generation False --inference beam --optimizer fused_adam --learningrate 0.0005 --gradient-clip 10.0 --adam-eps 1e-06 --betas 0.9,0.98 --weight-decay 0.01 --lr-scheduler invsqrt --warmup-updates 20000 --gpu 0 --beam-block-ngram 3 --beam-context-block-ngram 3\u001b[0m\n",
            "05:41:18 | Using CUDA\n",
            "05:41:18 | loading dictionary from /usr/local/lib/python3.8/dist-packages/data/models/tutorial_transformer_generator/model.dict\n",
            "05:41:18 | num words = 54944\n",
            "05:41:20 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "05:41:20 | Loading existing model params from /usr/local/lib/python3.8/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "05:41:21 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n",
            "05:41:21 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n",
            "05:41:22 | Opt:\n",
            "05:41:22 |     activation: gelu\n",
            "05:41:22 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "05:41:22 |     adam_eps: 1e-08\n",
            "05:41:22 |     add_p1_after_newln: False\n",
            "05:41:22 |     aggregate_micro: False\n",
            "05:41:22 |     allow_missing_init_opts: False\n",
            "05:41:22 |     attention_dropout: 0.0\n",
            "05:41:22 |     batchsize: 12\n",
            "05:41:22 |     beam_block_full_context: True\n",
            "05:41:22 |     beam_block_list_filename: None\n",
            "05:41:22 |     beam_block_ngram: -1\n",
            "05:41:22 |     beam_context_block_ngram: -1\n",
            "05:41:22 |     beam_delay: 30\n",
            "05:41:22 |     beam_length_penalty: 0.65\n",
            "05:41:22 |     beam_min_length: 1\n",
            "05:41:22 |     beam_size: 1\n",
            "05:41:22 |     betas: '(0.9, 0.999)'\n",
            "05:41:22 |     bpe_add_prefix_space: None\n",
            "05:41:22 |     bpe_debug: False\n",
            "05:41:22 |     bpe_dropout: None\n",
            "05:41:22 |     bpe_merge: None\n",
            "05:41:22 |     bpe_vocab: None\n",
            "05:41:22 |     checkpoint_activations: False\n",
            "05:41:22 |     compute_tokenized_bleu: False\n",
            "05:41:22 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
            "05:41:22 |     datatype: train\n",
            "05:41:22 |     delimiter: '\\n'\n",
            "05:41:22 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "05:41:22 |     dict_endtoken: __end__\n",
            "05:41:22 |     dict_file: /usr/local/lib/python3.8/dist-packages/data/models/tutorial_transformer_generator/model.dict\n",
            "05:41:22 |     dict_include_test: False\n",
            "05:41:22 |     dict_include_valid: False\n",
            "05:41:22 |     dict_initpath: None\n",
            "05:41:22 |     dict_language: english\n",
            "05:41:22 |     dict_loaded: True\n",
            "05:41:22 |     dict_lower: True\n",
            "05:41:22 |     dict_max_ngram_size: -1\n",
            "05:41:22 |     dict_maxexs: -1\n",
            "05:41:22 |     dict_maxtokens: -1\n",
            "05:41:22 |     dict_minfreq: 0\n",
            "05:41:22 |     dict_nulltoken: __null__\n",
            "05:41:22 |     dict_starttoken: __start__\n",
            "05:41:22 |     dict_textfields: text,labels\n",
            "05:41:22 |     dict_tokenizer: bpe\n",
            "05:41:22 |     dict_unktoken: __unk__\n",
            "05:41:22 |     display_examples: False\n",
            "05:41:22 |     download_path: None\n",
            "05:41:22 |     dropout: 0.0\n",
            "05:41:22 |     dynamic_batching: full\n",
            "05:41:22 |     embedding_projection: random\n",
            "05:41:22 |     embedding_size: 512\n",
            "05:41:22 |     embedding_type: random\n",
            "05:41:22 |     embeddings_scale: True\n",
            "05:41:22 |     eval_batchsize: None\n",
            "05:41:22 |     eval_dynamic_batching: None\n",
            "05:41:22 |     evaltask: None\n",
            "05:41:22 |     ffn_size: 2048\n",
            "05:41:22 |     final_extra_opt: \n",
            "05:41:22 |     force_fp16_tokens: False\n",
            "05:41:22 |     fp16: True\n",
            "05:41:22 |     fp16_impl: mem_efficient\n",
            "05:41:22 |     gpu: -1\n",
            "05:41:22 |     gradient_clip: 0.1\n",
            "05:41:22 |     hide_labels: False\n",
            "05:41:22 |     history_add_global_end_token: None\n",
            "05:41:22 |     history_reversed: False\n",
            "05:41:22 |     history_size: -1\n",
            "05:41:22 |     image_cropsize: 224\n",
            "05:41:22 |     image_mode: raw\n",
            "05:41:22 |     image_size: 256\n",
            "05:41:22 |     inference: greedy\n",
            "05:41:22 |     init_model: /usr/local/lib/python3.8/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "05:41:22 |     init_opt: None\n",
            "05:41:22 |     interactive_mode: False\n",
            "05:41:22 |     invsqrt_lr_decay_gamma: -1\n",
            "05:41:22 |     is_debug: False\n",
            "05:41:22 |     label_truncate: 128\n",
            "05:41:22 |     learn_positional_embeddings: True\n",
            "05:41:22 |     learningrate: 1e-05\n",
            "05:41:22 |     load_from_checkpoint: True\n",
            "05:41:22 |     log_every_n_secs: -1\n",
            "05:41:22 |     log_every_n_steps: 50\n",
            "05:41:22 |     log_keep_fields: all\n",
            "05:41:22 |     loglevel: info\n",
            "05:41:22 |     lr_scheduler: reduceonplateau\n",
            "05:41:22 |     lr_scheduler_decay: 0.5\n",
            "05:41:22 |     lr_scheduler_patience: 3\n",
            "05:41:22 |     max_train_steps: -1\n",
            "05:41:22 |     max_train_time: 60.0\n",
            "05:41:22 |     metrics: default\n",
            "05:41:22 |     model: transformer/generator\n",
            "05:41:22 |     model_file: from_pretrained/model2\n",
            "05:41:22 |     model_parallel: False\n",
            "05:41:22 |     momentum: 0\n",
            "05:41:22 |     multitask_weights: [1]\n",
            "05:41:22 |     mutators: None\n",
            "05:41:22 |     n_decoder_layers: -1\n",
            "05:41:22 |     n_encoder_layers: -1\n",
            "05:41:22 |     n_heads: 16\n",
            "05:41:22 |     n_layers: 8\n",
            "05:41:22 |     n_positions: 512\n",
            "05:41:22 |     n_segments: 0\n",
            "05:41:22 |     nesterov: True\n",
            "05:41:22 |     no_cuda: False\n",
            "05:41:22 |     num_epochs: -1\n",
            "05:41:22 |     num_workers: 0\n",
            "05:41:22 |     nus: (0.7,)\n",
            "05:41:22 |     optimizer: mem_eff_adam\n",
            "05:41:22 |     output_scaling: 1.0\n",
            "05:41:22 |     override: \"{'task': 'empathetic_dialogues', 'model': 'transformer/generator', 'model_file': 'from_pretrained/model2', 'init_model': 'zoo:tutorial_transformer_generator/model', 'n_heads': 16, 'n_layers': 8, 'n_positions': 512, 'text_truncate': 512, 'label_truncate': 128, 'ffn_size': 2048, 'embedding_size': 512, 'activation': 'gelu', 'variant': 'xlm', 'dict_lower': True, 'dict_tokenizer': 'bpe', 'dict_file': '/usr/local/lib/python3.8/dist-packages/data/models/tutorial_transformer_generator/model.dict', 'learn_positional_embeddings': True, 'learningrate': 1e-05, 'optimizer': 'adam', 'warmup_updates': 100, 'validation_metric': 'ppl', 'max_train_time': 60.0, 'validation_every_n_epochs': 0.25, 'batchsize': 12, 'fp16': True, 'fp16_impl': 'mem_efficient', 'skip_generation': True, 'dynamic_batching': 'full'}\"\n",
            "05:41:22 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
            "05:41:22 |     person_tokens: False\n",
            "05:41:22 |     rank_candidates: False\n",
            "05:41:22 |     relu_dropout: 0.0\n",
            "05:41:22 |     save_after_valid: False\n",
            "05:41:22 |     save_every_n_secs: -1\n",
            "05:41:22 |     save_format: conversations\n",
            "05:41:22 |     share_word_embeddings: True\n",
            "05:41:22 |     short_final_eval: False\n",
            "05:41:22 |     skip_generation: True\n",
            "05:41:22 |     special_tok_lst: None\n",
            "05:41:22 |     split_lines: False\n",
            "05:41:22 |     starttime: Jan12_05-41\n",
            "05:41:22 |     task: empathetic_dialogues\n",
            "05:41:22 |     temperature: 1.0\n",
            "05:41:22 |     tensorboard_log: False\n",
            "05:41:22 |     tensorboard_logdir: None\n",
            "05:41:22 |     text_truncate: 512\n",
            "05:41:22 |     topk: 10\n",
            "05:41:22 |     topp: 0.9\n",
            "05:41:22 |     train_experiencer_only: False\n",
            "05:41:22 |     truncate: -1\n",
            "05:41:22 |     update_freq: 1\n",
            "05:41:22 |     use_reply: label\n",
            "05:41:22 |     validation_cutoff: 1.0\n",
            "05:41:22 |     validation_every_n_epochs: 0.25\n",
            "05:41:22 |     validation_every_n_secs: -1\n",
            "05:41:22 |     validation_every_n_steps: -1\n",
            "05:41:22 |     validation_max_exs: -1\n",
            "05:41:22 |     validation_metric: ppl\n",
            "05:41:22 |     validation_metric_mode: None\n",
            "05:41:22 |     validation_patience: 10\n",
            "05:41:22 |     validation_share_agent: False\n",
            "05:41:22 |     variant: xlm\n",
            "05:41:22 |     verbose: False\n",
            "05:41:22 |     wandb_entity: None\n",
            "05:41:22 |     wandb_log: False\n",
            "05:41:22 |     wandb_name: None\n",
            "05:41:22 |     wandb_project: None\n",
            "05:41:22 |     warmup_rate: 0.0001\n",
            "05:41:22 |     warmup_updates: 100\n",
            "05:41:22 |     weight_decay: None\n",
            "05:41:22 |     world_logs: \n",
            "05:41:23 | creating task(s): empathetic_dialogues\n",
            "05:41:25 | training...\n",
            "05:41:26 | Overflow: setting loss scale to 65536.0\n",
            "05:41:29 | Overflow: setting loss scale to 32768.0\n",
            "05:41:40 | time:15s total_exs:5212 total_steps:50 epochs:0.08\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n",
            "   25.42 .9600  2650  9055       0          0 356.2 5212             43909  4.086    .4697 16.11 2.859 5.001e-06  1679  5737   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "         0          0 17.44      .3915  .0003837                   50 4329 14792 3.42\n",
            "\n",
            "05:41:53 | time:28s total_exs:9860 total_steps:100 epochs:0.15\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   31.76     1  2953 11094       0          0 349.3 4648             32768  4.178    .4628 16.49 2.805 9.9e-06  1532  5758   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 16.53      .4002  .0008606                  100 4485 16852 3.758\n",
            "\n",
            "05:42:07 | time:42s total_exs:14344 total_steps:150 epochs:0.22\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   32.51     1  2916 10771       0          0 331.3 4484             32768  4.232    .4530  17.3 2.729 9.9e-06  1552  5732   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 15.31      .4071   .000223                  150 4467 16502 3.695\n",
            "\n",
            "05:42:12 | time:47s total_exs:16212 total_steps:169 epochs:0.25\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   31.63     1  3110 11665       0          0 368.8 1868             32768  3.966    .4406  16.9 2.736 9.9e-06  1661  6231   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 15.43      .4103         0                  169 4771 17896 3.752\n",
            "\n",
            "05:42:12 | creating task(s): empathetic_dialogues\n",
            "05:42:14 | running eval: valid\n",
            "05:42:20 | eval completed in 6.30s\n",
            "05:42:20 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   40.45  3464 40693       0          0  1006 5738    .0901 16.01 2.533 9.9e-06  1371 16106       0          0 12.6   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .4318   .001046                  169 4835 56800\n",
            "\u001b[0m\n",
            "05:42:20 | \u001b[1;32mnew best ppl: 12.6\u001b[0m\n",
            "05:42:20 | saving best valid model: from_pretrained/model2\n",
            "05:42:20 | Saving dictionary to from_pretrained/model2.dict\n",
            "05:42:25 | max_train_time elapsed:60.29167199134827s\n",
            "05:42:25 | \u001b[33mOverriding opt[\"init_model\"] to zoo:tutorial_transformer_generator/model (previously: /usr/local/lib/python3.8/dist-packages/data/models/tutorial_transformer_generator/model)\u001b[0m\n",
            "05:42:25 | \u001b[33mOverriding opt[\"optimizer\"] to adam (previously: mem_eff_adam)\u001b[0m\n",
            "05:42:25 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: full,is_debug: False,final_extra_opt: ,eval_dynamic_batching: None,num_workers: 0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_steps: -1,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,mutators: None,train_experiencer_only: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,beam_block_full_context: True,beam_length_penalty: 0.65,topk: 10,topp: 0.9,beam_delay: 30,beam_block_list_filename: None,temperature: 1.0,compute_tokenized_bleu: False,fp16_impl: mem_efficient,force_fp16_tokens: True,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,parlai_home: /usr/local/lib/python3.8/dist-packages,dict_loaded: True,download_path: None,verbose: False,datapath: /usr/local/lib/python3.8/dist-packages/data,load_from_checkpoint: True,interactive_mode: False\u001b[0m\n",
            "05:42:25 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "--show-advanced-args False --task internal:new_reddit:presorted --datatype train:stream --numthreads 1 --batchsize 48 --num-epochs 5.0 --max-train-time -1 --validation-every-n-secs 1800.0 --save-after-valid True --validation-every-n-epochs -1 --validation-max-exs 9920 --short-final-eval True --validation-patience 0 --validation-metric-mode min --dict-build-first True --numworkers 4 --pytorch-preprocess False --pytorch-teacher-batch-sort False --batch-sort-cache-type pop --batch-length-range 5 --shuffle False --batch-sort-field text --pytorch-context-length -1 --pytorch-include-labels True --log-every-n-secs 30.0 --distributed-world-size 64 --port 61337 --dropout 0.1 --beam-size 8 --beam-min-n-best 3 --beam-min-length 10 --skip-generation False --inference beam --optimizer fused_adam --learningrate 0.0005 --gradient-clip 10.0 --adam-eps 1e-06 --betas 0.9,0.98 --weight-decay 0.01 --lr-scheduler invsqrt --warmup-updates 20000 --gpu 0 --beam-block-ngram 3 --beam-context-block-ngram 3\u001b[0m\n",
            "05:42:25 | Using CUDA\n",
            "05:42:25 | loading dictionary from from_pretrained/model2.dict\n",
            "05:42:25 | num words = 54944\n",
            "05:42:27 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "05:42:27 | Loading existing model params from from_pretrained/model2\n",
            "05:42:31 | creating task(s): empathetic_dialogues\n",
            "05:42:33 | running eval: valid\n",
            "05:42:39 | eval completed in 6.38s\n",
            "05:42:39 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   40.45  3464 38555       0          0 953.1 5738   .07835 16.01 2.533 9.9e-06  1371 15260       0          0 12.6   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .4317   .001046                  169 4835 53815\n",
            "\u001b[0m\n",
            "05:42:39 | creating task(s): empathetic_dialogues\n",
            "05:42:41 | running eval: test\n",
            "05:42:47 | eval completed in 6.16s\n",
            "05:42:47 | \u001b[1mtest:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   43.69  3647 39240       0          0 898.2 5259   .07842 16.23 2.552 9.9e-06  1355 14578       0          0 12.83   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .4297  .0001902                  169 5002 53818\n",
            "\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'exs': SumMetric(5738),\n",
              "  'clen': AverageMetric(40.45),\n",
              "  'ctrunc': AverageMetric(0),\n",
              "  'ctrunclen': AverageMetric(0),\n",
              "  'llen': AverageMetric(16.01),\n",
              "  'ltrunc': AverageMetric(0),\n",
              "  'ltrunclen': AverageMetric(0),\n",
              "  'loss': AverageMetric(2.533),\n",
              "  'ppl': PPLMetric(12.6),\n",
              "  'token_acc': AverageMetric(0.4317),\n",
              "  'token_em': AverageMetric(0.001046),\n",
              "  'exps': GlobalTimerMetric(953.1),\n",
              "  'ltpb': GlobalAverageMetric(1371),\n",
              "  'ltps': GlobalTimerMetric(1.526e+04),\n",
              "  'ctpb': GlobalAverageMetric(3464),\n",
              "  'ctps': GlobalTimerMetric(3.855e+04),\n",
              "  'tpb': GlobalAverageMetric(4835),\n",
              "  'tps': GlobalTimerMetric(5.381e+04),\n",
              "  'lr': GlobalAverageMetric(9.9e-06),\n",
              "  'gpu_mem': GlobalAverageMetric(0.07835),\n",
              "  'total_train_updates': GlobalFixedMetric(169)},\n",
              " {'exs': SumMetric(5259),\n",
              "  'clen': AverageMetric(43.69),\n",
              "  'ctrunc': AverageMetric(0),\n",
              "  'ctrunclen': AverageMetric(0),\n",
              "  'llen': AverageMetric(16.23),\n",
              "  'ltrunc': AverageMetric(0),\n",
              "  'ltrunclen': AverageMetric(0),\n",
              "  'loss': AverageMetric(2.552),\n",
              "  'ppl': PPLMetric(12.83),\n",
              "  'token_acc': AverageMetric(0.4297),\n",
              "  'token_em': AverageMetric(0.0001902),\n",
              "  'exps': GlobalTimerMetric(898.2),\n",
              "  'ltpb': GlobalAverageMetric(1355),\n",
              "  'ltps': GlobalTimerMetric(1.458e+04),\n",
              "  'ctpb': GlobalAverageMetric(3647),\n",
              "  'ctps': GlobalTimerMetric(3.924e+04),\n",
              "  'tpb': GlobalAverageMetric(5002),\n",
              "  'tps': GlobalTimerMetric(5.382e+04),\n",
              "  'lr': GlobalAverageMetric(9.9e-06),\n",
              "  'gpu_mem': GlobalAverageMetric(0.07842),\n",
              "  'total_train_updates': GlobalFixedMetric(169)})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습 결과 살펴보기"
      ],
      "metadata": {
        "id": "ChR4ZTa8uKPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from parlai.scripts.display_model import DisplayModel\n",
        "DisplayModel.main(\n",
        "    task='empathetic_dialogues',\n",
        "    model_file='from_scratch_empathetic_dialogues/model',\n",
        "    num_examples=2,\n",
        ")"
      ],
      "metadata": {
        "id": "TXkIWGexuNHF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5d36542-c5d6-4dae-862a-b2a0036672cd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "05:44:08 | Using CUDA\n",
            "05:44:08 | loading dictionary from from_scratch_empathetic_dialogues/model.dict\n",
            "05:44:08 | num words = 22419\n",
            "05:44:08 | Total parameters: 3,453,203 (3,453,203 trainable)\n",
            "05:44:08 | Loading existing model params from from_scratch_empathetic_dialogues/model\n",
            "05:44:08 | creating task(s): empathetic_dialogues\n",
            "05:44:09 | Opt:\n",
            "05:44:09 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "05:44:09 |     adam_eps: 1e-08\n",
            "05:44:09 |     add_p1_after_newln: False\n",
            "05:44:09 |     aggregate_micro: False\n",
            "05:44:09 |     allow_missing_init_opts: False\n",
            "05:44:09 |     attention: dot\n",
            "05:44:09 |     attention_length: 48\n",
            "05:44:09 |     attention_time: post\n",
            "05:44:09 |     batchsize: 16\n",
            "05:44:09 |     beam_block_full_context: True\n",
            "05:44:09 |     beam_block_list_filename: None\n",
            "05:44:09 |     beam_block_ngram: -1\n",
            "05:44:09 |     beam_context_block_ngram: -1\n",
            "05:44:09 |     beam_delay: 30\n",
            "05:44:09 |     beam_length_penalty: 0.65\n",
            "05:44:09 |     beam_min_length: 1\n",
            "05:44:09 |     beam_size: 1\n",
            "05:44:09 |     betas: '[0.9, 0.999]'\n",
            "05:44:09 |     bidirectional: False\n",
            "05:44:09 |     bpe_add_prefix_space: None\n",
            "05:44:09 |     bpe_debug: False\n",
            "05:44:09 |     bpe_dropout: None\n",
            "05:44:09 |     bpe_merge: None\n",
            "05:44:09 |     bpe_vocab: None\n",
            "05:44:09 |     compute_tokenized_bleu: False\n",
            "05:44:09 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
            "05:44:09 |     datatype: train\n",
            "05:44:09 |     decoder: same\n",
            "05:44:09 |     delimiter: '\\n'\n",
            "05:44:09 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "05:44:09 |     dict_endtoken: __end__\n",
            "05:44:09 |     dict_file: from_scratch_empathetic_dialogues/model.dict\n",
            "05:44:09 |     dict_include_test: False\n",
            "05:44:09 |     dict_include_valid: False\n",
            "05:44:09 |     dict_initpath: None\n",
            "05:44:09 |     dict_language: english\n",
            "05:44:09 |     dict_loaded: True\n",
            "05:44:09 |     dict_lower: False\n",
            "05:44:09 |     dict_max_ngram_size: -1\n",
            "05:44:09 |     dict_maxexs: -1\n",
            "05:44:09 |     dict_maxtokens: -1\n",
            "05:44:09 |     dict_minfreq: 0\n",
            "05:44:09 |     dict_nulltoken: __null__\n",
            "05:44:09 |     dict_starttoken: __start__\n",
            "05:44:09 |     dict_textfields: text,labels\n",
            "05:44:09 |     dict_tokenizer: re\n",
            "05:44:09 |     dict_unktoken: __unk__\n",
            "05:44:09 |     display_add_fields: \n",
            "05:44:09 |     display_examples: False\n",
            "05:44:09 |     download_path: None\n",
            "05:44:09 |     dropout: 0.1\n",
            "05:44:09 |     dynamic_batching: None\n",
            "05:44:09 |     embedding_projection: random\n",
            "05:44:09 |     embedding_type: random\n",
            "05:44:09 |     embeddingsize: 128\n",
            "05:44:09 |     eval_batchsize: None\n",
            "05:44:09 |     eval_dynamic_batching: None\n",
            "05:44:09 |     evaltask: None\n",
            "05:44:09 |     final_extra_opt: \n",
            "05:44:09 |     force_fp16_tokens: False\n",
            "05:44:09 |     fp16: False\n",
            "05:44:09 |     fp16_impl: safe\n",
            "05:44:09 |     gpu: -1\n",
            "05:44:09 |     gradient_clip: 0.1\n",
            "05:44:09 |     hiddensize: 128\n",
            "05:44:09 |     hide_labels: False\n",
            "05:44:09 |     history_add_global_end_token: None\n",
            "05:44:09 |     history_reversed: False\n",
            "05:44:09 |     history_size: -1\n",
            "05:44:09 |     image_cropsize: 224\n",
            "05:44:09 |     image_mode: raw\n",
            "05:44:09 |     image_size: 256\n",
            "05:44:09 |     inference: greedy\n",
            "05:44:09 |     init_model: None\n",
            "05:44:09 |     init_opt: None\n",
            "05:44:09 |     input_dropout: 0.0\n",
            "05:44:09 |     interactive_mode: False\n",
            "05:44:09 |     invsqrt_lr_decay_gamma: -1\n",
            "05:44:09 |     is_debug: False\n",
            "05:44:09 |     label_truncate: None\n",
            "05:44:09 |     learningrate: 1\n",
            "05:44:09 |     log_every_n_secs: -1\n",
            "05:44:09 |     log_every_n_steps: 50\n",
            "05:44:09 |     log_keep_fields: all\n",
            "05:44:09 |     loglevel: info\n",
            "05:44:09 |     lookuptable: all\n",
            "05:44:09 |     lr_scheduler: reduceonplateau\n",
            "05:44:09 |     lr_scheduler_decay: 0.5\n",
            "05:44:09 |     lr_scheduler_patience: 3\n",
            "05:44:09 |     max_train_steps: -1\n",
            "05:44:09 |     max_train_time: 60.0\n",
            "05:44:09 |     metrics: default\n",
            "05:44:09 |     model: seq2seq\n",
            "05:44:09 |     model_file: from_scratch_empathetic_dialogues/model\n",
            "05:44:09 |     momentum: 0\n",
            "05:44:09 |     multitask_weights: [1]\n",
            "05:44:09 |     mutators: None\n",
            "05:44:09 |     nesterov: True\n",
            "05:44:09 |     no_cuda: False\n",
            "05:44:09 |     num_epochs: -1\n",
            "05:44:09 |     num_examples: 2\n",
            "05:44:09 |     num_workers: 0\n",
            "05:44:09 |     numlayers: 2\n",
            "05:44:09 |     numsoftmax: 1\n",
            "05:44:09 |     nus: [0.7]\n",
            "05:44:09 |     optimizer: sgd\n",
            "05:44:09 |     override: \"{'task': 'empathetic_dialogues', 'model_file': 'from_scratch_empathetic_dialogues/model', 'num_examples': '2'}\"\n",
            "05:44:09 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
            "05:44:09 |     person_tokens: False\n",
            "05:44:09 |     rank_candidates: False\n",
            "05:44:09 |     rnn_class: lstm\n",
            "05:44:09 |     save_after_valid: False\n",
            "05:44:09 |     save_every_n_secs: -1\n",
            "05:44:09 |     save_format: conversations\n",
            "05:44:09 |     short_final_eval: False\n",
            "05:44:09 |     skip_generation: False\n",
            "05:44:09 |     special_tok_lst: None\n",
            "05:44:09 |     split_lines: False\n",
            "05:44:09 |     starttime: Jan12_05-32\n",
            "05:44:09 |     task: empathetic_dialogues\n",
            "05:44:09 |     temperature: 1.0\n",
            "05:44:09 |     tensorboard_log: False\n",
            "05:44:09 |     tensorboard_logdir: None\n",
            "05:44:09 |     text_truncate: None\n",
            "05:44:09 |     topk: 10\n",
            "05:44:09 |     topp: 0.9\n",
            "05:44:09 |     train_experiencer_only: False\n",
            "05:44:09 |     truncate: 64\n",
            "05:44:09 |     update_freq: 1\n",
            "05:44:09 |     use_reply: label\n",
            "05:44:09 |     validation_cutoff: 1.0\n",
            "05:44:09 |     validation_every_n_epochs: -1\n",
            "05:44:09 |     validation_every_n_secs: -1\n",
            "05:44:09 |     validation_every_n_steps: -1\n",
            "05:44:09 |     validation_max_exs: -1\n",
            "05:44:09 |     validation_metric: accuracy\n",
            "05:44:09 |     validation_metric_mode: None\n",
            "05:44:09 |     validation_patience: 10\n",
            "05:44:09 |     validation_share_agent: False\n",
            "05:44:09 |     verbose: False\n",
            "05:44:09 |     wandb_entity: None\n",
            "05:44:09 |     wandb_log: False\n",
            "05:44:09 |     wandb_name: None\n",
            "05:44:09 |     wandb_project: None\n",
            "05:44:09 |     warmup_rate: 0.0001\n",
            "05:44:09 |     warmup_updates: -1\n",
            "05:44:09 |     weight_decay: None\n",
            "05:44:09 |     world_logs: \n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues- - -\u001b[0;0m\n",
            "\u001b[0mToday,as i was leaving for work in the morning,i had a tire burst in the middle of a busy road. That scared the hell out of me!\u001b[0;0m\n",
            "\u001b[1;94m    labels: Are you fine now?\u001b[0;0m\n",
            "\u001b[0;95m     model: I ' m to I ' m to the .\u001b[0;0m\n",
            "\u001b[0mYeah,i'm doing alright now, but with minor injuries.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Cool :) Is your car damaged a lot?\u001b[0;0m\n",
            "\u001b[0;95m     model: I ' m to I ' m to the .\u001b[0;0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from parlai.scripts.display_model import DisplayModel\n",
        "DisplayModel.main(\n",
        "    task='empathetic_dialogues',\n",
        "    model_file='from_pretrained/model2',\n",
        "    num_examples=2,\n",
        "    skip_generation=False,\n",
        ")"
      ],
      "metadata": {
        "id": "RJiNXaJtuSzc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "726cd17a-f290-4884-fa24-b575115ab3f7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "05:49:20 | \u001b[33mOverriding opt[\"skip_generation\"] to False (previously: True)\u001b[0m\n",
            "05:49:20 | Using CUDA\n",
            "05:49:20 | loading dictionary from from_pretrained/model2.dict\n",
            "05:49:21 | num words = 54944\n",
            "05:49:22 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "05:49:22 | Loading existing model params from from_pretrained/model2\n",
            "05:49:26 | creating task(s): empathetic_dialogues\n",
            "05:49:27 | Opt:\n",
            "05:49:27 |     activation: gelu\n",
            "05:49:27 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "05:49:27 |     adam_eps: 1e-08\n",
            "05:49:27 |     add_p1_after_newln: False\n",
            "05:49:27 |     aggregate_micro: False\n",
            "05:49:27 |     allow_missing_init_opts: False\n",
            "05:49:27 |     attention_dropout: 0.0\n",
            "05:49:27 |     batchsize: 12\n",
            "05:49:27 |     beam_block_full_context: True\n",
            "05:49:27 |     beam_block_list_filename: None\n",
            "05:49:27 |     beam_block_ngram: -1\n",
            "05:49:27 |     beam_context_block_ngram: -1\n",
            "05:49:27 |     beam_delay: 30\n",
            "05:49:27 |     beam_length_penalty: 0.65\n",
            "05:49:27 |     beam_min_length: 1\n",
            "05:49:27 |     beam_size: 1\n",
            "05:49:27 |     betas: '[0.9, 0.999]'\n",
            "05:49:27 |     bpe_add_prefix_space: None\n",
            "05:49:27 |     bpe_debug: False\n",
            "05:49:27 |     bpe_dropout: None\n",
            "05:49:27 |     bpe_merge: None\n",
            "05:49:27 |     bpe_vocab: None\n",
            "05:49:27 |     checkpoint_activations: False\n",
            "05:49:27 |     compute_tokenized_bleu: False\n",
            "05:49:27 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
            "05:49:27 |     datatype: train\n",
            "05:49:27 |     delimiter: '\\n'\n",
            "05:49:27 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "05:49:27 |     dict_endtoken: __end__\n",
            "05:49:27 |     dict_file: from_pretrained/model2.dict\n",
            "05:49:27 |     dict_include_test: False\n",
            "05:49:27 |     dict_include_valid: False\n",
            "05:49:27 |     dict_initpath: None\n",
            "05:49:27 |     dict_language: english\n",
            "05:49:27 |     dict_loaded: True\n",
            "05:49:27 |     dict_lower: True\n",
            "05:49:27 |     dict_max_ngram_size: -1\n",
            "05:49:27 |     dict_maxexs: -1\n",
            "05:49:27 |     dict_maxtokens: -1\n",
            "05:49:27 |     dict_minfreq: 0\n",
            "05:49:27 |     dict_nulltoken: __null__\n",
            "05:49:27 |     dict_starttoken: __start__\n",
            "05:49:27 |     dict_textfields: text,labels\n",
            "05:49:27 |     dict_tokenizer: bpe\n",
            "05:49:27 |     dict_unktoken: __unk__\n",
            "05:49:27 |     display_add_fields: \n",
            "05:49:27 |     display_examples: False\n",
            "05:49:27 |     download_path: None\n",
            "05:49:27 |     dropout: 0.0\n",
            "05:49:27 |     dynamic_batching: full\n",
            "05:49:27 |     embedding_projection: random\n",
            "05:49:27 |     embedding_size: 512\n",
            "05:49:27 |     embedding_type: random\n",
            "05:49:27 |     embeddings_scale: True\n",
            "05:49:27 |     eval_batchsize: None\n",
            "05:49:27 |     eval_dynamic_batching: None\n",
            "05:49:27 |     evaltask: None\n",
            "05:49:27 |     ffn_size: 2048\n",
            "05:49:27 |     final_extra_opt: \n",
            "05:49:27 |     force_fp16_tokens: True\n",
            "05:49:27 |     fp16: True\n",
            "05:49:27 |     fp16_impl: mem_efficient\n",
            "05:49:27 |     gpu: -1\n",
            "05:49:27 |     gradient_clip: 0.1\n",
            "05:49:27 |     hide_labels: False\n",
            "05:49:27 |     history_add_global_end_token: None\n",
            "05:49:27 |     history_reversed: False\n",
            "05:49:27 |     history_size: -1\n",
            "05:49:27 |     image_cropsize: 224\n",
            "05:49:27 |     image_mode: raw\n",
            "05:49:27 |     image_size: 256\n",
            "05:49:27 |     inference: greedy\n",
            "05:49:27 |     init_model: /usr/local/lib/python3.8/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "05:49:27 |     init_opt: None\n",
            "05:49:27 |     interactive_mode: False\n",
            "05:49:27 |     invsqrt_lr_decay_gamma: -1\n",
            "05:49:27 |     is_debug: False\n",
            "05:49:27 |     label_truncate: 128\n",
            "05:49:27 |     learn_positional_embeddings: True\n",
            "05:49:27 |     learningrate: 1e-05\n",
            "05:49:27 |     log_every_n_secs: -1\n",
            "05:49:27 |     log_every_n_steps: 50\n",
            "05:49:27 |     log_keep_fields: all\n",
            "05:49:27 |     loglevel: info\n",
            "05:49:27 |     lr_scheduler: reduceonplateau\n",
            "05:49:27 |     lr_scheduler_decay: 0.5\n",
            "05:49:27 |     lr_scheduler_patience: 3\n",
            "05:49:27 |     max_train_steps: -1\n",
            "05:49:27 |     max_train_time: 60.0\n",
            "05:49:27 |     metrics: default\n",
            "05:49:27 |     model: transformer/generator\n",
            "05:49:27 |     model_file: from_pretrained/model2\n",
            "05:49:27 |     model_parallel: False\n",
            "05:49:27 |     momentum: 0\n",
            "05:49:27 |     multitask_weights: [1]\n",
            "05:49:27 |     mutators: None\n",
            "05:49:27 |     n_decoder_layers: -1\n",
            "05:49:27 |     n_encoder_layers: -1\n",
            "05:49:27 |     n_heads: 16\n",
            "05:49:27 |     n_layers: 8\n",
            "05:49:27 |     n_positions: 512\n",
            "05:49:27 |     n_segments: 0\n",
            "05:49:27 |     nesterov: True\n",
            "05:49:27 |     no_cuda: False\n",
            "05:49:27 |     num_epochs: -1\n",
            "05:49:27 |     num_examples: 2\n",
            "05:49:27 |     num_workers: 0\n",
            "05:49:27 |     nus: [0.7]\n",
            "05:49:27 |     optimizer: mem_eff_adam\n",
            "05:49:27 |     output_scaling: 1.0\n",
            "05:49:27 |     override: \"{'task': 'empathetic_dialogues', 'model_file': 'from_pretrained/model2', 'num_examples': '2', 'skip_generation': False}\"\n",
            "05:49:27 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
            "05:49:27 |     person_tokens: False\n",
            "05:49:27 |     rank_candidates: False\n",
            "05:49:27 |     relu_dropout: 0.0\n",
            "05:49:27 |     save_after_valid: False\n",
            "05:49:27 |     save_every_n_secs: -1\n",
            "05:49:27 |     save_format: conversations\n",
            "05:49:27 |     share_word_embeddings: True\n",
            "05:49:27 |     short_final_eval: False\n",
            "05:49:27 |     skip_generation: False\n",
            "05:49:27 |     special_tok_lst: None\n",
            "05:49:27 |     split_lines: False\n",
            "05:49:27 |     starttime: Jan12_05-41\n",
            "05:49:27 |     task: empathetic_dialogues\n",
            "05:49:27 |     temperature: 1.0\n",
            "05:49:27 |     tensorboard_log: False\n",
            "05:49:27 |     tensorboard_logdir: None\n",
            "05:49:27 |     text_truncate: 512\n",
            "05:49:27 |     topk: 10\n",
            "05:49:27 |     topp: 0.9\n",
            "05:49:27 |     train_experiencer_only: False\n",
            "05:49:27 |     truncate: -1\n",
            "05:49:27 |     update_freq: 1\n",
            "05:49:27 |     use_reply: label\n",
            "05:49:27 |     validation_cutoff: 1.0\n",
            "05:49:27 |     validation_every_n_epochs: 0.25\n",
            "05:49:27 |     validation_every_n_secs: -1\n",
            "05:49:27 |     validation_every_n_steps: -1\n",
            "05:49:27 |     validation_max_exs: -1\n",
            "05:49:27 |     validation_metric: ppl\n",
            "05:49:27 |     validation_metric_mode: None\n",
            "05:49:27 |     validation_patience: 10\n",
            "05:49:27 |     validation_share_agent: False\n",
            "05:49:27 |     variant: xlm\n",
            "05:49:27 |     verbose: False\n",
            "05:49:27 |     wandb_entity: None\n",
            "05:49:27 |     wandb_log: False\n",
            "05:49:27 |     wandb_name: None\n",
            "05:49:27 |     wandb_project: None\n",
            "05:49:27 |     warmup_rate: 0.0001\n",
            "05:49:27 |     warmup_updates: 100\n",
            "05:49:27 |     weight_decay: None\n",
            "05:49:27 |     world_logs: \n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues- - -\u001b[0;0m\n",
            "\u001b[0mToday,as i was leaving for work in the morning,i had a tire burst in the middle of a busy road. That scared the hell out of me!\u001b[0;0m\n",
            "\u001b[1;94m    labels: Are you fine now?\u001b[0;0m\n",
            "\u001b[0;95m     model: oh no ! that ' s so scary !\u001b[0;0m\n",
            "\u001b[0mYeah,i'm doing alright now, but with minor injuries.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Cool :) Is your car damaged a lot?\u001b[0;0m\n",
            "\u001b[0;95m     model: oh no ! i hope you ' re okay .\u001b[0;0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델과 상호작용하기"
      ],
      "metadata": {
        "id": "QSD_qkkguYMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from parlai.scripts.interactive import Interactive\n",
        "\n",
        "Interactive.main(\n",
        "    model_file='from_pretrained/model2'\n",
        ")"
      ],
      "metadata": {
        "id": "vPuydJHduaRX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ba2940b-05aa-4bd2-d210-0a865648ae1a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "05:51:43 | Using CUDA\n",
            "05:51:43 | loading dictionary from from_pretrained/model2.dict\n",
            "05:51:43 | num words = 54944\n",
            "05:51:43 | TransformerGenerator: full interactive mode on.\n",
            "05:51:45 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "05:51:45 | Loading existing model params from from_pretrained/model2\n",
            "05:51:46 | Opt:\n",
            "05:51:46 |     activation: gelu\n",
            "05:51:46 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "05:51:46 |     adam_eps: 1e-08\n",
            "05:51:46 |     add_p1_after_newln: False\n",
            "05:51:46 |     aggregate_micro: False\n",
            "05:51:46 |     allow_missing_init_opts: False\n",
            "05:51:46 |     attention_dropout: 0.0\n",
            "05:51:46 |     batchsize: 12\n",
            "05:51:46 |     beam_block_full_context: True\n",
            "05:51:46 |     beam_block_list_filename: None\n",
            "05:51:46 |     beam_block_ngram: -1\n",
            "05:51:46 |     beam_context_block_ngram: -1\n",
            "05:51:46 |     beam_delay: 30\n",
            "05:51:46 |     beam_length_penalty: 0.65\n",
            "05:51:46 |     beam_min_length: 1\n",
            "05:51:46 |     beam_size: 1\n",
            "05:51:46 |     betas: '[0.9, 0.999]'\n",
            "05:51:46 |     bpe_add_prefix_space: None\n",
            "05:51:46 |     bpe_debug: False\n",
            "05:51:46 |     bpe_dropout: None\n",
            "05:51:46 |     bpe_merge: None\n",
            "05:51:46 |     bpe_vocab: None\n",
            "05:51:46 |     checkpoint_activations: False\n",
            "05:51:46 |     compute_tokenized_bleu: False\n",
            "05:51:46 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
            "05:51:46 |     datatype: train\n",
            "05:51:46 |     delimiter: '\\n'\n",
            "05:51:46 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "05:51:46 |     dict_endtoken: __end__\n",
            "05:51:46 |     dict_file: from_pretrained/model2.dict\n",
            "05:51:46 |     dict_include_test: False\n",
            "05:51:46 |     dict_include_valid: False\n",
            "05:51:46 |     dict_initpath: None\n",
            "05:51:46 |     dict_language: english\n",
            "05:51:46 |     dict_loaded: True\n",
            "05:51:46 |     dict_lower: True\n",
            "05:51:46 |     dict_max_ngram_size: -1\n",
            "05:51:46 |     dict_maxexs: -1\n",
            "05:51:46 |     dict_maxtokens: -1\n",
            "05:51:46 |     dict_minfreq: 0\n",
            "05:51:46 |     dict_nulltoken: __null__\n",
            "05:51:46 |     dict_starttoken: __start__\n",
            "05:51:46 |     dict_textfields: text,labels\n",
            "05:51:46 |     dict_tokenizer: bpe\n",
            "05:51:46 |     dict_unktoken: __unk__\n",
            "05:51:46 |     display_add_fields: \n",
            "05:51:46 |     display_examples: False\n",
            "05:51:46 |     display_prettify: False\n",
            "05:51:46 |     download_path: None\n",
            "05:51:46 |     dropout: 0.0\n",
            "05:51:46 |     dynamic_batching: full\n",
            "05:51:46 |     embedding_projection: random\n",
            "05:51:46 |     embedding_size: 512\n",
            "05:51:46 |     embedding_type: random\n",
            "05:51:46 |     embeddings_scale: True\n",
            "05:51:46 |     eval_batchsize: None\n",
            "05:51:46 |     eval_dynamic_batching: None\n",
            "05:51:46 |     evaltask: None\n",
            "05:51:46 |     ffn_size: 2048\n",
            "05:51:46 |     final_extra_opt: \n",
            "05:51:46 |     force_fp16_tokens: True\n",
            "05:51:46 |     fp16: True\n",
            "05:51:46 |     fp16_impl: mem_efficient\n",
            "05:51:46 |     gpu: -1\n",
            "05:51:46 |     gradient_clip: 0.1\n",
            "05:51:46 |     hide_labels: False\n",
            "05:51:46 |     history_add_global_end_token: None\n",
            "05:51:46 |     history_reversed: False\n",
            "05:51:46 |     history_size: -1\n",
            "05:51:46 |     image_cropsize: 224\n",
            "05:51:46 |     image_mode: raw\n",
            "05:51:46 |     image_size: 256\n",
            "05:51:46 |     inference: greedy\n",
            "05:51:46 |     init_model: /usr/local/lib/python3.8/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "05:51:46 |     init_opt: None\n",
            "05:51:46 |     interactive_mode: True\n",
            "05:51:46 |     interactive_task: True\n",
            "05:51:46 |     invsqrt_lr_decay_gamma: -1\n",
            "05:51:46 |     is_debug: False\n",
            "05:51:46 |     label_truncate: 128\n",
            "05:51:46 |     learn_positional_embeddings: True\n",
            "05:51:46 |     learningrate: 1e-05\n",
            "05:51:46 |     local_human_candidates_file: None\n",
            "05:51:46 |     log_every_n_secs: -1\n",
            "05:51:46 |     log_every_n_steps: 50\n",
            "05:51:46 |     log_keep_fields: all\n",
            "05:51:46 |     loglevel: info\n",
            "05:51:46 |     lr_scheduler: reduceonplateau\n",
            "05:51:46 |     lr_scheduler_decay: 0.5\n",
            "05:51:46 |     lr_scheduler_patience: 3\n",
            "05:51:46 |     max_train_steps: -1\n",
            "05:51:46 |     max_train_time: 60.0\n",
            "05:51:46 |     metrics: default\n",
            "05:51:46 |     model: transformer/generator\n",
            "05:51:46 |     model_file: from_pretrained/model2\n",
            "05:51:46 |     model_parallel: False\n",
            "05:51:46 |     momentum: 0\n",
            "05:51:46 |     multitask_weights: [1]\n",
            "05:51:46 |     mutators: None\n",
            "05:51:46 |     n_decoder_layers: -1\n",
            "05:51:46 |     n_encoder_layers: -1\n",
            "05:51:46 |     n_heads: 16\n",
            "05:51:46 |     n_layers: 8\n",
            "05:51:46 |     n_positions: 512\n",
            "05:51:46 |     n_segments: 0\n",
            "05:51:46 |     nesterov: True\n",
            "05:51:46 |     no_cuda: False\n",
            "05:51:46 |     num_epochs: -1\n",
            "05:51:46 |     num_workers: 0\n",
            "05:51:46 |     nus: [0.7]\n",
            "05:51:46 |     optimizer: mem_eff_adam\n",
            "05:51:46 |     outfile: \n",
            "05:51:46 |     output_scaling: 1.0\n",
            "05:51:46 |     override: \"{'model_file': 'from_pretrained/model2'}\"\n",
            "05:51:46 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
            "05:51:46 |     person_tokens: False\n",
            "05:51:46 |     rank_candidates: False\n",
            "05:51:46 |     relu_dropout: 0.0\n",
            "05:51:46 |     save_after_valid: False\n",
            "05:51:46 |     save_every_n_secs: -1\n",
            "05:51:46 |     save_format: conversations\n",
            "05:51:46 |     share_word_embeddings: True\n",
            "05:51:46 |     short_final_eval: False\n",
            "05:51:46 |     single_turn: False\n",
            "05:51:46 |     skip_generation: True\n",
            "05:51:46 |     special_tok_lst: None\n",
            "05:51:46 |     split_lines: False\n",
            "05:51:46 |     starttime: Jan12_05-41\n",
            "05:51:46 |     task: empathetic_dialogues\n",
            "05:51:46 |     temperature: 1.0\n",
            "05:51:46 |     tensorboard_log: False\n",
            "05:51:46 |     tensorboard_logdir: None\n",
            "05:51:46 |     text_truncate: 512\n",
            "05:51:46 |     topk: 10\n",
            "05:51:46 |     topp: 0.9\n",
            "05:51:46 |     train_experiencer_only: False\n",
            "05:51:46 |     truncate: -1\n",
            "05:51:46 |     update_freq: 1\n",
            "05:51:46 |     use_reply: label\n",
            "05:51:46 |     validation_cutoff: 1.0\n",
            "05:51:46 |     validation_every_n_epochs: 0.25\n",
            "05:51:46 |     validation_every_n_secs: -1\n",
            "05:51:46 |     validation_every_n_steps: -1\n",
            "05:51:46 |     validation_max_exs: -1\n",
            "05:51:46 |     validation_metric: ppl\n",
            "05:51:46 |     validation_metric_mode: None\n",
            "05:51:46 |     validation_patience: 10\n",
            "05:51:46 |     validation_share_agent: False\n",
            "05:51:46 |     variant: xlm\n",
            "05:51:46 |     verbose: False\n",
            "05:51:46 |     wandb_entity: None\n",
            "05:51:46 |     wandb_log: False\n",
            "05:51:46 |     wandb_name: None\n",
            "05:51:46 |     wandb_project: None\n",
            "05:51:46 |     warmup_rate: 0.0001\n",
            "05:51:46 |     warmup_updates: 100\n",
            "05:51:46 |     weight_decay: None\n",
            "05:51:46 |     world_logs: \n",
            "\u001b[1;31mEnter [DONE] if you want to end the episode, [EXIT] to quit.\u001b[0;0m\n",
            "05:51:47 | creating task(s): interactive\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m hi how are you?\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mi ' m good , how are you ?\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m I'm fine\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mthat ' s good to hear , what ' s going on ?\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m I'm so afraid\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mi ' m sorry to hear that , what ' s going on ?\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m my grandfather died\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mi ' m sorry to hear that , i hope you can find peace soon .\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m thank you\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mi ' m sorry to hear that , i hope you find peace soon .\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m how about you?\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mi ' m fine , i ' m just trying to find a way to get through this .\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m [done]\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mi ' m glad you ' re doing well , i hope you find peace soon .\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m [exit]\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mi ' m sorry to hear that , i hope you find peace soon .\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m exit\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mi ' m sorry to hear that , i hope you find peace soon .\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m [EXIT]\n",
            "CHAT DONE \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 평가하기"
      ],
      "metadata": {
        "id": "uCSdNmlxuitw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from parlai.scripts.eval_model import EvalModel\n",
        "#eval_model->모델 평가가능\n",
        "EvalModel.main(\n",
        "    task='empathetic_dialogues',\n",
        "    model_file='from_pretrained/model2',\n",
        "    num_examples=2,\n",
        "    skip_generation=False,\n",
        "    datatype='test',\n",
        "    metrics='bleu,ppl' #모델 평가 요소\n",
        ")\n",
        "#모델의 욕설이나 정치적인 발언을 하는지를 파악하는 연구도 있음\n",
        "#chatgpt의 경우, 회피를 잘함"
      ],
      "metadata": {
        "id": "-cdsZsqjuknH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "433b3a2d-eda6-4161-b04e-dab78d412661"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "05:55:35 | \u001b[33mOverriding opt[\"datatype\"] to test (previously: train)\u001b[0m\n",
            "05:55:35 | \u001b[33mOverriding opt[\"skip_generation\"] to False (previously: True)\u001b[0m\n",
            "05:55:35 | \u001b[33mOverriding opt[\"metrics\"] to bleu,ppl (previously: default)\u001b[0m\n",
            "05:55:35 | Using CUDA\n",
            "05:55:35 | loading dictionary from from_pretrained/model2.dict\n",
            "05:55:35 | num words = 54944\n",
            "05:55:37 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "05:55:37 | Loading existing model params from from_pretrained/model2\n",
            "05:55:38 | Opt:\n",
            "05:55:38 |     activation: gelu\n",
            "05:55:38 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "05:55:38 |     adam_eps: 1e-08\n",
            "05:55:38 |     add_p1_after_newln: False\n",
            "05:55:38 |     aggregate_micro: False\n",
            "05:55:38 |     allow_missing_init_opts: False\n",
            "05:55:38 |     area_under_curve_class: None\n",
            "05:55:38 |     area_under_curve_digits: -1\n",
            "05:55:38 |     attention_dropout: 0.0\n",
            "05:55:38 |     batchsize: 12\n",
            "05:55:38 |     beam_block_full_context: True\n",
            "05:55:38 |     beam_block_list_filename: None\n",
            "05:55:38 |     beam_block_ngram: -1\n",
            "05:55:38 |     beam_context_block_ngram: -1\n",
            "05:55:38 |     beam_delay: 30\n",
            "05:55:38 |     beam_length_penalty: 0.65\n",
            "05:55:38 |     beam_min_length: 1\n",
            "05:55:38 |     beam_size: 1\n",
            "05:55:38 |     betas: '[0.9, 0.999]'\n",
            "05:55:38 |     bpe_add_prefix_space: None\n",
            "05:55:38 |     bpe_debug: False\n",
            "05:55:38 |     bpe_dropout: None\n",
            "05:55:38 |     bpe_merge: None\n",
            "05:55:38 |     bpe_vocab: None\n",
            "05:55:38 |     checkpoint_activations: False\n",
            "05:55:38 |     compute_tokenized_bleu: False\n",
            "05:55:38 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
            "05:55:38 |     datatype: test\n",
            "05:55:38 |     delimiter: '\\n'\n",
            "05:55:38 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "05:55:38 |     dict_endtoken: __end__\n",
            "05:55:38 |     dict_file: from_pretrained/model2.dict\n",
            "05:55:38 |     dict_include_test: False\n",
            "05:55:38 |     dict_include_valid: False\n",
            "05:55:38 |     dict_initpath: None\n",
            "05:55:38 |     dict_language: english\n",
            "05:55:38 |     dict_loaded: True\n",
            "05:55:38 |     dict_lower: True\n",
            "05:55:38 |     dict_max_ngram_size: -1\n",
            "05:55:38 |     dict_maxexs: -1\n",
            "05:55:38 |     dict_maxtokens: -1\n",
            "05:55:38 |     dict_minfreq: 0\n",
            "05:55:38 |     dict_nulltoken: __null__\n",
            "05:55:38 |     dict_starttoken: __start__\n",
            "05:55:38 |     dict_textfields: text,labels\n",
            "05:55:38 |     dict_tokenizer: bpe\n",
            "05:55:38 |     dict_unktoken: __unk__\n",
            "05:55:38 |     display_examples: False\n",
            "05:55:38 |     download_path: None\n",
            "05:55:38 |     dropout: 0.0\n",
            "05:55:38 |     dynamic_batching: full\n",
            "05:55:38 |     embedding_projection: random\n",
            "05:55:38 |     embedding_size: 512\n",
            "05:55:38 |     embedding_type: random\n",
            "05:55:38 |     embeddings_scale: True\n",
            "05:55:38 |     eval_batchsize: None\n",
            "05:55:38 |     eval_dynamic_batching: None\n",
            "05:55:38 |     evaltask: None\n",
            "05:55:38 |     ffn_size: 2048\n",
            "05:55:38 |     final_extra_opt: \n",
            "05:55:38 |     force_fp16_tokens: True\n",
            "05:55:38 |     fp16: True\n",
            "05:55:38 |     fp16_impl: mem_efficient\n",
            "05:55:38 |     gpu: -1\n",
            "05:55:38 |     gradient_clip: 0.1\n",
            "05:55:38 |     hide_labels: False\n",
            "05:55:38 |     history_add_global_end_token: None\n",
            "05:55:38 |     history_reversed: False\n",
            "05:55:38 |     history_size: -1\n",
            "05:55:38 |     image_cropsize: 224\n",
            "05:55:38 |     image_mode: raw\n",
            "05:55:38 |     image_size: 256\n",
            "05:55:38 |     inference: greedy\n",
            "05:55:38 |     init_model: /usr/local/lib/python3.8/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "05:55:38 |     init_opt: None\n",
            "05:55:38 |     interactive_mode: False\n",
            "05:55:38 |     invsqrt_lr_decay_gamma: -1\n",
            "05:55:38 |     is_debug: False\n",
            "05:55:38 |     label_truncate: 128\n",
            "05:55:38 |     learn_positional_embeddings: True\n",
            "05:55:38 |     learningrate: 1e-05\n",
            "05:55:38 |     log_every_n_secs: -1\n",
            "05:55:38 |     log_every_n_steps: 50\n",
            "05:55:38 |     log_keep_fields: all\n",
            "05:55:38 |     loglevel: info\n",
            "05:55:38 |     lr_scheduler: reduceonplateau\n",
            "05:55:38 |     lr_scheduler_decay: 0.5\n",
            "05:55:38 |     lr_scheduler_patience: 3\n",
            "05:55:38 |     max_train_steps: -1\n",
            "05:55:38 |     max_train_time: 60.0\n",
            "05:55:38 |     metrics: bleu,ppl\n",
            "05:55:38 |     model: transformer/generator\n",
            "05:55:38 |     model_file: from_pretrained/model2\n",
            "05:55:38 |     model_parallel: False\n",
            "05:55:38 |     momentum: 0\n",
            "05:55:38 |     multitask_weights: [1]\n",
            "05:55:38 |     mutators: None\n",
            "05:55:38 |     n_decoder_layers: -1\n",
            "05:55:38 |     n_encoder_layers: -1\n",
            "05:55:38 |     n_heads: 16\n",
            "05:55:38 |     n_layers: 8\n",
            "05:55:38 |     n_positions: 512\n",
            "05:55:38 |     n_segments: 0\n",
            "05:55:38 |     nesterov: True\n",
            "05:55:38 |     no_cuda: False\n",
            "05:55:38 |     num_epochs: -1\n",
            "05:55:38 |     num_examples: 2\n",
            "05:55:38 |     num_workers: 0\n",
            "05:55:38 |     nus: [0.7]\n",
            "05:55:38 |     optimizer: mem_eff_adam\n",
            "05:55:38 |     output_scaling: 1.0\n",
            "05:55:38 |     override: \"{'datatype': 'test', 'task': 'empathetic_dialogues', 'model_file': 'from_pretrained/model2', 'num_examples': 2, 'skip_generation': False, 'metrics': 'bleu,ppl'}\"\n",
            "05:55:38 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
            "05:55:38 |     person_tokens: False\n",
            "05:55:38 |     rank_candidates: False\n",
            "05:55:38 |     relu_dropout: 0.0\n",
            "05:55:38 |     report_filename: \n",
            "05:55:38 |     save_after_valid: False\n",
            "05:55:38 |     save_every_n_secs: -1\n",
            "05:55:38 |     save_format: conversations\n",
            "05:55:38 |     share_word_embeddings: True\n",
            "05:55:38 |     short_final_eval: False\n",
            "05:55:38 |     skip_generation: False\n",
            "05:55:38 |     special_tok_lst: None\n",
            "05:55:38 |     split_lines: False\n",
            "05:55:38 |     starttime: Jan12_05-41\n",
            "05:55:38 |     task: empathetic_dialogues\n",
            "05:55:38 |     temperature: 1.0\n",
            "05:55:38 |     tensorboard_log: False\n",
            "05:55:38 |     tensorboard_logdir: None\n",
            "05:55:38 |     text_truncate: 512\n",
            "05:55:38 |     topk: 10\n",
            "05:55:38 |     topp: 0.9\n",
            "05:55:38 |     train_experiencer_only: False\n",
            "05:55:38 |     truncate: -1\n",
            "05:55:38 |     update_freq: 1\n",
            "05:55:38 |     use_reply: label\n",
            "05:55:38 |     validation_cutoff: 1.0\n",
            "05:55:38 |     validation_every_n_epochs: 0.25\n",
            "05:55:38 |     validation_every_n_secs: -1\n",
            "05:55:38 |     validation_every_n_steps: -1\n",
            "05:55:38 |     validation_max_exs: -1\n",
            "05:55:38 |     validation_metric: ppl\n",
            "05:55:38 |     validation_metric_mode: None\n",
            "05:55:38 |     validation_patience: 10\n",
            "05:55:38 |     validation_share_agent: False\n",
            "05:55:38 |     variant: xlm\n",
            "05:55:38 |     verbose: False\n",
            "05:55:38 |     wandb_entity: None\n",
            "05:55:38 |     wandb_log: False\n",
            "05:55:38 |     wandb_name: None\n",
            "05:55:38 |     wandb_project: None\n",
            "05:55:38 |     warmup_rate: 0.0001\n",
            "05:55:38 |     warmup_updates: 100\n",
            "05:55:38 |     weight_decay: None\n",
            "05:55:38 |     world_logs: \n",
            "05:55:39 | Evaluating task empathetic_dialogues using datatype test.\n",
            "05:55:39 | creating task(s): empathetic_dialogues\n",
            "05:55:40 | \u001b[1mReport for empathetic_dialogues:\n",
            "    accuracy  bleu-1    bleu-2    bleu-3    bleu-4  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  gen_n_toks  gpu_mem  \\\n",
            "           0  .08013 8.355e-08 8.734e-10 9.158e-11    56    56 247.3       0          0 4.415    2 .1005        18.5   .07869   \n",
            "    llen  loss  ltpb  ltps  ltrunc  ltrunclen  ppl  token_acc  token_em  tpb   tps  \n",
            "      11 1.866    11 48.58       0          0 6.46      .6818         0   67 295.9\u001b[0m\n",
            "05:55:40 | Finished evaluating tasks ['empathetic_dialogues'] using datatype test\n",
            "    accuracy  bleu-1    bleu-2    bleu-3    bleu-4  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  gen_n_toks  gpu_mem  \\\n",
            "           0  .08013 8.355e-08 8.734e-10 9.158e-11    56    56 247.3       0          0 4.415    2 .1005        18.5   .07869   \n",
            "    llen  loss  ltpb  ltps  ltrunc  ltrunclen  ppl  token_acc  token_em  tpb   tps  \n",
            "      11 1.866    11 48.58       0          0 6.46      .6818         0   67 295.9\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'exs': SumMetric(2),\n",
              " 'accuracy': ExactMatchMetric(0),\n",
              " 'f1': F1Metric(0.1005),\n",
              " 'bleu-1': BleuMetric(0.08013),\n",
              " 'bleu-2': BleuMetric(8.355e-08),\n",
              " 'bleu-3': BleuMetric(8.734e-10),\n",
              " 'bleu-4': BleuMetric(9.158e-11),\n",
              " 'clen': AverageMetric(56),\n",
              " 'ctrunc': AverageMetric(0),\n",
              " 'ctrunclen': AverageMetric(0),\n",
              " 'llen': AverageMetric(11),\n",
              " 'ltrunc': AverageMetric(0),\n",
              " 'ltrunclen': AverageMetric(0),\n",
              " 'loss': AverageMetric(1.866),\n",
              " 'ppl': PPLMetric(6.46),\n",
              " 'token_acc': AverageMetric(0.6818),\n",
              " 'token_em': AverageMetric(0),\n",
              " 'gen_n_toks': AverageMetric(18.5),\n",
              " 'exps': GlobalTimerMetric(4.415),\n",
              " 'ltpb': GlobalAverageMetric(11),\n",
              " 'ltps': GlobalTimerMetric(48.58),\n",
              " 'ctpb': GlobalAverageMetric(56),\n",
              " 'ctps': GlobalTimerMetric(247.3),\n",
              " 'tpb': GlobalAverageMetric(67),\n",
              " 'tps': GlobalTimerMetric(295.9),\n",
              " 'gpu_mem': GlobalAverageMetric(0.07869)}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#처음 학습시킨 모델 평가\n",
        "from parlai.scripts.eval_model import EvalModel\n",
        "#eval_model->모델 평가가능\n",
        "EvalModel.main(\n",
        "    task='empathetic_dialogues',\n",
        "    model_file='from_scratch_empathetic_dialogues/model',\n",
        "    num_examples=2,\n",
        "    skip_generation=False,\n",
        "    datatype='test',\n",
        "    metrics='all' #모델 평가 요소\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9g7pYm6NkgpG",
        "outputId": "8559bae2-a0b2-4736-e1e2-479be1209da6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "06:07:05 | \u001b[33mOverriding opt[\"datatype\"] to test (previously: train)\u001b[0m\n",
            "06:07:05 | \u001b[33mOverriding opt[\"metrics\"] to all (previously: default)\u001b[0m\n",
            "06:07:05 | Using CUDA\n",
            "06:07:05 | loading dictionary from from_scratch_empathetic_dialogues/model.dict\n",
            "06:07:05 | num words = 22419\n",
            "06:07:05 | Total parameters: 3,453,203 (3,453,203 trainable)\n",
            "06:07:05 | Loading existing model params from from_scratch_empathetic_dialogues/model\n",
            "06:07:05 | Opt:\n",
            "06:07:05 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "06:07:05 |     adam_eps: 1e-08\n",
            "06:07:05 |     add_p1_after_newln: False\n",
            "06:07:05 |     aggregate_micro: False\n",
            "06:07:05 |     allow_missing_init_opts: False\n",
            "06:07:05 |     area_under_curve_class: None\n",
            "06:07:05 |     area_under_curve_digits: -1\n",
            "06:07:05 |     attention: dot\n",
            "06:07:05 |     attention_length: 48\n",
            "06:07:05 |     attention_time: post\n",
            "06:07:05 |     batchsize: 16\n",
            "06:07:05 |     beam_block_full_context: True\n",
            "06:07:05 |     beam_block_list_filename: None\n",
            "06:07:05 |     beam_block_ngram: -1\n",
            "06:07:05 |     beam_context_block_ngram: -1\n",
            "06:07:05 |     beam_delay: 30\n",
            "06:07:05 |     beam_length_penalty: 0.65\n",
            "06:07:05 |     beam_min_length: 1\n",
            "06:07:05 |     beam_size: 1\n",
            "06:07:05 |     betas: '[0.9, 0.999]'\n",
            "06:07:05 |     bidirectional: False\n",
            "06:07:05 |     bpe_add_prefix_space: None\n",
            "06:07:05 |     bpe_debug: False\n",
            "06:07:05 |     bpe_dropout: None\n",
            "06:07:05 |     bpe_merge: None\n",
            "06:07:05 |     bpe_vocab: None\n",
            "06:07:05 |     compute_tokenized_bleu: False\n",
            "06:07:05 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
            "06:07:05 |     datatype: test\n",
            "06:07:05 |     decoder: same\n",
            "06:07:05 |     delimiter: '\\n'\n",
            "06:07:05 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "06:07:05 |     dict_endtoken: __end__\n",
            "06:07:05 |     dict_file: from_scratch_empathetic_dialogues/model.dict\n",
            "06:07:05 |     dict_include_test: False\n",
            "06:07:05 |     dict_include_valid: False\n",
            "06:07:05 |     dict_initpath: None\n",
            "06:07:05 |     dict_language: english\n",
            "06:07:05 |     dict_loaded: True\n",
            "06:07:05 |     dict_lower: False\n",
            "06:07:05 |     dict_max_ngram_size: -1\n",
            "06:07:05 |     dict_maxexs: -1\n",
            "06:07:05 |     dict_maxtokens: -1\n",
            "06:07:05 |     dict_minfreq: 0\n",
            "06:07:05 |     dict_nulltoken: __null__\n",
            "06:07:05 |     dict_starttoken: __start__\n",
            "06:07:05 |     dict_textfields: text,labels\n",
            "06:07:05 |     dict_tokenizer: re\n",
            "06:07:05 |     dict_unktoken: __unk__\n",
            "06:07:05 |     display_examples: False\n",
            "06:07:05 |     download_path: None\n",
            "06:07:05 |     dropout: 0.1\n",
            "06:07:05 |     dynamic_batching: None\n",
            "06:07:05 |     embedding_projection: random\n",
            "06:07:05 |     embedding_type: random\n",
            "06:07:05 |     embeddingsize: 128\n",
            "06:07:05 |     eval_batchsize: None\n",
            "06:07:05 |     eval_dynamic_batching: None\n",
            "06:07:05 |     evaltask: None\n",
            "06:07:05 |     final_extra_opt: \n",
            "06:07:05 |     force_fp16_tokens: False\n",
            "06:07:05 |     fp16: False\n",
            "06:07:05 |     fp16_impl: safe\n",
            "06:07:05 |     gpu: -1\n",
            "06:07:05 |     gradient_clip: 0.1\n",
            "06:07:05 |     hiddensize: 128\n",
            "06:07:05 |     hide_labels: False\n",
            "06:07:05 |     history_add_global_end_token: None\n",
            "06:07:05 |     history_reversed: False\n",
            "06:07:05 |     history_size: -1\n",
            "06:07:05 |     image_cropsize: 224\n",
            "06:07:05 |     image_mode: raw\n",
            "06:07:05 |     image_size: 256\n",
            "06:07:05 |     inference: greedy\n",
            "06:07:05 |     init_model: None\n",
            "06:07:05 |     init_opt: None\n",
            "06:07:05 |     input_dropout: 0.0\n",
            "06:07:05 |     interactive_mode: False\n",
            "06:07:05 |     invsqrt_lr_decay_gamma: -1\n",
            "06:07:05 |     is_debug: False\n",
            "06:07:05 |     label_truncate: None\n",
            "06:07:05 |     learningrate: 1\n",
            "06:07:05 |     log_every_n_secs: -1\n",
            "06:07:05 |     log_every_n_steps: 50\n",
            "06:07:05 |     log_keep_fields: all\n",
            "06:07:05 |     loglevel: info\n",
            "06:07:05 |     lookuptable: all\n",
            "06:07:05 |     lr_scheduler: reduceonplateau\n",
            "06:07:05 |     lr_scheduler_decay: 0.5\n",
            "06:07:05 |     lr_scheduler_patience: 3\n",
            "06:07:05 |     max_train_steps: -1\n",
            "06:07:05 |     max_train_time: 60.0\n",
            "06:07:05 |     metrics: all\n",
            "06:07:05 |     model: seq2seq\n",
            "06:07:05 |     model_file: from_scratch_empathetic_dialogues/model\n",
            "06:07:05 |     momentum: 0\n",
            "06:07:05 |     multitask_weights: [1]\n",
            "06:07:05 |     mutators: None\n",
            "06:07:05 |     nesterov: True\n",
            "06:07:05 |     no_cuda: False\n",
            "06:07:05 |     num_epochs: -1\n",
            "06:07:05 |     num_examples: 2\n",
            "06:07:05 |     num_workers: 0\n",
            "06:07:05 |     numlayers: 2\n",
            "06:07:05 |     numsoftmax: 1\n",
            "06:07:05 |     nus: [0.7]\n",
            "06:07:05 |     optimizer: sgd\n",
            "06:07:05 |     override: \"{'datatype': 'test', 'task': 'empathetic_dialogues', 'model_file': 'from_scratch_empathetic_dialogues/model', 'num_examples': 2, 'skip_generation': False, 'metrics': 'all'}\"\n",
            "06:07:05 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
            "06:07:05 |     person_tokens: False\n",
            "06:07:05 |     rank_candidates: False\n",
            "06:07:05 |     report_filename: \n",
            "06:07:05 |     rnn_class: lstm\n",
            "06:07:05 |     save_after_valid: False\n",
            "06:07:05 |     save_every_n_secs: -1\n",
            "06:07:05 |     save_format: conversations\n",
            "06:07:05 |     short_final_eval: False\n",
            "06:07:05 |     skip_generation: False\n",
            "06:07:05 |     special_tok_lst: None\n",
            "06:07:05 |     split_lines: False\n",
            "06:07:05 |     starttime: Jan12_05-32\n",
            "06:07:05 |     task: empathetic_dialogues\n",
            "06:07:05 |     temperature: 1.0\n",
            "06:07:05 |     tensorboard_log: False\n",
            "06:07:05 |     tensorboard_logdir: None\n",
            "06:07:05 |     text_truncate: None\n",
            "06:07:05 |     topk: 10\n",
            "06:07:05 |     topp: 0.9\n",
            "06:07:05 |     train_experiencer_only: False\n",
            "06:07:05 |     truncate: 64\n",
            "06:07:05 |     update_freq: 1\n",
            "06:07:05 |     use_reply: label\n",
            "06:07:05 |     validation_cutoff: 1.0\n",
            "06:07:05 |     validation_every_n_epochs: -1\n",
            "06:07:05 |     validation_every_n_secs: -1\n",
            "06:07:05 |     validation_every_n_steps: -1\n",
            "06:07:05 |     validation_max_exs: -1\n",
            "06:07:05 |     validation_metric: accuracy\n",
            "06:07:05 |     validation_metric_mode: None\n",
            "06:07:05 |     validation_patience: 10\n",
            "06:07:05 |     validation_share_agent: False\n",
            "06:07:05 |     verbose: False\n",
            "06:07:05 |     wandb_entity: None\n",
            "06:07:05 |     wandb_log: False\n",
            "06:07:05 |     wandb_name: None\n",
            "06:07:05 |     wandb_project: None\n",
            "06:07:05 |     warmup_rate: 0.0001\n",
            "06:07:05 |     warmup_updates: -1\n",
            "06:07:05 |     weight_decay: None\n",
            "06:07:05 |     world_logs: \n",
            "06:07:06 | Evaluating task empathetic_dialogues using datatype test.\n",
            "06:07:06 | creating task(s): empathetic_dialogues\n",
            "06:07:06 | \u001b[33mROUGE requires nltk punkt tokenizer. Please run `python -c \"import nltk; nltk.download('punkt')`\u001b[0m\n",
            "06:07:06 | \u001b[1mReport for empathetic_dialogues:\n",
            "    accuracy  bleu-1    bleu-2    bleu-3    bleu-4  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs     f1  gen_n_toks  \\\n",
            "           0  .05556 5.893e-08 6.283e-10 6.743e-11    55  51.5  1435   .5000        3.5 27.83    2 .07143        15.5   \n",
            "    gpu_mem  interdistinct-1  interdistinct-2  intradistinct-1  intradistinct-2  llen  loss  ltpb  ltps  ltrunc  ltrunclen  \\\n",
            "    .003034            .2222            .3750            .3889            .5625    11  7.23    11 306.6       0          0   \n",
            "    ppl  token_acc  token_em  tpb  tps  \n",
            "   1380      .3182         0 62.5 1742\u001b[0m\n",
            "06:07:06 | Finished evaluating tasks ['empathetic_dialogues'] using datatype test\n",
            "    accuracy  bleu-1    bleu-2    bleu-3    bleu-4  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs     f1  gen_n_toks  \\\n",
            "           0  .05556 5.893e-08 6.283e-10 6.743e-11    55  51.5  1435   .5000        3.5 27.83    2 .07143        15.5   \n",
            "    gpu_mem  interdistinct-1  interdistinct-2  intradistinct-1  intradistinct-2  llen  loss  ltpb  ltps  ltrunc  ltrunclen  \\\n",
            "    .003034            .2222            .3750            .3889            .5625    11  7.23    11 306.6       0          0   \n",
            "    ppl  token_acc  token_em  tpb  tps  \n",
            "   1380      .3182         0 62.5 1742\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'exs': SumMetric(2),\n",
              " 'accuracy': ExactMatchMetric(0),\n",
              " 'f1': F1Metric(0.07143),\n",
              " 'bleu-1': BleuMetric(0.05556),\n",
              " 'bleu-2': BleuMetric(5.893e-08),\n",
              " 'bleu-3': BleuMetric(6.283e-10),\n",
              " 'bleu-4': BleuMetric(6.743e-11),\n",
              " 'interdistinct-1': InterDistinctMetric(0.2222),\n",
              " 'intradistinct-1': IntraDistinctMetric(0.3889),\n",
              " 'interdistinct-2': InterDistinctMetric(0.375),\n",
              " 'intradistinct-2': IntraDistinctMetric(0.5625),\n",
              " 'clen': AverageMetric(55),\n",
              " 'ctrunc': AverageMetric(0.5),\n",
              " 'ctrunclen': AverageMetric(3.5),\n",
              " 'llen': AverageMetric(11),\n",
              " 'ltrunc': AverageMetric(0),\n",
              " 'ltrunclen': AverageMetric(0),\n",
              " 'loss': AverageMetric(7.23),\n",
              " 'ppl': PPLMetric(1380),\n",
              " 'token_acc': AverageMetric(0.3182),\n",
              " 'token_em': AverageMetric(0),\n",
              " 'gen_n_toks': AverageMetric(15.5),\n",
              " 'exps': GlobalTimerMetric(27.83),\n",
              " 'ltpb': GlobalAverageMetric(11),\n",
              " 'ltps': GlobalTimerMetric(306.6),\n",
              " 'ctpb': GlobalAverageMetric(51.5),\n",
              " 'ctps': GlobalTimerMetric(1435),\n",
              " 'tpb': GlobalAverageMetric(62.5),\n",
              " 'tps': GlobalTimerMetric(1742),\n",
              " 'gpu_mem': GlobalAverageMetric(0.003034)}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. ParlAI에 데이터 추가하기"
      ],
      "metadata": {
        "id": "NecGveEpTigc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "txt format"
      ],
      "metadata": {
        "id": "WBSBp5SKdgQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"tmp_data.txt\", 'w') as f:\n",
        "    f.write(\"text:hello how are you today?\\tlabels:i'm great thanks! what are you doing?\\n\")\n",
        "    f.write(\"text:i've just been biking.\\tlabels:oh nice, i haven't got on a bike in years!\\tepisode_done:True\\n\")"
      ],
      "metadata": {
        "id": "PIjGCQ6zTtTq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!parlai display_data --task fromfile:parlaiformat --fromfile_datapath tmp_data.txt"
      ],
      "metadata": {
        "id": "dW_aPC7WTkT9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83aae670-20e0-422e-f471-1e2fa759685b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "06:08:57 | Opt:\n",
            "06:08:57 |     allow_missing_init_opts: False\n",
            "06:08:57 |     batchsize: 1\n",
            "06:08:57 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
            "06:08:57 |     datatype: train:ordered\n",
            "06:08:57 |     dict_class: None\n",
            "06:08:57 |     display_add_fields: \n",
            "06:08:57 |     download_path: None\n",
            "06:08:57 |     dynamic_batching: None\n",
            "06:08:57 |     fromfile_datapath: tmp_data.txt\n",
            "06:08:57 |     fromfile_datatype_extension: False\n",
            "06:08:57 |     hide_labels: False\n",
            "06:08:57 |     ignore_agent_reply: True\n",
            "06:08:57 |     image_cropsize: 224\n",
            "06:08:57 |     image_mode: raw\n",
            "06:08:57 |     image_size: 256\n",
            "06:08:57 |     init_model: None\n",
            "06:08:57 |     init_opt: None\n",
            "06:08:57 |     is_debug: False\n",
            "06:08:57 |     loglevel: info\n",
            "06:08:57 |     max_display_len: 1000\n",
            "06:08:57 |     model: None\n",
            "06:08:57 |     model_file: None\n",
            "06:08:57 |     multitask_weights: [1]\n",
            "06:08:57 |     mutators: None\n",
            "06:08:57 |     num_examples: 10\n",
            "06:08:57 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': 'tmp_data.txt'}\"\n",
            "06:08:57 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
            "06:08:57 |     starttime: Jan12_06-08\n",
            "06:08:57 |     task: fromfile:parlaiformat\n",
            "06:08:57 |     verbose: False\n",
            "06:08:58 | creating task(s): fromfile:parlaiformat\n",
            "06:08:58 | Loading ParlAI text data: tmp_data.txt\n",
            "\u001b[1;31m- - - NEW EPISODE: tmp_data.txt - - -\u001b[0;0m\n",
            "\u001b[0mhello how are you today?\u001b[0;0m\n",
            "   \u001b[1;94mi'm great thanks! what are you doing?\u001b[0;0m\n",
            "\u001b[0mi've just been biking.\u001b[0;0m\n",
            "   \u001b[1;94moh nice, i haven't got on a bike in years!\u001b[0;0m\n",
            "06:08:58 | epoch done\n",
            "06:08:58 | loaded 1 episodes with a total of 2 examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✨실습✨\n",
        "\n",
        "- 위의 터미널 명령어를 파이썬 코드로 변경하세요."
      ],
      "metadata": {
        "id": "aZH_jdXqeWXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from parlai.scripts.display_data import DisplayData\n",
        "DisplayData.main(task='fromfile:parlaiformat', fromfile_datapath='tmp_data.txt', num_examples=5)"
      ],
      "metadata": {
        "id": "1x4AmuHAeC7K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "364a8615-7cde-4a46-8bc6-3acec059487d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "06:16:45 | Opt:\n",
            "06:16:45 |     allow_missing_init_opts: False\n",
            "06:16:45 |     batchsize: 1\n",
            "06:16:45 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
            "06:16:45 |     datatype: train:ordered\n",
            "06:16:45 |     dict_class: None\n",
            "06:16:45 |     display_add_fields: \n",
            "06:16:45 |     download_path: None\n",
            "06:16:45 |     dynamic_batching: None\n",
            "06:16:45 |     fromfile_datapath: tmp_data.txt\n",
            "06:16:45 |     fromfile_datatype_extension: False\n",
            "06:16:45 |     hide_labels: False\n",
            "06:16:45 |     ignore_agent_reply: True\n",
            "06:16:45 |     image_cropsize: 224\n",
            "06:16:45 |     image_mode: raw\n",
            "06:16:45 |     image_size: 256\n",
            "06:16:45 |     init_model: None\n",
            "06:16:45 |     init_opt: None\n",
            "06:16:45 |     is_debug: False\n",
            "06:16:45 |     loglevel: info\n",
            "06:16:45 |     max_display_len: 1000\n",
            "06:16:45 |     model: None\n",
            "06:16:45 |     model_file: None\n",
            "06:16:45 |     multitask_weights: [1]\n",
            "06:16:45 |     mutators: None\n",
            "06:16:45 |     num_examples: 5\n",
            "06:16:45 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': 'tmp_data.txt', 'num_examples': 5}\"\n",
            "06:16:45 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
            "06:16:45 |     starttime: Jan12_06-16\n",
            "06:16:45 |     task: fromfile:parlaiformat\n",
            "06:16:45 |     verbose: False\n",
            "06:16:45 | creating task(s): fromfile:parlaiformat\n",
            "06:16:45 | Loading ParlAI text data: tmp_data.txt\n",
            "\u001b[1;31m- - - NEW EPISODE: tmp_data.txt - - -\u001b[0;0m\n",
            "\u001b[0mhello how are you today?\u001b[0;0m\n",
            "   \u001b[1;94mi'm great thanks! what are you doing?\u001b[0;0m\n",
            "\u001b[0mi've just been biking.\u001b[0;0m\n",
            "   \u001b[1;94moh nice, i haven't got on a bike in years!\u001b[0;0m\n",
            "06:16:45 | epoch done\n",
            "06:16:45 | loaded 1 episodes with a total of 2 examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "json format"
      ],
      "metadata": {
        "id": "ISZD70o4dhny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dict = {\"dialog\": \n",
        "              [[{\"id\": \"partner1\", \"text\": \"hello how are you today?\"},\n",
        "                {\"id\": \"partner2\", \"text\": \"i'm great thanks! what are you doing?\"},\n",
        "                {\"id\": \"partner1\", \"text\": \"i've just been bikinig.\"},\n",
        "                {\"id\": \"partner2\", \"text\": \"oh nice, i haven't got on a bike in years!\"}]]}"
      ],
      "metadata": {
        "id": "behODJv9diQj"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open(\"tmp_data.json\", \"w\") as json_file:\n",
        "    json.dump(data_dict, json_file)"
      ],
      "metadata": {
        "id": "FOA5bPOvdzah"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from parlai.scripts.display_data import DisplayData\n",
        "DisplayData.main(task='jsonfile', jsonfile_datapath='tmp_data.json', num_examples=5)"
      ],
      "metadata": {
        "id": "oy8d-QQHeefl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e735bc0-3a40-4144-ddb7-d6231b8e2b3c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "06:12:07 | Opt:\n",
            "06:12:07 |     allow_missing_init_opts: False\n",
            "06:12:07 |     batchsize: 1\n",
            "06:12:07 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
            "06:12:07 |     datatype: train:ordered\n",
            "06:12:07 |     dict_class: None\n",
            "06:12:07 |     display_add_fields: \n",
            "06:12:07 |     download_path: None\n",
            "06:12:07 |     dynamic_batching: None\n",
            "06:12:07 |     hide_labels: False\n",
            "06:12:07 |     ignore_agent_reply: True\n",
            "06:12:07 |     image_cropsize: 224\n",
            "06:12:07 |     image_mode: raw\n",
            "06:12:07 |     image_size: 256\n",
            "06:12:07 |     init_model: None\n",
            "06:12:07 |     init_opt: None\n",
            "06:12:07 |     is_debug: False\n",
            "06:12:07 |     jsonfile_datapath: tmp_data.json\n",
            "06:12:07 |     jsonfile_datatype_extension: False\n",
            "06:12:07 |     label_turns: secondspeaker\n",
            "06:12:07 |     loglevel: info\n",
            "06:12:07 |     max_display_len: 1000\n",
            "06:12:07 |     model: None\n",
            "06:12:07 |     model_file: None\n",
            "06:12:07 |     multitask_weights: [1]\n",
            "06:12:07 |     mutators: None\n",
            "06:12:07 |     num_examples: 5\n",
            "06:12:07 |     override: \"{'task': 'jsonfile', 'jsonfile_datapath': 'tmp_data.json', 'num_examples': 5}\"\n",
            "06:12:07 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
            "06:12:07 |     starttime: Jan12_06-12\n",
            "06:12:07 |     task: jsonfile\n",
            "06:12:07 |     verbose: False\n",
            "06:12:08 | creating task(s): jsonfile\n",
            "06:12:08 | [loading data from json file into task: tmp_data.json ]\n",
            "\u001b[1;31m- - - NEW EPISODE: tmp_data.json - - -\u001b[0;0m\n",
            "\u001b[0mhello how are you today?\u001b[0;0m\n",
            "   \u001b[1;94mi'm great thanks! what are you doing?\u001b[0;0m\n",
            "\u001b[0mi've just been bikinig.\u001b[0;0m\n",
            "   \u001b[1;94moh nice, i haven't got on a bike in years!\u001b[0;0m\n",
            "06:12:08 | epoch done\n",
            "06:12:08 | loaded 1 episodes with a total of 2 examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✨실습✨\n",
        "\n",
        "- 위의 파이썬 코드를 명령어로 변경하세요"
      ],
      "metadata": {
        "id": "Z72MF7tFenzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!parlai display_data --task jsonfile  --fromfile_datapath tmp_data.json"
      ],
      "metadata": {
        "id": "EisWXkzQd6JO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caee645a-02fc-4bdf-992d-60196261230f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: parlai display_data\n",
            "       [--help]\n",
            "       [--helpall]\n",
            "       [--init-opt INIT_OPT]\n",
            "       [--allow-missing-init-opts ALLOW_MISSING_INIT_OPTS]\n",
            "       [--task TASK]\n",
            "       [--datatype DATATYPE]\n",
            "       [--batchsize BATCHSIZE]\n",
            "       [--dynamic-batching {batchsort,full,None}]\n",
            "       [--verbose]\n",
            "       [--debug]\n",
            "       [--datapath DATAPATH]\n",
            "       [--model MODEL]\n",
            "       [--model-file MODEL_FILE]\n",
            "       [--init-model INIT_MODEL]\n",
            "       [--num-examples NUM_EXAMPLES]\n",
            "       [--max-display-len MAX_DISPLAY_LEN]\n",
            "       [--display-add-fields DISPLAY_ADD_FIELDS]\n",
            "       [--ignore-agent-reply IGNORE_AGENT_REPLY]\n",
            "       [--mutators MUTATORS]\n",
            "       [--jsonfile-datapath JSONFILE_DATAPATH]\n",
            "       [--jsonfile-datatype-extension JSONFILE_DATATYPE_EXTENSION]\n",
            "       [--label-turns {firstspeaker,secondspeaker,both}]\n",
            "\n",
            "Display data from a task\n",
            "\n",
            "optional arguments:\n",
            "  --help, --h\n",
            "    show this\n",
            "    help\n",
            "    message and\n",
            "    exit\n",
            "  --helpall\n",
            "    Show usage,\n",
            "    including\n",
            "    advanced\n",
            "    arguments.\n",
            "  --init-opt, --o INIT_OPT\n",
            "    Path to\n",
            "    json file\n",
            "    of options.\n",
            "    Note:\n",
            "    Further\n",
            "    Command-\n",
            "    line\n",
            "    arguments\n",
            "    override\n",
            "    file-based\n",
            "    options.\n",
            "    (default:\n",
            "    None)\n",
            "  --allow-missing-init-opts ALLOW_MISSING_INIT_OPTS\n",
            "    Warn\n",
            "    instead of\n",
            "    raising if\n",
            "    an argument\n",
            "    passed in\n",
            "    with\n",
            "    --init-opt\n",
            "    is not in\n",
            "    the target\n",
            "    opt.\n",
            "    (default:\n",
            "    False)\n",
            "  --task, --t TASK\n",
            "    ParlAI\n",
            "    task(s),\n",
            "    e.g. \"babi:\n",
            "    Task1\" or\n",
            "    \"babi,cbt\"\n",
            "    (default:\n",
            "    None)\n",
            "  --datatype, --dt DATATYPE\n",
            "    choose\n",
            "    from:\n",
            "    train, trai\n",
            "    n:ordered,\n",
            "    valid,\n",
            "    test. to\n",
            "    stream data\n",
            "    add\n",
            "    \":stream\"\n",
            "    to any\n",
            "    option\n",
            "    (e.g., trai\n",
            "    n:stream).\n",
            "    by default\n",
            "    train is\n",
            "    random with\n",
            "    replacement\n",
            "    , valid is\n",
            "    ordered,\n",
            "    test is\n",
            "    ordered.\n",
            "    (default: t\n",
            "    rain:ordere\n",
            "    d)\n",
            "  --batchsize, --bs BATCHSIZE\n",
            "    batch size\n",
            "    for\n",
            "    minibatch\n",
            "    training\n",
            "    schemes\n",
            "    (default:\n",
            "    1)\n",
            "  --dynamic-batching, --dynb {batchsort,full,None}\n",
            "    Use dynamic\n",
            "    batching\n",
            "    (default:\n",
            "    None)\n",
            "  --verbose, --v\n",
            "    Print all\n",
            "    messages\n",
            "  --debug\n",
            "    Enables\n",
            "    some debug\n",
            "    behavior\n",
            "  --datapath, --dp DATAPATH\n",
            "    path to\n",
            "    datasets,\n",
            "    defaults to\n",
            "    {parlai_dir\n",
            "    }/data\n",
            "    (default:\n",
            "    None)\n",
            "  --model, --m MODEL\n",
            "    the model\n",
            "    class name.\n",
            "    can match p\n",
            "    arlai/agent\n",
            "    s/<model>\n",
            "    for agents\n",
            "    in that\n",
            "    directory,\n",
            "    or can\n",
            "    provide a\n",
            "    fully\n",
            "    specified\n",
            "    module for\n",
            "    `from X\n",
            "    import Y`\n",
            "    via `-m\n",
            "    X:Y` (e.g.\n",
            "    `-m parlai.\n",
            "    agents.seq2\n",
            "    seq.seq2seq\n",
            "    :Seq2SeqAge\n",
            "    nt`)\n",
            "    (default:\n",
            "    None)\n",
            "  --model-file, --mf MODEL_FILE\n",
            "    model file\n",
            "    name for\n",
            "    loading and\n",
            "    saving\n",
            "    models\n",
            "    (default:\n",
            "    None)\n",
            "  --init-model, --im INIT_MODEL\n",
            "    Initialize\n",
            "    model\n",
            "    weights and\n",
            "    dict from\n",
            "    this file\n",
            "    (default:\n",
            "    None)\n",
            "  --num-examples, --n, --ne NUM_EXAMPLES\n",
            "  --max-display-len, --mdl MAX_DISPLAY_LEN\n",
            "  --display-add-fields DISPLAY_ADD_FIELDS\n",
            "    Display\n",
            "    these\n",
            "    fields when\n",
            "    verbose is\n",
            "    off (e.g.,\n",
            "    \"--display-\n",
            "    add-fields \n",
            "    label_candi\n",
            "    dates,beam_\n",
            "    texts\")\n",
            "    (default: )\n",
            "  --ignore-agent-reply IGNORE_AGENT_REPLY\n",
            "  --mutators, --mut MUTATORS\n",
            "    Apply one\n",
            "    or more\n",
            "    mutators to\n",
            "    the data.\n",
            "    (default:\n",
            "    None)\n",
            "\n",
            "optional arguments:\n",
            "  --help, --h\n",
            "    show this\n",
            "    help\n",
            "    message and\n",
            "    exit\n",
            "  --helpall\n",
            "    Show usage,\n",
            "    including\n",
            "    advanced\n",
            "    arguments.\n",
            "  --num-examples, --n, --ne NUM_EXAMPLES\n",
            "  --max-display-len, --mdl MAX_DISPLAY_LEN\n",
            "  --display-add-fields DISPLAY_ADD_FIELDS\n",
            "    Display\n",
            "    these\n",
            "    fields when\n",
            "    verbose is\n",
            "    off (e.g.,\n",
            "    \"--display-\n",
            "    add-fields \n",
            "    label_candi\n",
            "    dates,beam_\n",
            "    texts\")\n",
            "    (default: )\n",
            "  --ignore-agent-reply IGNORE_AGENT_REPLY\n",
            "\n",
            "Main ParlAI Arguments:\n",
            "  --init-opt, --o INIT_OPT\n",
            "    Path to\n",
            "    json file\n",
            "    of options.\n",
            "    Note:\n",
            "    Further\n",
            "    Command-\n",
            "    line\n",
            "    arguments\n",
            "    override\n",
            "    file-based\n",
            "    options.\n",
            "    (default:\n",
            "    None)\n",
            "  --allow-missing-init-opts ALLOW_MISSING_INIT_OPTS\n",
            "    Warn\n",
            "    instead of\n",
            "    raising if\n",
            "    an argument\n",
            "    passed in\n",
            "    with\n",
            "    --init-opt\n",
            "    is not in\n",
            "    the target\n",
            "    opt.\n",
            "    (default:\n",
            "    False)\n",
            "  --task, --t TASK\n",
            "    ParlAI\n",
            "    task(s),\n",
            "    e.g. \"babi:\n",
            "    Task1\" or\n",
            "    \"babi,cbt\"\n",
            "    (default:\n",
            "    None)\n",
            "  --datatype, --dt DATATYPE\n",
            "    choose\n",
            "    from:\n",
            "    train, trai\n",
            "    n:ordered,\n",
            "    valid,\n",
            "    test. to\n",
            "    stream data\n",
            "    add\n",
            "    \":stream\"\n",
            "    to any\n",
            "    option\n",
            "    (e.g., trai\n",
            "    n:stream).\n",
            "    by default\n",
            "    train is\n",
            "    random with\n",
            "    replacement\n",
            "    , valid is\n",
            "    ordered,\n",
            "    test is\n",
            "    ordered.\n",
            "    (default: t\n",
            "    rain:ordere\n",
            "    d)\n",
            "  --batchsize, --bs BATCHSIZE\n",
            "    batch size\n",
            "    for\n",
            "    minibatch\n",
            "    training\n",
            "    schemes\n",
            "    (default:\n",
            "    1)\n",
            "  --dynamic-batching, --dynb {batchsort,full,None}\n",
            "    Use dynamic\n",
            "    batching\n",
            "    (default:\n",
            "    None)\n",
            "  --verbose, --v\n",
            "    Print all\n",
            "    messages\n",
            "  --debug\n",
            "    Enables\n",
            "    some debug\n",
            "    behavior\n",
            "  --datapath, --dp DATAPATH\n",
            "    path to\n",
            "    datasets,\n",
            "    defaults to\n",
            "    {parlai_dir\n",
            "    }/data\n",
            "    (default:\n",
            "    None)\n",
            "\n",
            "ParlAI Model Arguments:\n",
            "  --model, --m MODEL\n",
            "    the model\n",
            "    class name.\n",
            "    can match p\n",
            "    arlai/agent\n",
            "    s/<model>\n",
            "    for agents\n",
            "    in that\n",
            "    directory,\n",
            "    or can\n",
            "    provide a\n",
            "    fully\n",
            "    specified\n",
            "    module for\n",
            "    `from X\n",
            "    import Y`\n",
            "    via `-m\n",
            "    X:Y` (e.g.\n",
            "    `-m parlai.\n",
            "    agents.seq2\n",
            "    seq.seq2seq\n",
            "    :Seq2SeqAge\n",
            "    nt`)\n",
            "    (default:\n",
            "    None)\n",
            "  --model-file, --mf MODEL_FILE\n",
            "    model file\n",
            "    name for\n",
            "    loading and\n",
            "    saving\n",
            "    models\n",
            "    (default:\n",
            "    None)\n",
            "  --init-model, --im INIT_MODEL\n",
            "    Initialize\n",
            "    model\n",
            "    weights and\n",
            "    dict from\n",
            "    this file\n",
            "    (default:\n",
            "    None)\n",
            "\n",
            "JsonFile Task Arguments:\n",
            "  --jsonfile-datapath, --jfdp JSONFILE_DATAPATH\n",
            "    Data file\n",
            "    (default:\n",
            "    None)\n",
            "  --jsonfile-datatype-extension, --jfdt JSONFILE_DATATYPE_EXTENSION\n",
            "    If true,\n",
            "    use _train.\n",
            "    jsonl, _val\n",
            "    id.jsonl,\n",
            "    _test.jsonl\n",
            "    file\n",
            "    extensions\n",
            "    (default:\n",
            "    False)\n",
            "  --label-turns {firstspeaker,secondspeaker,both}\n",
            "    which\n",
            "    speaker to\n",
            "    use as\n",
            "    label\n",
            "    (default: s\n",
            "    econdspeake\n",
            "    r)\n",
            "\n",
            "Parse Error: unrecognized arguments: --fromfile-datapath tmp_data.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터 task 직접 만들기"
      ],
      "metadata": {
        "id": "aX-hLevxewSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from parlai.core.teachers import register_teacher, DialogTeacher\n",
        "\n",
        "@register_teacher(\"my_teacher\")\n",
        "class MyTeacher(DialogTeacher):\n",
        "    def __init__(self, opt, shared=None):\n",
        "        opt['datafile'] = opt['datatype'].split(':')[0] + \".txt\"\n",
        "        super().__init__(opt, shared)\n",
        "    \n",
        "    def setup_data(self, datafile):\n",
        "        print(f\" ~~ Loading from {datafile} ~~ \")\n",
        "        \n",
        "        yield ('Hello', 'Hi'), True\n",
        "        yield ('How are you', 'I am fine'), False\n",
        "        yield (\"Let's say goodbye\", 'Goodbye!'), False\n",
        "        \n",
        "        yield (\"Hey\", \"hi there\"), True\n",
        "        yield (\"Deja vu?\", \"Deja vu!\"), False\n",
        "        yield (\"Last chance\", \"This is it\"), False\n",
        "        \n",
        "        \n",
        "DisplayData.main(task=\"my_teacher\")"
      ],
      "metadata": {
        "id": "vfjBZGxpSoP0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c6ecd78-ae64-46e9-f279-3ddb59bd312b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "06:24:47 | Opt:\n",
            "06:24:47 |     allow_missing_init_opts: False\n",
            "06:24:47 |     batchsize: 1\n",
            "06:24:47 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
            "06:24:47 |     datatype: train:ordered\n",
            "06:24:47 |     dict_class: None\n",
            "06:24:47 |     display_add_fields: \n",
            "06:24:47 |     download_path: None\n",
            "06:24:47 |     dynamic_batching: None\n",
            "06:24:47 |     hide_labels: False\n",
            "06:24:47 |     ignore_agent_reply: True\n",
            "06:24:47 |     image_cropsize: 224\n",
            "06:24:47 |     image_mode: raw\n",
            "06:24:47 |     image_size: 256\n",
            "06:24:47 |     init_model: None\n",
            "06:24:47 |     init_opt: None\n",
            "06:24:47 |     is_debug: False\n",
            "06:24:47 |     loglevel: info\n",
            "06:24:47 |     max_display_len: 1000\n",
            "06:24:47 |     model: None\n",
            "06:24:47 |     model_file: None\n",
            "06:24:47 |     multitask_weights: [1]\n",
            "06:24:47 |     mutators: None\n",
            "06:24:47 |     num_examples: 10\n",
            "06:24:47 |     override: \"{'task': 'my_teacher'}\"\n",
            "06:24:47 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
            "06:24:47 |     starttime: Jan12_06-24\n",
            "06:24:47 |     task: my_teacher\n",
            "06:24:47 |     verbose: False\n",
            "06:24:47 | creating task(s): my_teacher\n",
            " ~~ Loading from train.txt ~~ \n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher - - -\u001b[0;0m\n",
            "\u001b[0mHello\u001b[0;0m\n",
            "   \u001b[1;94mHi\u001b[0;0m\n",
            "\u001b[0mHow are you\u001b[0;0m\n",
            "   \u001b[1;94mI am fine\u001b[0;0m\n",
            "\u001b[0mLet's say goodbye\u001b[0;0m\n",
            "   \u001b[1;94mGoodbye!\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher - - -\u001b[0;0m\n",
            "\u001b[0mHey\u001b[0;0m\n",
            "   \u001b[1;94mhi there\u001b[0;0m\n",
            "\u001b[0mDeja vu?\u001b[0;0m\n",
            "   \u001b[1;94mDeja vu!\u001b[0;0m\n",
            "\u001b[0mLast chance\u001b[0;0m\n",
            "   \u001b[1;94mThis is it\u001b[0;0m\n",
            "06:24:47 | epoch done\n",
            "06:24:47 | loaded 2 episodes with a total of 6 examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✨실습✨\n",
        "\n",
        "- 위의 코드를 이용하여 자신만의 데이터를 만들어보세요"
      ],
      "metadata": {
        "id": "43oDOLuVe1Tg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from parlai.core.teachers import register_teacher, DialogTeacher\n",
        "\n",
        "@register_teacher(\"my_teacher\")\n",
        "class MyTeacher(DialogTeacher):\n",
        "    def __init__(self, opt, shared=None):\n",
        "        opt['datafile'] = opt['datatype'].split(':')[0] + \".txt\"\n",
        "        super().__init__(opt, shared)\n",
        "    \n",
        "    def setup_data(self, datafile):\n",
        "        print(f\" ~~ Loading from {datafile} ~~ \")\n",
        "        \n",
        "        yield ('Hello', 'Hi'), True\n",
        "        yield ('how are you today?', 'I am fine'), False\n",
        "        yield (\"Let's say goodbye\", 'Goodbye!'), False\n",
        "        \n",
        "        yield (\"Hey\", \"hi there\"), True\n",
        "        yield (\"Did you watch the Oscars last week?\", \"Yes\"), False\n",
        "        yield (\"Last chance\", \"This is it\"), False\n",
        "        \n",
        "        \n",
        "DisplayData.main(task=\"my_teacher\")"
      ],
      "metadata": {
        "id": "xEHA4wYLe4OF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3ddcb69-1b95-427b-cb48-0566812696a9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "06:27:48 | Opt:\n",
            "06:27:48 |     allow_missing_init_opts: False\n",
            "06:27:48 |     batchsize: 1\n",
            "06:27:48 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
            "06:27:48 |     datatype: train:ordered\n",
            "06:27:48 |     dict_class: None\n",
            "06:27:48 |     display_add_fields: \n",
            "06:27:48 |     download_path: None\n",
            "06:27:48 |     dynamic_batching: None\n",
            "06:27:48 |     hide_labels: False\n",
            "06:27:48 |     ignore_agent_reply: True\n",
            "06:27:48 |     image_cropsize: 224\n",
            "06:27:48 |     image_mode: raw\n",
            "06:27:48 |     image_size: 256\n",
            "06:27:48 |     init_model: None\n",
            "06:27:48 |     init_opt: None\n",
            "06:27:48 |     is_debug: False\n",
            "06:27:48 |     loglevel: info\n",
            "06:27:48 |     max_display_len: 1000\n",
            "06:27:48 |     model: None\n",
            "06:27:48 |     model_file: None\n",
            "06:27:48 |     multitask_weights: [1]\n",
            "06:27:48 |     mutators: None\n",
            "06:27:48 |     num_examples: 10\n",
            "06:27:48 |     override: \"{'task': 'my_teacher'}\"\n",
            "06:27:48 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
            "06:27:48 |     starttime: Jan12_06-27\n",
            "06:27:48 |     task: my_teacher\n",
            "06:27:48 |     verbose: False\n",
            "06:27:48 | creating task(s): my_teacher\n",
            " ~~ Loading from train.txt ~~ \n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher - - -\u001b[0;0m\n",
            "\u001b[0mHello\u001b[0;0m\n",
            "   \u001b[1;94mHi\u001b[0;0m\n",
            "\u001b[0mhow are you today?\u001b[0;0m\n",
            "   \u001b[1;94mI am fine\u001b[0;0m\n",
            "\u001b[0mLet's say goodbye\u001b[0;0m\n",
            "   \u001b[1;94mGoodbye!\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher - - -\u001b[0;0m\n",
            "\u001b[0mHey\u001b[0;0m\n",
            "   \u001b[1;94mhi there\u001b[0;0m\n",
            "\u001b[0mDid you watch the Oscars last week?\u001b[0;0m\n",
            "   \u001b[1;94mYes\u001b[0;0m\n",
            "\u001b[0mLast chance\u001b[0;0m\n",
            "   \u001b[1;94mThis is it\u001b[0;0m\n",
            "06:27:48 | epoch done\n",
            "06:27:48 | loaded 2 episodes with a total of 6 examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "실제 데이터 파일로 task 추가하기"
      ],
      "metadata": {
        "id": "LSyug_pHg_oJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from parlai.core.build_data import DownloadableFile\n",
        "\n",
        "RESOURCES = [\n",
        "    DownloadableFile(\n",
        "        'http://parl.ai/downloads/dailydialog/dailydialog.tar.gz',\n",
        "        'dailydialog.tar.gz',\n",
        "        'c3adb09bd715b9fa5cd1ac41613b7de61eb5afbe477826a6146abefef573e6bb',\n",
        "    )\n",
        "]\n",
        "\n",
        "\n",
        "version = 'v5.0'\n",
        "dpath = '/content/'\n",
        "\n",
        "for downloadable_file in RESOURCES:\n",
        "    downloadable_file.download_file(dpath)"
      ],
      "metadata": {
        "id": "lJvRi7k7hBYF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fda02385-383b-4b4d-87c0-b339e5c785a7"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "06:31:09 | Downloading http://parl.ai/downloads/dailydialog/dailydialog.tar.gz to /content/dailydialog.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading dailydialog.tar.gz: 100%|██████████| 2.72M/2.72M [00:01<00:00, 2.31MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dailydialog 대화 예시\n",
        "\n",
        "```\n",
        "{\"fold\": \"test\", \n",
        "\"topic\": \"politics\", \n",
        "\"dialogue\": \n",
        "  [{\"emotion\": \"no_emotion\", \n",
        "    \"act\": \"directive\", \n",
        "    \"text\": \"Hey man , you wanna buy some weed ?\"}, \n",
        "    {\"emotion\": \"surprise\", \n",
        "    \"act\": \"question\", \n",
        "    \"text\": \"Some what ?\"}, \n",
        "    {\"emotion\": \"no_emotion\", \n",
        "    \"act\": \"directive\", \n",
        "    \"text\": \"Weed ! You know ? Pot , Ganja , Mary Jane some chronic !\"}, \n",
        "    {\"emotion\": \"no_emotion\", \n",
        "    \"act\": \"commissive\", \n",
        "    \"text\": \"Oh , umm , no thanks .\"}, \n",
        "    {\"emotion\": \"no_emotion\", \n",
        "    \"act\": \"directive\", \n",
        "    \"text\": \"I also have blow if you prefer to do a few lines .\"}, \n",
        "    {\"emotion\": \"no_emotion\", \n",
        "    \"act\": \"commissive\", \n",
        "    \"text\": \"No , I am ok , really .\"}, \n",
        "    {\"emotion\": \"no_emotion\", \n",
        "    \"act\": \"directive\", \n",
        "    \"text\": \"Come on man ! I even got dope and acid ! Try some !\"}, \n",
        "    {\"emotion\": \"no_emotion\", \n",
        "    \"act\": \"question\", \n",
        "    \"text\": \"Do you really have all of these drugs ? Where do you get them from ?\"}, \n",
        "    {\"emotion\": \"no_emotion\", \n",
        "    \"act\": \"directive\", \n",
        "    \"text\": \"I got my connections ! Just tell me what you want and I ' ll even give you one ounce for free .\"}, \n",
        "    {\"emotion\": \"no_emotion\", \n",
        "    \"act\": \"commissive\", \n",
        "    \"text\": \"Sounds good ! Let ' s see , I want .\"}, \n",
        "    {\"emotion\": \"fear\", \n",
        "    \"act\": \"question\", \n",
        "    \"text\": \"Yeah ?\"}, \n",
        "    {\"emotion\": \"no_emotion\", \n",
        "    \"act\": \"directive\", \n",
        "    \"text\": \"I want you to put your hands behind your head ! You are under arrest !\"}]}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "UkcUMBwoiFlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from parlai.core.teachers import register_teacher, DialogTeacher\n",
        "import json\n",
        "\n",
        "@register_teacher(\"my_dailydialog\")\n",
        "class MyDailyDialog(DialogTeacher):\n",
        "    def __init__(self, opt, shared=None):\n",
        "        opt['datafile'] = opt['datatype'].split(':')[0] + \".json\"\n",
        "        super().__init__(opt, shared)\n",
        "        self.setup_data(opt['datafile'])\n",
        "    \n",
        "    def setup_data(self, datafile):\n",
        "        data_path = '/content/'\n",
        "        raw_data = []\n",
        "        with open(os.path.join(data_path, datafile), 'r') as f:\n",
        "            for line in f:\n",
        "                  turn_dict = json.loads(line)\n",
        "                  raw_data.append(turn_dict)\n",
        "\n",
        "        for epi in raw_data:\n",
        "            if len(epi['dialogue']) % 2 == 1:\n",
        "                yield ('__SILENCE__', epi['dialogue'][0]['text']), True\n",
        "                start_idx = 1\n",
        "            else:\n",
        "                start_idx = 0\n",
        "            for i in range(start_idx, len(epi['dialogue']), 2):\n",
        "                if i == 0:\n",
        "                    episode_done = True\n",
        "                else:\n",
        "                    episode_done = False\n",
        "                yield (epi['dialogue'][i]['text'], epi['dialogue'][i+1]['text']), episode_done\n",
        "        \n",
        "        \n",
        "DisplayData.main(task=\"my_dailydialog\")"
      ],
      "metadata": {
        "id": "ZcBcluarAhiz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40bd2415-050e-42b2-8576-ade8be46ab77"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "06:31:14 | Opt:\n",
            "06:31:14 |     allow_missing_init_opts: False\n",
            "06:31:14 |     batchsize: 1\n",
            "06:31:14 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
            "06:31:14 |     datatype: train:ordered\n",
            "06:31:14 |     dict_class: None\n",
            "06:31:14 |     display_add_fields: \n",
            "06:31:14 |     download_path: None\n",
            "06:31:14 |     dynamic_batching: None\n",
            "06:31:14 |     hide_labels: False\n",
            "06:31:14 |     ignore_agent_reply: True\n",
            "06:31:14 |     image_cropsize: 224\n",
            "06:31:14 |     image_mode: raw\n",
            "06:31:14 |     image_size: 256\n",
            "06:31:14 |     init_model: None\n",
            "06:31:14 |     init_opt: None\n",
            "06:31:14 |     is_debug: False\n",
            "06:31:14 |     loglevel: info\n",
            "06:31:14 |     max_display_len: 1000\n",
            "06:31:14 |     model: None\n",
            "06:31:14 |     model_file: None\n",
            "06:31:14 |     multitask_weights: [1]\n",
            "06:31:14 |     mutators: None\n",
            "06:31:14 |     num_examples: 10\n",
            "06:31:14 |     override: \"{'task': 'my_dailydialog'}\"\n",
            "06:31:14 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
            "06:31:14 |     starttime: Jan12_06-31\n",
            "06:31:14 |     task: my_dailydialog\n",
            "06:31:14 |     verbose: False\n",
            "06:31:14 | creating task(s): my_dailydialog\n",
            "\u001b[1;31m- - - NEW EPISODE: my_dailydialog - - -\u001b[0;0m\n",
            "\u001b[0mSay , Jim , how about going for a few beers after dinner ?\u001b[0;0m\n",
            "   \u001b[1;94mYou know that is tempting but is really not good for our fitness .\u001b[0;0m\n",
            "\u001b[0mWhat do you mean ? It will help us to relax .\u001b[0;0m\n",
            "   \u001b[1;94mDo you really think so ? I don't . It will just make us fat and act silly . Remember last time ?\u001b[0;0m\n",
            "\u001b[0mI guess you are right . But what shall we do ? I don't feel like sitting at home .\u001b[0;0m\n",
            "   \u001b[1;94mI suggest a walk over to the gym where we can play singsong and meet some of our friends .\u001b[0;0m\n",
            "\u001b[0mThat's a good idea . I hear Mary and Sally often go there to play pingpong . Perhaps we can make a foursome with them .\u001b[0;0m\n",
            "   \u001b[1;94mSounds great to me ! If they are willing , we could ask them to go dancing with us . That is excellent exercise and fun , too .\u001b[0;0m\n",
            "\u001b[0mGood . Let ' s go now .\u001b[0;0m\n",
            "   \u001b[1;94mAll right .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: my_dailydialog - - -\u001b[0;0m\n",
            "\u001b[0mCan you do push-ups ?\u001b[0;0m\n",
            "   \u001b[1;94mOf course I can . It's a piece of cake ! Believe it or not , I can do 30 push-ups a minute .\u001b[0;0m\n",
            "\u001b[0mReally ? I think that's impossible !\u001b[0;0m\n",
            "   \u001b[1;94mYou mean 30 push-ups ?\u001b[0;0m\n",
            "\u001b[0mYeah !\u001b[0;0m\n",
            "   \u001b[1;94mIt's easy . If you do exercise everyday , you can make it , too .\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: my_dailydialog - - -\u001b[0;0m\n",
            "\u001b[0m__SILENCE__\u001b[0;0m\n",
            "   \u001b[1;94mCan you study with the radio on ?\u001b[0;0m\n",
            "\u001b[0mNo , I listen to background music .\u001b[0;0m\n",
            "   \u001b[1;94mWhat is the difference ?\u001b[0;0m\n",
            "06:31:14 | loaded 11118 episodes with a total of 45533 examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "buOtjx994TC8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 불러오기"
      ],
      "metadata": {
        "id": "VkoJCaVzIZkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from parlai.scripts.display_data import DisplayData\n",
        "DisplayData.main(task='personachat', num_examples=5)"
      ],
      "metadata": {
        "id": "Pw_deqtFIWE0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0314c819-97c7-4ec9-f365-91444f63bce1"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "06:31:19 | Opt:\n",
            "06:31:19 |     allow_missing_init_opts: False\n",
            "06:31:19 |     batchsize: 1\n",
            "06:31:19 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
            "06:31:19 |     datatype: train:ordered\n",
            "06:31:19 |     dict_class: None\n",
            "06:31:19 |     display_add_fields: \n",
            "06:31:19 |     download_path: None\n",
            "06:31:19 |     dynamic_batching: None\n",
            "06:31:19 |     hide_labels: False\n",
            "06:31:19 |     ignore_agent_reply: True\n",
            "06:31:19 |     image_cropsize: 224\n",
            "06:31:19 |     image_mode: raw\n",
            "06:31:19 |     image_size: 256\n",
            "06:31:19 |     init_model: None\n",
            "06:31:19 |     init_opt: None\n",
            "06:31:19 |     is_debug: False\n",
            "06:31:19 |     loglevel: info\n",
            "06:31:19 |     max_display_len: 1000\n",
            "06:31:19 |     model: None\n",
            "06:31:19 |     model_file: None\n",
            "06:31:19 |     multitask_weights: [1]\n",
            "06:31:19 |     mutators: None\n",
            "06:31:19 |     num_examples: 5\n",
            "06:31:19 |     override: \"{'task': 'personachat', 'num_examples': 5}\"\n",
            "06:31:19 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
            "06:31:19 |     starttime: Jan12_06-31\n",
            "06:31:19 |     task: personachat\n",
            "06:31:19 |     verbose: False\n",
            "06:31:19 | creating task(s): personachat\n",
            "06:31:19 | loading fbdialog data: /usr/local/lib/python3.8/dist-packages/data/Persona-Chat/personachat/train_self_original.txt\n",
            "\u001b[1;31m- - - NEW EPISODE: personachat - - -\u001b[0;0m\n",
            "\u001b[0myour persona: i like to remodel homes.\n",
            "your persona: i like to go hunting.\n",
            "your persona: i like to shoot a bow.\n",
            "your persona: my favorite holiday is halloween.\n",
            "hi , how are you doing ? i am getting ready to do some cheetah chasing to stay in shape .\u001b[0;0m\n",
            "   \u001b[1;94myou must be very fast . hunting is one of my favorite hobbies .\u001b[0;0m\n",
            "\u001b[0mi am ! for my hobby i like to do canning or some whittling .\u001b[0;0m\n",
            "   \u001b[1;94mi also remodel homes when i am not out bow hunting .\u001b[0;0m\n",
            "\u001b[0mthat is neat . when i was in high school i placed 6th in 100m dash !\u001b[0;0m\n",
            "   \u001b[1;94mthat is awesome . do you have a favorite season or time of year ?\u001b[0;0m\n",
            "\u001b[0mi do not . but i do have a favorite meat since that is all i eat exclusively .\u001b[0;0m\n",
            "   \u001b[1;94mwhat is your favorite meat to eat ?\u001b[0;0m\n",
            "\u001b[0mi would have to say its prime rib . do you have any favorite foods ?\u001b[0;0m\n",
            "   \u001b[1;94mi like chicken or macaroni and cheese .\u001b[0;0m\n",
            "06:31:20 | loaded 8939 episodes with a total of 65719 examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!parlai display_data --task personachat --num-examples 5"
      ],
      "metadata": {
        "id": "hIH39SGCImbL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33e836dc-7e3e-4df9-ccac-c58e1441a493"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "06:31:32 | Opt:\n",
            "06:31:32 |     allow_missing_init_opts: False\n",
            "06:31:32 |     batchsize: 1\n",
            "06:31:32 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
            "06:31:32 |     datatype: train:ordered\n",
            "06:31:32 |     dict_class: None\n",
            "06:31:32 |     display_add_fields: \n",
            "06:31:32 |     download_path: None\n",
            "06:31:32 |     dynamic_batching: None\n",
            "06:31:32 |     hide_labels: False\n",
            "06:31:32 |     ignore_agent_reply: True\n",
            "06:31:32 |     image_cropsize: 224\n",
            "06:31:32 |     image_mode: raw\n",
            "06:31:32 |     image_size: 256\n",
            "06:31:32 |     init_model: None\n",
            "06:31:32 |     init_opt: None\n",
            "06:31:32 |     is_debug: False\n",
            "06:31:32 |     loglevel: info\n",
            "06:31:32 |     max_display_len: 1000\n",
            "06:31:32 |     model: None\n",
            "06:31:32 |     model_file: None\n",
            "06:31:32 |     multitask_weights: [1]\n",
            "06:31:32 |     mutators: None\n",
            "06:31:32 |     num_examples: 5\n",
            "06:31:32 |     override: \"{'task': 'personachat', 'num_examples': 5}\"\n",
            "06:31:32 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
            "06:31:32 |     starttime: Jan12_06-31\n",
            "06:31:32 |     task: personachat\n",
            "06:31:32 |     verbose: False\n",
            "06:31:33 | creating task(s): personachat\n",
            "06:31:33 | loading fbdialog data: /usr/local/lib/python3.8/dist-packages/data/Persona-Chat/personachat/train_self_original.txt\n",
            "\u001b[1;31m- - - NEW EPISODE: personachat - - -\u001b[0;0m\n",
            "\u001b[0myour persona: i like to remodel homes.\n",
            "your persona: i like to go hunting.\n",
            "your persona: i like to shoot a bow.\n",
            "your persona: my favorite holiday is halloween.\n",
            "hi , how are you doing ? i am getting ready to do some cheetah chasing to stay in shape .\u001b[0;0m\n",
            "   \u001b[1;94myou must be very fast . hunting is one of my favorite hobbies .\u001b[0;0m\n",
            "\u001b[0mi am ! for my hobby i like to do canning or some whittling .\u001b[0;0m\n",
            "   \u001b[1;94mi also remodel homes when i am not out bow hunting .\u001b[0;0m\n",
            "\u001b[0mthat is neat . when i was in high school i placed 6th in 100m dash !\u001b[0;0m\n",
            "   \u001b[1;94mthat is awesome . do you have a favorite season or time of year ?\u001b[0;0m\n",
            "\u001b[0mi do not . but i do have a favorite meat since that is all i eat exclusively .\u001b[0;0m\n",
            "   \u001b[1;94mwhat is your favorite meat to eat ?\u001b[0;0m\n",
            "\u001b[0mi would have to say its prime rib . do you have any favorite foods ?\u001b[0;0m\n",
            "   \u001b[1;94mi like chicken or macaroni and cheese .\u001b[0;0m\n",
            "06:31:34 | loaded 8939 episodes with a total of 65719 examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✨실습✨\n",
        "- 10개 턴을 display 하세요."
      ],
      "metadata": {
        "id": "a1XniXcIIoAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!parlai display_data --task personachat --num-examples 10"
      ],
      "metadata": {
        "id": "T45Wu4VSImuX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "483eb1d8-deec-418d-d0f2-75196e03525a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "06:35:58 | Opt:\n",
            "06:35:58 |     allow_missing_init_opts: False\n",
            "06:35:58 |     batchsize: 1\n",
            "06:35:58 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
            "06:35:58 |     datatype: train:ordered\n",
            "06:35:58 |     dict_class: None\n",
            "06:35:58 |     display_add_fields: \n",
            "06:35:58 |     download_path: None\n",
            "06:35:58 |     dynamic_batching: None\n",
            "06:35:58 |     hide_labels: False\n",
            "06:35:58 |     ignore_agent_reply: True\n",
            "06:35:58 |     image_cropsize: 224\n",
            "06:35:58 |     image_mode: raw\n",
            "06:35:58 |     image_size: 256\n",
            "06:35:58 |     init_model: None\n",
            "06:35:58 |     init_opt: None\n",
            "06:35:58 |     is_debug: False\n",
            "06:35:58 |     loglevel: info\n",
            "06:35:58 |     max_display_len: 1000\n",
            "06:35:58 |     model: None\n",
            "06:35:58 |     model_file: None\n",
            "06:35:58 |     multitask_weights: [1]\n",
            "06:35:58 |     mutators: None\n",
            "06:35:58 |     num_examples: 10\n",
            "06:35:58 |     override: \"{'task': 'personachat', 'num_examples': 10}\"\n",
            "06:35:58 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
            "06:35:58 |     starttime: Jan12_06-35\n",
            "06:35:58 |     task: personachat\n",
            "06:35:58 |     verbose: False\n",
            "06:35:59 | creating task(s): personachat\n",
            "06:35:59 | loading fbdialog data: /usr/local/lib/python3.8/dist-packages/data/Persona-Chat/personachat/train_self_original.txt\n",
            "\u001b[1;31m- - - NEW EPISODE: personachat - - -\u001b[0;0m\n",
            "\u001b[0myour persona: i like to remodel homes.\n",
            "your persona: i like to go hunting.\n",
            "your persona: i like to shoot a bow.\n",
            "your persona: my favorite holiday is halloween.\n",
            "hi , how are you doing ? i am getting ready to do some cheetah chasing to stay in shape .\u001b[0;0m\n",
            "   \u001b[1;94myou must be very fast . hunting is one of my favorite hobbies .\u001b[0;0m\n",
            "\u001b[0mi am ! for my hobby i like to do canning or some whittling .\u001b[0;0m\n",
            "   \u001b[1;94mi also remodel homes when i am not out bow hunting .\u001b[0;0m\n",
            "\u001b[0mthat is neat . when i was in high school i placed 6th in 100m dash !\u001b[0;0m\n",
            "   \u001b[1;94mthat is awesome . do you have a favorite season or time of year ?\u001b[0;0m\n",
            "\u001b[0mi do not . but i do have a favorite meat since that is all i eat exclusively .\u001b[0;0m\n",
            "   \u001b[1;94mwhat is your favorite meat to eat ?\u001b[0;0m\n",
            "\u001b[0mi would have to say its prime rib . do you have any favorite foods ?\u001b[0;0m\n",
            "   \u001b[1;94mi like chicken or macaroni and cheese .\u001b[0;0m\n",
            "\u001b[0mdo you have anything planned for today ? i think i am going to do some canning .\u001b[0;0m\n",
            "   \u001b[1;94mi am going to watch football . what are you canning ?\u001b[0;0m\n",
            "\u001b[0mi think i will can some jam . do you also play footfall for fun ?\u001b[0;0m\n",
            "   \u001b[1;94mif i have time outside of hunting and remodeling homes . which is not much !\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: personachat - - -\u001b[0;0m\n",
            "\u001b[0myour persona: my mom is my best friend.\n",
            "your persona: i have four sisters.\n",
            "your persona: i believe that mermaids are real.\n",
            "your persona: i love iced tea.\n",
            "hi , how are you doing today ?\u001b[0;0m\n",
            "   \u001b[1;94mi am spending time with my 4 sisters what are you up to\u001b[0;0m\n",
            "\u001b[0mwow , four sisters . just watching game of thrones .\u001b[0;0m\n",
            "   \u001b[1;94mthat is a good show i watch that while drinking iced tea\u001b[0;0m\n",
            "\u001b[0mi agree . what do you do for a living ?\u001b[0;0m\n",
            "   \u001b[1;94mi am a researcher i am researching the fact that mermaids are real\u001b[0;0m\n",
            "06:36:00 | loaded 8939 episodes with a total of 65719 examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 학습하기"
      ],
      "metadata": {
        "id": "LS28wyHaI22H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "seq2seq 모델 학습시키기"
      ],
      "metadata": {
        "id": "wYN2ZW-OJAWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf from_scratch_model\n",
        "!mkdir -p from_scratch_model\n",
        "\n",
        "from parlai.scripts.train_model import TrainModel\n",
        "TrainModel.main(\n",
        "    model_file='from_scratch/model',\n",
        "    task='personachat',\n",
        "    max_train_time=60,\n",
        "    batchsize=16,\n",
        "    model='seq2seq',\n",
        "    attention='dot',\n",
        "    lookuptable='all',\n",
        "    truncate=64,\n",
        "    skip_generation= True\n",
        ")"
      ],
      "metadata": {
        "id": "kuU28TifIxrZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "413a10fa-47d2-4080-d0a7-48533497c451"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "06:39:45 | building dictionary first...\n",
            "06:39:45 | No model with opt yet at: from_scratch/model(.opt)\n",
            "06:39:45 | Using CUDA\n",
            "06:39:45 | loading dictionary from from_scratch/model.dict\n",
            "06:39:45 | num words = 18745\n",
            "06:39:45 | Total parameters: 2,979,257 (2,979,257 trainable)\n",
            "06:39:45 | Opt:\n",
            "06:39:45 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "06:39:45 |     adam_eps: 1e-08\n",
            "06:39:45 |     add_p1_after_newln: False\n",
            "06:39:45 |     aggregate_micro: False\n",
            "06:39:45 |     allow_missing_init_opts: False\n",
            "06:39:45 |     attention: dot\n",
            "06:39:45 |     attention_length: 48\n",
            "06:39:45 |     attention_time: post\n",
            "06:39:45 |     batchsize: 16\n",
            "06:39:45 |     beam_block_full_context: True\n",
            "06:39:45 |     beam_block_list_filename: None\n",
            "06:39:45 |     beam_block_ngram: -1\n",
            "06:39:45 |     beam_context_block_ngram: -1\n",
            "06:39:45 |     beam_delay: 30\n",
            "06:39:45 |     beam_length_penalty: 0.65\n",
            "06:39:45 |     beam_min_length: 1\n",
            "06:39:45 |     beam_size: 1\n",
            "06:39:45 |     betas: '(0.9, 0.999)'\n",
            "06:39:45 |     bidirectional: False\n",
            "06:39:45 |     bpe_add_prefix_space: None\n",
            "06:39:45 |     bpe_debug: False\n",
            "06:39:45 |     bpe_dropout: None\n",
            "06:39:45 |     bpe_merge: None\n",
            "06:39:45 |     bpe_vocab: None\n",
            "06:39:45 |     compute_tokenized_bleu: False\n",
            "06:39:45 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
            "06:39:45 |     datatype: train\n",
            "06:39:45 |     decoder: same\n",
            "06:39:45 |     delimiter: '\\n'\n",
            "06:39:45 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "06:39:45 |     dict_endtoken: __end__\n",
            "06:39:45 |     dict_file: from_scratch/model.dict\n",
            "06:39:45 |     dict_include_test: False\n",
            "06:39:45 |     dict_include_valid: False\n",
            "06:39:45 |     dict_initpath: None\n",
            "06:39:45 |     dict_language: english\n",
            "06:39:45 |     dict_loaded: True\n",
            "06:39:45 |     dict_lower: False\n",
            "06:39:45 |     dict_max_ngram_size: -1\n",
            "06:39:45 |     dict_maxexs: -1\n",
            "06:39:45 |     dict_maxtokens: -1\n",
            "06:39:45 |     dict_minfreq: 0\n",
            "06:39:45 |     dict_nulltoken: __null__\n",
            "06:39:45 |     dict_starttoken: __start__\n",
            "06:39:45 |     dict_textfields: text,labels\n",
            "06:39:45 |     dict_tokenizer: re\n",
            "06:39:45 |     dict_unktoken: __unk__\n",
            "06:39:45 |     display_examples: False\n",
            "06:39:45 |     download_path: None\n",
            "06:39:45 |     dropout: 0.1\n",
            "06:39:45 |     dynamic_batching: None\n",
            "06:39:45 |     embedding_projection: random\n",
            "06:39:45 |     embedding_type: random\n",
            "06:39:45 |     embeddingsize: 128\n",
            "06:39:45 |     eval_batchsize: None\n",
            "06:39:45 |     eval_dynamic_batching: None\n",
            "06:39:45 |     evaltask: None\n",
            "06:39:45 |     final_extra_opt: \n",
            "06:39:45 |     force_fp16_tokens: False\n",
            "06:39:45 |     fp16: False\n",
            "06:39:45 |     fp16_impl: safe\n",
            "06:39:46 |     gpu: -1\n",
            "06:39:46 |     gradient_clip: 0.1\n",
            "06:39:46 |     hiddensize: 128\n",
            "06:39:46 |     hide_labels: False\n",
            "06:39:46 |     history_add_global_end_token: None\n",
            "06:39:46 |     history_reversed: False\n",
            "06:39:46 |     history_size: -1\n",
            "06:39:46 |     image_cropsize: 224\n",
            "06:39:46 |     image_mode: raw\n",
            "06:39:46 |     image_size: 256\n",
            "06:39:46 |     inference: greedy\n",
            "06:39:46 |     init_model: None\n",
            "06:39:46 |     init_opt: None\n",
            "06:39:46 |     input_dropout: 0.0\n",
            "06:39:46 |     interactive_mode: False\n",
            "06:39:46 |     invsqrt_lr_decay_gamma: -1\n",
            "06:39:46 |     is_debug: False\n",
            "06:39:46 |     label_truncate: None\n",
            "06:39:46 |     learningrate: 1\n",
            "06:39:46 |     load_from_checkpoint: True\n",
            "06:39:46 |     log_every_n_secs: -1\n",
            "06:39:46 |     log_every_n_steps: 50\n",
            "06:39:46 |     log_keep_fields: all\n",
            "06:39:46 |     loglevel: info\n",
            "06:39:46 |     lookuptable: all\n",
            "06:39:46 |     lr_scheduler: reduceonplateau\n",
            "06:39:46 |     lr_scheduler_decay: 0.5\n",
            "06:39:46 |     lr_scheduler_patience: 3\n",
            "06:39:46 |     max_train_steps: -1\n",
            "06:39:46 |     max_train_time: 60.0\n",
            "06:39:46 |     metrics: default\n",
            "06:39:46 |     model: seq2seq\n",
            "06:39:46 |     model_file: from_scratch/model\n",
            "06:39:46 |     momentum: 0\n",
            "06:39:46 |     multitask_weights: [1]\n",
            "06:39:46 |     mutators: None\n",
            "06:39:46 |     nesterov: True\n",
            "06:39:46 |     no_cuda: False\n",
            "06:39:46 |     num_epochs: -1\n",
            "06:39:46 |     num_workers: 0\n",
            "06:39:46 |     numlayers: 2\n",
            "06:39:46 |     numsoftmax: 1\n",
            "06:39:46 |     nus: (0.7,)\n",
            "06:39:46 |     optimizer: sgd\n",
            "06:39:46 |     override: \"{'model_file': 'from_scratch/model', 'task': 'personachat', 'max_train_time': 60.0, 'batchsize': 16, 'model': 'seq2seq', 'attention': 'dot', 'lookuptable': 'all', 'truncate': 64, 'skip_generation': True}\"\n",
            "06:39:46 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
            "06:39:46 |     person_tokens: False\n",
            "06:39:46 |     rank_candidates: False\n",
            "06:39:46 |     rnn_class: lstm\n",
            "06:39:46 |     save_after_valid: False\n",
            "06:39:46 |     save_every_n_secs: -1\n",
            "06:39:46 |     save_format: conversations\n",
            "06:39:46 |     short_final_eval: False\n",
            "06:39:46 |     skip_generation: True\n",
            "06:39:46 |     special_tok_lst: None\n",
            "06:39:46 |     split_lines: False\n",
            "06:39:46 |     starttime: Jan12_06-39\n",
            "06:39:46 |     task: personachat\n",
            "06:39:46 |     temperature: 1.0\n",
            "06:39:46 |     tensorboard_log: False\n",
            "06:39:46 |     tensorboard_logdir: None\n",
            "06:39:46 |     text_truncate: None\n",
            "06:39:46 |     topk: 10\n",
            "06:39:46 |     topp: 0.9\n",
            "06:39:46 |     truncate: 64\n",
            "06:39:46 |     update_freq: 1\n",
            "06:39:46 |     use_reply: label\n",
            "06:39:46 |     validation_cutoff: 1.0\n",
            "06:39:46 |     validation_every_n_epochs: -1\n",
            "06:39:46 |     validation_every_n_secs: -1\n",
            "06:39:46 |     validation_every_n_steps: -1\n",
            "06:39:46 |     validation_max_exs: -1\n",
            "06:39:46 |     validation_metric: accuracy\n",
            "06:39:46 |     validation_metric_mode: None\n",
            "06:39:46 |     validation_patience: 10\n",
            "06:39:46 |     validation_share_agent: False\n",
            "06:39:46 |     verbose: False\n",
            "06:39:46 |     wandb_entity: None\n",
            "06:39:46 |     wandb_log: False\n",
            "06:39:46 |     wandb_name: None\n",
            "06:39:46 |     wandb_project: None\n",
            "06:39:46 |     warmup_rate: 0.0001\n",
            "06:39:46 |     warmup_updates: -1\n",
            "06:39:46 |     weight_decay: None\n",
            "06:39:46 |     world_logs: \n",
            "06:39:46 | creating task(s): personachat\n",
            "06:39:46 | loading fbdialog data: /usr/local/lib/python3.8/dist-packages/data/Persona-Chat/personachat/train_self_original.txt\n",
            "06:39:47 | training...\n",
            "06:39:51 | time:4s total_exs:800 total_steps:50 epochs:0.01\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   140.4     1  1009 13971   .8875      77.33 221.5  800  1.154  .009178 12.95 8.815   1 207.2  2869       0          0 6733   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "       .09586         0                   50 1216 16840 13.85\n",
            "\n",
            "06:39:54 | time:6s total_exs:1600 total_steps:100 epochs:0.02\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   140.7     1  1009 19034   .9000      77.67 298.8  800  1.215   .00885 12.64 8.247   1 202.2  3813       0          0 3815   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "        .1328         0                  100 1211 22848 18.9\n",
            "\n",
            "06:39:56 | time:9s total_exs:2400 total_steps:150 epochs:0.04\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   144.5     1  1011 20222   .9038      81.36 320.1  800  1.233  .008944  12.9 8.123   1 206.5  4131       0          0 3371   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "        .1413         0                  150 1217 24353 20.02\n",
            "\n",
            "06:39:59 | time:12s total_exs:3200 total_steps:200 epochs:0.05\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   143.5     1  1010 16596   .8975      80.41   263  800  1.246   .01086 13.03 7.872   1 208.5  3427       0          0 2624   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "        .1609         0                  200 1218 20023 16.44\n",
            "\n",
            "06:40:01 | time:14s total_exs:4000 total_steps:250 epochs:0.06\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   145.3     1  1010 25753   .9050      82.22 407.7  800  1.296   .01062 12.73  7.73   1 203.8  5195       0          0 2276   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "        .1706         0                  250 1214 30948 25.51\n",
            "\n",
            "06:40:03 | time:16s total_exs:4800 total_steps:300 epochs:0.07\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   146.8     1  1012 25300   .8975      83.58 400.1  800   1.28  .008462 13.08 7.592   1 209.3  5235       0          0 1982   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "        .1836         0                  300 1221 30535 25.02\n",
            "\n",
            "06:40:06 | time:18s total_exs:5600 total_steps:350 epochs:0.09\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   144.9     1  1012 22287   .9113       81.7 352.4  800  1.346   .00883 12.71  7.46   1 203.3  4479       0          0 1737   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "        .1861         0                  350 1215 26766 22.04\n",
            "\n",
            "06:40:08 | time:20s total_exs:6400 total_steps:400 epochs:0.10\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   148.3     1  1010 23738   .8988      85.18 375.9  800  1.333  .008499 12.95 7.475   1 207.3  4870       0          0 1764   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "        .1788         0                  400 1218 28608 23.5\n",
            "\n",
            "06:40:10 | time:22s total_exs:7200 total_steps:450 epochs:0.11\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   142.3     1  1006 25460   .8912       79.4 404.8  800  1.343  .008944 12.94 7.412   1   207  5236       0          0 1655   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "        .1854         0                  450 1213 30696 25.31\n",
            "\n",
            "06:40:12 | time:24s total_exs:8000 total_steps:500 epochs:0.12\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   142.8     1  1010 25804   .9087      79.67 408.8  800  1.354  .008843 12.81  7.15   1   205  5236       0          0 1274   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "        .2027         0                  500 1215 31041 25.56\n",
            "\n",
            "06:40:14 | time:26s total_exs:8800 total_steps:550 epochs:0.13\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   145.2     1  1011 26386   .8975         82 417.8  800  1.371  .008493 13.03 7.091   1 208.4  5443       0          0 1201   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "        .2091         0                  550 1219 31829 26.12\n",
            "\n",
            "06:40:16 | time:28s total_exs:9600 total_steps:600 epochs:0.15\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   140.9     1  1008 26545   .8925      77.91 421.2  800  1.448  .009002 12.95 7.082   1 207.2  5455       0          0 1191   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "        .2034         0                  600 1215 32000 26.35\n",
            "\n",
            "06:40:18 | time:30s total_exs:10400 total_steps:650 epochs:0.16\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   145.4     1  1010 26038   .9062      82.29 412.5  800   1.38  .009198 13.17 6.999   1 210.7  5431       0          0 1096   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "        .2067         0                  650 1221 31469 25.79\n",
            "\n",
            "06:40:20 | time:32s total_exs:11200 total_steps:700 epochs:0.17\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   145.5     1  1011 25460   .9038      82.29 403.1  800  1.435  .008848 12.84 6.959   1 205.4  5175       0          0 1053   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "        .2095         0                  700 1216 30636 25.2\n",
            "\n",
            "06:40:22 | time:34s total_exs:12000 total_steps:750 epochs:0.18\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   143.1     1  1011 25997   .9025      79.91 411.4  800  1.451   .00849 12.76 6.796   1 204.2  5252       0          0 894.2   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "        .2258         0                  750 1215 31250 25.73\n",
            "\n",
            "06:40:24 | time:36s total_exs:12800 total_steps:800 epochs:0.19\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   143.4     1  1011 25846   .9038      80.24 409.1  800   1.43  .008857 12.63 6.835   1 202.1  5168       0          0 930.1   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "        .2100         0                  800 1213 31015 25.58\n",
            "\n",
            "06:40:26 | time:38s total_exs:13600 total_steps:850 epochs:0.21\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   147.7     1  1012 25596   .9150      84.47 404.7  800  1.451  .009669 13.21 6.795   1 211.4  5346       0          0 893.8   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "        .2135         0                  850 1223 30943 25.3\n",
            "\n",
            "06:40:27 | time:40s total_exs:14400 total_steps:900 epochs:0.22\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   146.2     1  1011 25793   .9050      82.99   408  800  1.424  .008999 13.24 6.694   1 211.8  5401       0          0 807.7   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "        .2229         0                  900 1223 31194 25.51\n",
            "\n",
            "06:40:29 | time:42s total_exs:15200 total_steps:950 epochs:0.23\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   142.1     1  1007 25993   .8912      79.12 412.8  800  1.495   .00882 12.76 6.756   1 204.1  5267       0          0 859.3   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "        .2135         0                  950 1211 31260 25.82\n",
            "\n",
            "06:40:32 | time:44s total_exs:16000 total_steps:1000 epochs:0.24\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   142.9     1  1009 23202   .8975      79.76 367.7  800  1.451  .009279 12.93 6.739   1 206.9  4756       0          0 844.4   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "        .2156         0                 1000 1216 27958 22.99\n",
            "\n",
            "06:40:34 | time:46s total_exs:16800 total_steps:1050 epochs:0.26\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   146.3     1  1011 25451   .9125      83.11 402.6  800  1.467  .009434 12.88 6.672   1   206  5184       0          0 789.8   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "        .2132         0                 1050 1217 30635 25.17\n",
            "\n",
            "06:40:36 | time:48s total_exs:17600 total_steps:1100 epochs:0.27\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   145.5     1  1011 25890   .9113      82.28 409.7  800  1.468  .008492 12.81 6.641   1   205  5251       0          0 765.7   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "        .2163         0                 1100 1216 31141 25.62\n",
            "\n",
            "06:40:38 | time:50s total_exs:18400 total_steps:1150 epochs:0.28\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   141.7     1  1008 26668   .8975      78.72 423.2  800  1.463  .008478 12.72 6.608   1 203.5  5383       0          0  741   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "        .2273         0                 1150 1212 32052 26.46\n",
            "\n",
            "06:40:40 | time:52s total_exs:19200 total_steps:1200 epochs:0.29\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   142.9     1  1009 25652   .8950      79.83 406.7  800  1.424  .009437 13.34 6.575   1 213.4  5423       0          0 716.9   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "        .2236         0                 1200 1223 31075 25.43\n",
            "\n",
            "06:40:42 | time:54s total_exs:20000 total_steps:1250 epochs:0.30\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   149.2     1  1010 25440   .8962      86.15 403.2  800  1.502  .008507 12.88 6.426   1 206.1  5194       0          0  618   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "        .2310         0                 1250 1216 30634 25.21\n",
            "\n",
            "06:40:43 | time:56s total_exs:20800 total_steps:1300 epochs:0.32\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   144.5     1  1009 26478   .9025      81.42 419.9  800  1.481  .008484 12.88 6.568   1   206  5406       0          0 711.7   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "        .2140         0                 1300 1215 31884 26.25\n",
            "\n",
            "06:40:45 | time:58s total_exs:21600 total_steps:1350 epochs:0.33\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   150.2     1  1011 26538   .9000      87.05 420.1  800  1.478  .008497 13.24 6.551   1 211.8  5562       0          0 699.8   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "        .2224         0                 1350 1223 32099 26.26\n",
            "\n",
            "06:40:47 | time:60s total_exs:22400 total_steps:1400 epochs:0.34\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   141.4     1  1009 26886   .8950      78.41 426.5  800  1.488  .008851 12.56 6.361   1   201  5356       0          0 579.1   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "        .2378         0                 1400 1210 32242 26.67\n",
            "\n",
            "06:40:47 | max_train_time elapsed:60.02313947677612s\n",
            "06:40:48 | Using CUDA\n",
            "06:40:48 | loading dictionary from from_scratch/model.dict\n",
            "06:40:48 | num words = 18745\n",
            "06:40:48 | Total parameters: 2,979,257 (2,979,257 trainable)\n",
            "06:40:48 | Loading existing model params from from_scratch/model\n",
            "06:40:48 | creating task(s): personachat\n",
            "06:40:48 | loading fbdialog data: /usr/local/lib/python3.8/dist-packages/data/Persona-Chat/personachat/valid_self_original.txt\n",
            "06:40:48 | running eval: valid\n",
            "06:40:55 | eval completed in 7.04s\n",
            "06:40:55 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  token_acc  \\\n",
            "   153.2  1005 70361   .9171      89.78  1110 7801  .001756  13.1 6.284   1 207.8 14549       0          0  536      .2452   \n",
            "    token_em  total_train_updates  tpb   tps  \n",
            "           0                 1405 1213 84910\n",
            "\u001b[0m\n",
            "06:40:55 | creating task(s): personachat\n",
            "06:40:55 | loading fbdialog data: /usr/local/lib/python3.8/dist-packages/data/Persona-Chat/personachat/test_self_original.txt\n",
            "06:40:55 | running eval: test\n",
            "06:41:02 | eval completed in 6.68s\n",
            "06:41:02 | \u001b[1mtest:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
            "   149.6  1002 71298   .9087      86.32  1127 7512  .001761 12.96 6.247   1 205.4 14614       0          0 516.5      .2507   \n",
            "    token_em  total_train_updates  tpb   tps  \n",
            "           0                 1405 1208 85912\n",
            "\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'exs': SumMetric(7801),\n",
              "  'clen': AverageMetric(153.2),\n",
              "  'ctrunc': AverageMetric(0.9171),\n",
              "  'ctrunclen': AverageMetric(89.78),\n",
              "  'llen': AverageMetric(13.1),\n",
              "  'ltrunc': AverageMetric(0),\n",
              "  'ltrunclen': AverageMetric(0),\n",
              "  'loss': AverageMetric(6.284),\n",
              "  'ppl': PPLMetric(536),\n",
              "  'token_acc': AverageMetric(0.2452),\n",
              "  'token_em': AverageMetric(0),\n",
              "  'exps': GlobalTimerMetric(1110),\n",
              "  'ltpb': GlobalAverageMetric(207.8),\n",
              "  'ltps': GlobalTimerMetric(1.455e+04),\n",
              "  'ctpb': GlobalAverageMetric(1005),\n",
              "  'ctps': GlobalTimerMetric(7.036e+04),\n",
              "  'tpb': GlobalAverageMetric(1213),\n",
              "  'tps': GlobalTimerMetric(8.491e+04),\n",
              "  'lr': GlobalAverageMetric(1),\n",
              "  'gpu_mem': GlobalAverageMetric(0.001756),\n",
              "  'total_train_updates': GlobalFixedMetric(1405)},\n",
              " {'exs': SumMetric(7512),\n",
              "  'clen': AverageMetric(149.6),\n",
              "  'ctrunc': AverageMetric(0.9087),\n",
              "  'ctrunclen': AverageMetric(86.32),\n",
              "  'llen': AverageMetric(12.96),\n",
              "  'ltrunc': AverageMetric(0),\n",
              "  'ltrunclen': AverageMetric(0),\n",
              "  'loss': AverageMetric(6.247),\n",
              "  'ppl': PPLMetric(516.5),\n",
              "  'token_acc': AverageMetric(0.2507),\n",
              "  'token_em': AverageMetric(0),\n",
              "  'exps': GlobalTimerMetric(1127),\n",
              "  'ltpb': GlobalAverageMetric(205.4),\n",
              "  'ltps': GlobalTimerMetric(1.461e+04),\n",
              "  'ctpb': GlobalAverageMetric(1002),\n",
              "  'ctps': GlobalTimerMetric(7.13e+04),\n",
              "  'tpb': GlobalAverageMetric(1208),\n",
              "  'tps': GlobalTimerMetric(8.591e+04),\n",
              "  'lr': GlobalAverageMetric(1),\n",
              "  'gpu_mem': GlobalAverageMetric(0.001761),\n",
              "  'total_train_updates': GlobalFixedMetric(1405)})"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "finetuning 진행하기\n"
      ],
      "metadata": {
        "id": "DqJH3P26JOGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf from_pretrained\n",
        "!mkdir -p from_pretrained\n",
        "\n",
        "TrainModel.main(\n",
        "    task='personachat', \n",
        "    model='transformer/generator',\n",
        "    model_file='from_pretrained/model',\n",
        "    \n",
        "    init_model='zoo:tutorial_transformer_generator/model',\n",
        "    \n",
        "    n_heads=16, n_layers=8, n_positions=512, text_truncate=512,\n",
        "    label_truncate=128, ffn_size=2048, embedding_size=512,\n",
        "    activation='gelu', variant='xlm',\n",
        "    dict_lower=True, dict_tokenizer='bpe',\n",
        "    dict_file='zoo:tutorial_transformer_generator/model.dict',\n",
        "    learn_positional_embeddings=True,\n",
        "    \n",
        "    lr=1e-5, optimizer='adam',\n",
        "    warmup_updates=100,\n",
        "    validation_metric='ppl',\n",
        "    max_train_time=60*2, validation_every_n_epochs=0.25,\n",
        "    \n",
        "    batchsize=12, fp16=True, fp16_impl='mem_efficient',\n",
        "    \n",
        "    skip_generation=True,\n",
        "    \n",
        "    dynamic_batching='full',\n",
        ")"
      ],
      "metadata": {
        "id": "AqnmOQqbJMkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "bi-encoder 모델 학습하기"
      ],
      "metadata": {
        "id": "42dUVtKPJwSi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✨실습✨\n",
        "\n",
        "- parlai 모델 목록 중 bi-encoder를 찾아 from scratch로 학습시키세요"
      ],
      "metadata": {
        "id": "e1W95dtdJ0CM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf from_scratch_bi_model\n",
        "!mkdir -p from_scratch_bi_model\n",
        "\n",
        "from parlai.scripts.train_model import TrainModel\n",
        "TrainModel.main(\n",
        "    model_file='from_scratch_bi_model/model',\n",
        "    task='personachat',\n",
        "    max_train_time=60,\n",
        "    batchsize=16,\n",
        "    model='transformer/biencoder',\n",
        ")"
      ],
      "metadata": {
        "id": "QlSzcXO6JTxl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1b5aa88-fcdd-4889-f6ba-13da855e3164"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "06:41:22 | building dictionary first...\n",
            "06:41:22 | Opt:\n",
            "06:41:22 |     activation: relu\n",
            "06:41:22 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "06:41:22 |     adam_eps: 1e-08\n",
            "06:41:22 |     add_p1_after_newln: False\n",
            "06:41:22 |     aggregate_micro: False\n",
            "06:41:22 |     allow_missing_init_opts: False\n",
            "06:41:22 |     attention_dropout: 0.0\n",
            "06:41:22 |     batchsize: 1\n",
            "06:41:22 |     betas: '(0.9, 0.999)'\n",
            "06:41:22 |     bpe_add_prefix_space: None\n",
            "06:41:22 |     bpe_debug: False\n",
            "06:41:22 |     bpe_dropout: None\n",
            "06:41:22 |     bpe_merge: None\n",
            "06:41:22 |     bpe_vocab: None\n",
            "06:41:22 |     candidates: inline\n",
            "06:41:22 |     cap_num_predictions: 100\n",
            "06:41:22 |     checkpoint_activations: False\n",
            "06:41:22 |     data_parallel: False\n",
            "06:41:22 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
            "06:41:22 |     datatype: train\n",
            "06:41:22 |     delimiter: '\\n'\n",
            "06:41:22 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "06:41:22 |     dict_endtoken: __end__\n",
            "06:41:22 |     dict_file: from_scratch_bi_model/model.dict\n",
            "06:41:22 |     dict_include_test: False\n",
            "06:41:22 |     dict_include_valid: False\n",
            "06:41:23 |     dict_initpath: None\n",
            "06:41:23 |     dict_language: english\n",
            "06:41:23 |     dict_loaded: False\n",
            "06:41:23 |     dict_lower: False\n",
            "06:41:23 |     dict_max_ngram_size: -1\n",
            "06:41:23 |     dict_maxexs: -1\n",
            "06:41:23 |     dict_maxtokens: -1\n",
            "06:41:23 |     dict_minfreq: 0\n",
            "06:41:23 |     dict_nulltoken: __null__\n",
            "06:41:23 |     dict_starttoken: __start__\n",
            "06:41:23 |     dict_textfields: text,labels\n",
            "06:41:23 |     dict_tokenizer: re\n",
            "06:41:23 |     dict_unktoken: __unk__\n",
            "06:41:23 |     display_examples: False\n",
            "06:41:23 |     download_path: None\n",
            "06:41:23 |     dropout: 0.0\n",
            "06:41:23 |     dynamic_batching: None\n",
            "06:41:23 |     embedding_projection: random\n",
            "06:41:23 |     embedding_size: 300\n",
            "06:41:23 |     embedding_type: random\n",
            "06:41:23 |     embeddings_scale: True\n",
            "06:41:23 |     encode_candidate_vecs: True\n",
            "06:41:23 |     encode_candidate_vecs_batchsize: 256\n",
            "06:41:23 |     eval_batchsize: None\n",
            "06:41:23 |     eval_candidates: inline\n",
            "06:41:23 |     eval_dynamic_batching: None\n",
            "06:41:23 |     evaltask: None\n",
            "06:41:23 |     ffn_size: 300\n",
            "06:41:23 |     final_extra_opt: \n",
            "06:41:23 |     fixed_candidate_vecs: reuse\n",
            "06:41:23 |     fixed_candidates_path: None\n",
            "06:41:23 |     force_fp16_tokens: False\n",
            "06:41:23 |     fp16: False\n",
            "06:41:23 |     fp16_impl: safe\n",
            "06:41:23 |     gpu: -1\n",
            "06:41:23 |     gradient_clip: 0.1\n",
            "06:41:23 |     hide_labels: False\n",
            "06:41:23 |     history_add_global_end_token: None\n",
            "06:41:23 |     history_reversed: False\n",
            "06:41:23 |     history_size: -1\n",
            "06:41:23 |     ignore_bad_candidates: False\n",
            "06:41:23 |     image_cropsize: 224\n",
            "06:41:23 |     image_mode: no_image_model\n",
            "06:41:23 |     image_size: 256\n",
            "06:41:23 |     inference: max\n",
            "06:41:23 |     init_model: None\n",
            "06:41:23 |     init_opt: None\n",
            "06:41:23 |     interactive_candidates: fixed\n",
            "06:41:23 |     interactive_mode: False\n",
            "06:41:23 |     invsqrt_lr_decay_gamma: -1\n",
            "06:41:23 |     is_debug: False\n",
            "06:41:23 |     label_truncate: None\n",
            "06:41:23 |     learn_embeddings: True\n",
            "06:41:23 |     learn_positional_embeddings: False\n",
            "06:41:23 |     learningrate: 0.0001\n",
            "06:41:23 |     load_from_checkpoint: True\n",
            "06:41:23 |     log_every_n_secs: -1\n",
            "06:41:23 |     log_every_n_steps: 50\n",
            "06:41:23 |     log_keep_fields: all\n",
            "06:41:23 |     loglevel: info\n",
            "06:41:23 |     lr_scheduler: reduceonplateau\n",
            "06:41:23 |     lr_scheduler_decay: 0.5\n",
            "06:41:23 |     lr_scheduler_patience: 3\n",
            "06:41:23 |     max_train_steps: -1\n",
            "06:41:23 |     max_train_time: 60.0\n",
            "06:41:23 |     memory_attention: sqrt\n",
            "06:41:23 |     metrics: default\n",
            "06:41:23 |     model: transformer/biencoder\n",
            "06:41:23 |     model_file: from_scratch_bi_model/model\n",
            "06:41:23 |     model_parallel: False\n",
            "06:41:23 |     momentum: 0\n",
            "06:41:23 |     multitask_weights: [1]\n",
            "06:41:23 |     mutators: None\n",
            "06:41:23 |     n_decoder_layers: -1\n",
            "06:41:23 |     n_encoder_layers: -1\n",
            "06:41:23 |     n_heads: 2\n",
            "06:41:23 |     n_layers: 2\n",
            "06:41:23 |     n_positions: None\n",
            "06:41:23 |     n_segments: 0\n",
            "06:41:23 |     nesterov: True\n",
            "06:41:23 |     no_cuda: False\n",
            "06:41:23 |     normalize_sent_emb: False\n",
            "06:41:23 |     num_epochs: -1\n",
            "06:41:23 |     num_workers: 0\n",
            "06:41:23 |     nus: (0.7,)\n",
            "06:41:23 |     optimizer: adamax\n",
            "06:41:23 |     output_scaling: 1.0\n",
            "06:41:23 |     override: \"{'model_file': 'from_scratch_bi_model/model', 'task': 'personachat', 'max_train_time': 60.0, 'batchsize': 16, 'model': 'transformer/biencoder'}\"\n",
            "06:41:23 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
            "06:41:23 |     person_tokens: False\n",
            "06:41:23 |     rank_candidates: False\n",
            "06:41:23 |     rank_top_k: -1\n",
            "06:41:23 |     reduction_type: mean\n",
            "06:41:23 |     relu_dropout: 0.0\n",
            "06:41:23 |     repeat_blocking_heuristic: True\n",
            "06:41:23 |     return_cand_scores: False\n",
            "06:41:23 |     save_after_valid: False\n",
            "06:41:23 |     save_every_n_secs: -1\n",
            "06:41:23 |     save_format: conversations\n",
            "06:41:23 |     share_encoders: True\n",
            "06:41:23 |     share_word_embeddings: True\n",
            "06:41:23 |     short_final_eval: False\n",
            "06:41:23 |     special_tok_lst: None\n",
            "06:41:23 |     split_lines: False\n",
            "06:41:23 |     starttime: Jan12_06-41\n",
            "06:41:23 |     task: personachat\n",
            "06:41:23 |     tensorboard_log: False\n",
            "06:41:23 |     tensorboard_logdir: None\n",
            "06:41:23 |     text_truncate: None\n",
            "06:41:23 |     topk: 5\n",
            "06:41:23 |     train_predict: False\n",
            "06:41:23 |     truncate: 1024\n",
            "06:41:23 |     update_freq: 1\n",
            "06:41:23 |     use_memories: False\n",
            "06:41:23 |     use_reply: label\n",
            "06:41:23 |     validation_cutoff: 1.0\n",
            "06:41:23 |     validation_every_n_epochs: -1\n",
            "06:41:23 |     validation_every_n_secs: -1\n",
            "06:41:23 |     validation_every_n_steps: -1\n",
            "06:41:23 |     validation_max_exs: -1\n",
            "06:41:23 |     validation_metric: accuracy\n",
            "06:41:23 |     validation_metric_mode: None\n",
            "06:41:23 |     validation_patience: 10\n",
            "06:41:23 |     validation_share_agent: False\n",
            "06:41:23 |     variant: aiayn\n",
            "06:41:23 |     verbose: False\n",
            "06:41:23 |     wandb_entity: None\n",
            "06:41:23 |     wandb_log: False\n",
            "06:41:23 |     wandb_name: None\n",
            "06:41:23 |     wandb_project: None\n",
            "06:41:23 |     warmup_rate: 0.0001\n",
            "06:41:23 |     warmup_updates: -1\n",
            "06:41:23 |     weight_decay: None\n",
            "06:41:23 |     world_logs: \n",
            "06:41:23 |     wrap_memory_encoder: False\n",
            "06:41:23 | creating task(s): personachat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBuilding dictionary:   0%|          | 0.00/65.7k [00:00<?, ?ex/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "06:41:23 | loading fbdialog data: /usr/local/lib/python3.8/dist-packages/data/Persona-Chat/personachat/train_self_original.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building dictionary: 100%|██████████| 65.7k/65.7k [00:03<00:00, 18.7kex/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "06:41:27 | Saving dictionary to from_scratch_bi_model/model.dict\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "06:41:27 | dictionary built with 18745 tokens in 0.0s\n",
            "06:41:27 | No model with opt yet at: from_scratch_bi_model/model(.opt)\n",
            "06:41:27 | Using CUDA\n",
            "06:41:27 | loading dictionary from from_scratch_bi_model/model.dict\n",
            "06:41:27 | num words = 18745\n",
            "06:41:27 | Total parameters: 7,197,300 (6,890,100 trainable)\n",
            "06:41:27 | Opt:\n",
            "06:41:27 |     activation: relu\n",
            "06:41:27 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "06:41:27 |     adam_eps: 1e-08\n",
            "06:41:27 |     add_p1_after_newln: False\n",
            "06:41:27 |     aggregate_micro: False\n",
            "06:41:27 |     allow_missing_init_opts: False\n",
            "06:41:27 |     attention_dropout: 0.0\n",
            "06:41:27 |     batchsize: 16\n",
            "06:41:27 |     betas: '(0.9, 0.999)'\n",
            "06:41:27 |     bpe_add_prefix_space: None\n",
            "06:41:27 |     bpe_debug: False\n",
            "06:41:27 |     bpe_dropout: None\n",
            "06:41:27 |     bpe_merge: None\n",
            "06:41:27 |     bpe_vocab: None\n",
            "06:41:27 |     candidates: inline\n",
            "06:41:27 |     cap_num_predictions: 100\n",
            "06:41:27 |     checkpoint_activations: False\n",
            "06:41:27 |     data_parallel: False\n",
            "06:41:27 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
            "06:41:27 |     datatype: train\n",
            "06:41:27 |     delimiter: '\\n'\n",
            "06:41:27 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "06:41:27 |     dict_endtoken: __end__\n",
            "06:41:27 |     dict_file: from_scratch_bi_model/model.dict\n",
            "06:41:27 |     dict_include_test: False\n",
            "06:41:27 |     dict_include_valid: False\n",
            "06:41:27 |     dict_initpath: None\n",
            "06:41:27 |     dict_language: english\n",
            "06:41:27 |     dict_loaded: True\n",
            "06:41:27 |     dict_lower: False\n",
            "06:41:27 |     dict_max_ngram_size: -1\n",
            "06:41:27 |     dict_maxexs: -1\n",
            "06:41:27 |     dict_maxtokens: -1\n",
            "06:41:27 |     dict_minfreq: 0\n",
            "06:41:27 |     dict_nulltoken: __null__\n",
            "06:41:27 |     dict_starttoken: __start__\n",
            "06:41:27 |     dict_textfields: text,labels\n",
            "06:41:27 |     dict_tokenizer: re\n",
            "06:41:27 |     dict_unktoken: __unk__\n",
            "06:41:27 |     display_examples: False\n",
            "06:41:27 |     download_path: None\n",
            "06:41:27 |     dropout: 0.0\n",
            "06:41:27 |     dynamic_batching: None\n",
            "06:41:27 |     embedding_projection: random\n",
            "06:41:27 |     embedding_size: 300\n",
            "06:41:27 |     embedding_type: random\n",
            "06:41:27 |     embeddings_scale: True\n",
            "06:41:27 |     encode_candidate_vecs: True\n",
            "06:41:27 |     encode_candidate_vecs_batchsize: 256\n",
            "06:41:27 |     eval_batchsize: None\n",
            "06:41:27 |     eval_candidates: inline\n",
            "06:41:27 |     eval_dynamic_batching: None\n",
            "06:41:27 |     evaltask: None\n",
            "06:41:27 |     ffn_size: 300\n",
            "06:41:27 |     final_extra_opt: \n",
            "06:41:27 |     fixed_candidate_vecs: reuse\n",
            "06:41:27 |     fixed_candidates_path: None\n",
            "06:41:27 |     force_fp16_tokens: False\n",
            "06:41:27 |     fp16: False\n",
            "06:41:27 |     fp16_impl: safe\n",
            "06:41:27 |     gpu: -1\n",
            "06:41:27 |     gradient_clip: 0.1\n",
            "06:41:27 |     hide_labels: False\n",
            "06:41:27 |     history_add_global_end_token: None\n",
            "06:41:27 |     history_reversed: False\n",
            "06:41:27 |     history_size: -1\n",
            "06:41:27 |     ignore_bad_candidates: False\n",
            "06:41:27 |     image_cropsize: 224\n",
            "06:41:27 |     image_mode: raw\n",
            "06:41:27 |     image_size: 256\n",
            "06:41:27 |     inference: max\n",
            "06:41:27 |     init_model: None\n",
            "06:41:27 |     init_opt: None\n",
            "06:41:27 |     interactive_candidates: fixed\n",
            "06:41:27 |     interactive_mode: False\n",
            "06:41:27 |     invsqrt_lr_decay_gamma: -1\n",
            "06:41:27 |     is_debug: False\n",
            "06:41:27 |     label_truncate: None\n",
            "06:41:27 |     learn_embeddings: True\n",
            "06:41:27 |     learn_positional_embeddings: False\n",
            "06:41:27 |     learningrate: 0.0001\n",
            "06:41:27 |     load_from_checkpoint: True\n",
            "06:41:27 |     log_every_n_secs: -1\n",
            "06:41:27 |     log_every_n_steps: 50\n",
            "06:41:27 |     log_keep_fields: all\n",
            "06:41:27 |     loglevel: info\n",
            "06:41:27 |     lr_scheduler: reduceonplateau\n",
            "06:41:27 |     lr_scheduler_decay: 0.5\n",
            "06:41:27 |     lr_scheduler_patience: 3\n",
            "06:41:27 |     max_train_steps: -1\n",
            "06:41:27 |     max_train_time: 60.0\n",
            "06:41:27 |     memory_attention: sqrt\n",
            "06:41:27 |     metrics: default\n",
            "06:41:27 |     model: transformer/biencoder\n",
            "06:41:27 |     model_file: from_scratch_bi_model/model\n",
            "06:41:27 |     model_parallel: False\n",
            "06:41:27 |     momentum: 0\n",
            "06:41:27 |     multitask_weights: [1]\n",
            "06:41:27 |     mutators: None\n",
            "06:41:27 |     n_decoder_layers: -1\n",
            "06:41:27 |     n_encoder_layers: -1\n",
            "06:41:27 |     n_heads: 2\n",
            "06:41:27 |     n_layers: 2\n",
            "06:41:27 |     n_positions: None\n",
            "06:41:27 |     n_segments: 0\n",
            "06:41:27 |     nesterov: True\n",
            "06:41:27 |     no_cuda: False\n",
            "06:41:27 |     normalize_sent_emb: False\n",
            "06:41:27 |     num_epochs: -1\n",
            "06:41:27 |     num_workers: 0\n",
            "06:41:27 |     nus: (0.7,)\n",
            "06:41:27 |     optimizer: adamax\n",
            "06:41:27 |     output_scaling: 1.0\n",
            "06:41:27 |     override: \"{'model_file': 'from_scratch_bi_model/model', 'task': 'personachat', 'max_train_time': 60.0, 'batchsize': 16, 'model': 'transformer/biencoder'}\"\n",
            "06:41:27 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
            "06:41:27 |     person_tokens: False\n",
            "06:41:27 |     rank_candidates: True\n",
            "06:41:27 |     rank_top_k: -1\n",
            "06:41:27 |     reduction_type: mean\n",
            "06:41:27 |     relu_dropout: 0.0\n",
            "06:41:27 |     repeat_blocking_heuristic: True\n",
            "06:41:27 |     return_cand_scores: False\n",
            "06:41:27 |     save_after_valid: False\n",
            "06:41:27 |     save_every_n_secs: -1\n",
            "06:41:27 |     save_format: conversations\n",
            "06:41:27 |     share_encoders: True\n",
            "06:41:27 |     share_word_embeddings: True\n",
            "06:41:27 |     short_final_eval: False\n",
            "06:41:27 |     special_tok_lst: None\n",
            "06:41:27 |     split_lines: False\n",
            "06:41:27 |     starttime: Jan12_06-41\n",
            "06:41:27 |     task: personachat\n",
            "06:41:27 |     tensorboard_log: False\n",
            "06:41:27 |     tensorboard_logdir: None\n",
            "06:41:27 |     text_truncate: None\n",
            "06:41:27 |     topk: 5\n",
            "06:41:27 |     train_predict: False\n",
            "06:41:27 |     truncate: 1024\n",
            "06:41:27 |     update_freq: 1\n",
            "06:41:27 |     use_memories: False\n",
            "06:41:27 |     use_reply: label\n",
            "06:41:27 |     validation_cutoff: 1.0\n",
            "06:41:27 |     validation_every_n_epochs: -1\n",
            "06:41:27 |     validation_every_n_secs: -1\n",
            "06:41:27 |     validation_every_n_steps: -1\n",
            "06:41:27 |     validation_max_exs: -1\n",
            "06:41:27 |     validation_metric: accuracy\n",
            "06:41:27 |     validation_metric_mode: None\n",
            "06:41:27 |     validation_patience: 10\n",
            "06:41:27 |     validation_share_agent: False\n",
            "06:41:27 |     variant: aiayn\n",
            "06:41:27 |     verbose: False\n",
            "06:41:27 |     wandb_entity: None\n",
            "06:41:27 |     wandb_log: False\n",
            "06:41:27 |     wandb_name: None\n",
            "06:41:27 |     wandb_project: None\n",
            "06:41:28 |     warmup_rate: 0.0001\n",
            "06:41:28 |     warmup_updates: -1\n",
            "06:41:28 |     weight_decay: None\n",
            "06:41:28 |     world_logs: \n",
            "06:41:28 |     wrap_memory_encoder: False\n",
            "06:41:28 | creating task(s): personachat\n",
            "06:41:28 | loading fbdialog data: /usr/local/lib/python3.8/dist-packages/data/Persona-Chat/personachat/train_self_original.txt\n",
            "06:41:29 | training...\n",
            "06:41:29 | \u001b[33m[ Executing train mode with provided inline set of candidates ]\u001b[0m\n",
            "06:41:29 | \u001b[33mSome training metrics are omitted for speed. Set the flag `--train-predict` to calculate train metrics.\u001b[0m\n",
            "06:41:32 | time:3s total_exs:800 total_steps:50 epochs:0.01\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen    lr  ltpb  ltps  ltrunc  ltrunclen  \\\n",
            "   139.9     1  2271 38597       0          0 271.9  800  14.99   .04446 13.95 .0001 223.3  3794       0          0   \n",
            "    mean_loss  total_train_updates  tpb   tps  ups  \n",
            "        3.126                   50 2494 42391   17\n",
            "\n",
            "06:41:35 | time:6s total_exs:1600 total_steps:100 epochs:0.02\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen    lr  ltpb  ltps  ltrunc  ltrunclen  \\\n",
            "   146.6     1  2378 40718       0          0   274  800  11.09   .03125 14.19 .0001   227  3887       0          0   \n",
            "    mean_loss  total_train_updates  tpb   tps   ups  \n",
            "        3.012                  100 2605 44605 17.13\n",
            "\n",
            "06:41:38 | time:9s total_exs:2400 total_steps:150 epochs:0.04\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen    lr  ltpb  ltps  ltrunc  ltrunclen  \\\n",
            "   144.4     1  2343 40397       0          0 275.8  800  9.485   .03194 14.28 .0001 228.5  3940       0          0   \n",
            "    mean_loss  total_train_updates  tpb   tps   ups  \n",
            "        2.971                  150 2572 44337 17.24\n",
            "\n",
            "06:41:41 | time:12s total_exs:3200 total_steps:200 epochs:0.05\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen    lr  ltpb  ltps  ltrunc  ltrunclen  \\\n",
            "   142.1     1  2306 39983       0          0 277.4  800  8.912    .0303 13.93 .0001 222.9  3864       0          0   \n",
            "    mean_loss  total_train_updates  tpb   tps   ups  \n",
            "        2.979                  200 2529 43847 17.34\n",
            "\n",
            "06:41:44 | time:15s total_exs:4000 total_steps:250 epochs:0.06\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen    lr  ltpb  ltps  ltrunc  ltrunclen  \\\n",
            "   143.8     1  2333 39767       0          0 272.7  800  8.581   .03627 13.61 .0001 217.7  3711       0          0   \n",
            "    mean_loss  total_train_updates  tpb   tps   ups  \n",
            "        2.937                  250 2551 43478 17.05\n",
            "\n",
            "06:41:47 | time:18s total_exs:4800 total_steps:300 epochs:0.07\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen    lr  ltpb  ltps  ltrunc  ltrunclen  \\\n",
            "   145.8     1  2364 40812       0          0 276.2  800  9.281   .02901 13.84 .0001 221.4  3822       0          0   \n",
            "    mean_loss  total_train_updates  tpb   tps   ups  \n",
            "        2.926                  300 2585 44634 17.27\n",
            "\n",
            "06:41:50 | time:21s total_exs:5600 total_steps:350 epochs:0.09\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen    lr  ltpb  ltps  ltrunc  ltrunclen  \\\n",
            "   141.3     1  2293 38473       0          0 268.5  800  7.786   .03039 13.63 .0001 218.1  3660       0          0   \n",
            "    mean_loss  total_train_updates  tpb   tps   ups  \n",
            "        2.913                  350 2511 42134 16.78\n",
            "\n",
            "06:41:53 | time:24s total_exs:6400 total_steps:400 epochs:0.10\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen    lr  ltpb  ltps  ltrunc  ltrunclen  \\\n",
            "   142.1     1  2306 38686       0          0 268.4  800  6.833   .04469 13.46 .0001 215.3  3611       0          0   \n",
            "    mean_loss  total_train_updates  tpb   tps   ups  \n",
            "        2.899                  400 2522 42297 16.78\n",
            "\n",
            "06:41:56 | time:27s total_exs:7200 total_steps:450 epochs:0.11\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen    lr  ltpb  ltps  ltrunc  ltrunclen  \\\n",
            "   143.9     1  2334 39466       0          0 270.5  800  7.797    .0301 14.18 .0001 226.9  3837       0          0   \n",
            "    mean_loss  total_train_updates  tpb   tps   ups  \n",
            "        2.908                  450 2561 43303 16.91\n",
            "\n",
            "06:41:59 | time:30s total_exs:8000 total_steps:500 epochs:0.12\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen    lr  ltpb  ltps  ltrunc  ltrunclen  \\\n",
            "   144.7     1  2348 39295       0          0 267.8  800  6.758   .04514 14.41 .0001 230.5  3858       0          0   \n",
            "    mean_loss  total_train_updates  tpb   tps   ups  \n",
            "        2.873                  500 2578 43153 16.74\n",
            "\n",
            "06:42:02 | time:33s total_exs:8800 total_steps:550 epochs:0.13\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen    lr  ltpb  ltps  ltrunc  ltrunclen  \\\n",
            "   144.7     1  2347 39263       0          0 267.7  800  7.258   .04577 13.91 .0001 222.6  3724       0          0   \n",
            "    mean_loss  total_train_updates  tpb   tps   ups  \n",
            "        2.874                  550 2570 42987 16.73\n",
            "\n",
            "06:42:05 | time:36s total_exs:9600 total_steps:600 epochs:0.15\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen    lr  ltpb  ltps  ltrunc  ltrunclen  \\\n",
            "   148.2     1  2402 39537       0          0 263.3  800  7.271   .04604 14.24 .0001 227.8  3749       0          0   \n",
            "    mean_loss  total_train_updates  tpb   tps   ups  \n",
            "        2.841                  600 2630 43287 16.46\n",
            "\n",
            "06:42:08 | time:39s total_exs:10400 total_steps:650 epochs:0.16\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen    lr  ltpb  ltps  ltrunc  ltrunclen  \\\n",
            "   140.9     1  2286 39869       0          0   279  800  7.022   .02844 13.57 .0001 217.1  3786       0          0   \n",
            "    mean_loss  total_train_updates  tpb   tps   ups  \n",
            "        2.841                  650 2503 43656 17.44\n",
            "\n",
            "06:42:11 | time:42s total_exs:11200 total_steps:700 epochs:0.17\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen    lr  ltpb  ltps  ltrunc  ltrunclen  \\\n",
            "   143.5     1  2327 39578       0          0 272.1  800  7.085   .03128 13.98 .0001 223.7  3805       0          0   \n",
            "    mean_loss  total_train_updates  tpb   tps   ups  \n",
            "        2.856                  700 2551 43383 17.01\n",
            "\n",
            "06:42:14 | time:45s total_exs:12000 total_steps:750 epochs:0.18\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen    lr  ltpb  ltps  ltrunc  ltrunclen  \\\n",
            "   143.2     1  2323 38946       0          0 268.2  800  7.338   .04576 13.71 .0001 219.3  3676       0          0   \n",
            "    mean_loss  total_train_updates  tpb   tps   ups  \n",
            "        2.855                  750 2543 42621 16.77\n",
            "\n",
            "06:42:17 | time:47s total_exs:12800 total_steps:800 epochs:0.19\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen    lr  ltpb  ltps  ltrunc  ltrunclen  \\\n",
            "   142.2     1  2308 40224       0          0 278.9  800  6.063   .02964 14.16 .0001 226.6  3950       0          0   \n",
            "    mean_loss  total_train_updates  tpb   tps   ups  \n",
            "        2.841                  800 2534 44175 17.43\n",
            "\n",
            "06:42:19 | time:50s total_exs:13600 total_steps:850 epochs:0.21\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen    lr  ltpb  ltps  ltrunc  ltrunclen  \\\n",
            "     141     1  2289 39489       0          0 276.1  800  6.785   .04514 13.53 .0001 216.4  3734       0          0   \n",
            "    mean_loss  total_train_updates  tpb   tps   ups  \n",
            "        2.871                  850 2505 43223 17.26\n",
            "\n",
            "06:42:22 | time:53s total_exs:14400 total_steps:900 epochs:0.22\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen    lr  ltpb  ltps  ltrunc  ltrunclen  \\\n",
            "   146.4     1  2374 40790       0          0 274.9  800  6.453   .04481  13.9 .0001 222.4  3822       0          0   \n",
            "    mean_loss  total_train_updates  tpb   tps   ups  \n",
            "        2.781                  900 2596 44612 17.19\n",
            "\n",
            "06:42:25 | time:56s total_exs:15200 total_steps:950 epochs:0.23\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen    lr  ltpb  ltps  ltrunc  ltrunclen  \\\n",
            "   146.1     1  2370 40140       0          0   271  800  7.122   .03164  14.6 .0001 233.6  3956       0          0   \n",
            "    mean_loss  total_train_updates  tpb   tps   ups  \n",
            "        2.829                  950 2604 44096 16.94\n",
            "\n",
            "06:42:28 | time:59s total_exs:16000 total_steps:1000 epochs:0.24\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen    lr  ltpb  ltps  ltrunc  ltrunclen  \\\n",
            "   144.3     1  2340 40369       0          0   276  800  6.679   .03753 13.66 .0001 218.5  3769       0          0   \n",
            "    mean_loss  total_train_updates  tpb   tps   ups  \n",
            "         2.79                 1000 2559 44138 17.25\n",
            "\n",
            "06:42:29 | max_train_time elapsed:60.02192544937134s\n",
            "06:42:29 | Using CUDA\n",
            "06:42:29 | loading dictionary from from_scratch_bi_model/model.dict\n",
            "06:42:29 | num words = 18745\n",
            "06:42:30 | Total parameters: 7,197,300 (6,890,100 trainable)\n",
            "06:42:30 | Loading existing model parameters from from_scratch_bi_model/model\n",
            "06:42:30 | creating task(s): personachat\n",
            "06:42:30 | loading fbdialog data: /usr/local/lib/python3.8/dist-packages/data/Persona-Chat/personachat/valid_self_original.txt\n",
            "06:42:30 | running eval: valid\n",
            "06:42:30 | \u001b[33m[ Executing eval mode with provided inline set of candidates ]\u001b[0m\n",
            "06:42:48 | eval completed in 17.90s\n",
            "06:42:48 | \u001b[1mvalid:\n",
            "    accuracy  bleu-4  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  gpu_mem  hits@1  hits@10  hits@100  hits@5  llen  \\\n",
            "       .1236   .1237 153.2  2460 67718       0          0 436.5 7801 .2123  .006239   .1236    .7057         1   .4240  14.1   \n",
            "    loss    lr  ltpb  ltps  ltrunc  ltrunclen   mrr  rank  total_train_updates  tpb   tps  \n",
            "   2.794 .0001 223.6  6156       0          0 .2829 7.616                 1014 2684 73874\n",
            "\u001b[0m\n",
            "06:42:48 | creating task(s): personachat\n",
            "06:42:48 | loading fbdialog data: /usr/local/lib/python3.8/dist-packages/data/Persona-Chat/personachat/test_self_original.txt\n",
            "06:42:48 | running eval: test\n",
            "06:43:04 | eval completed in 16.31s\n",
            "06:43:04 | \u001b[1mtest:\n",
            "    accuracy  bleu-4  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  gpu_mem  hits@1  hits@10  hits@100  hits@5  llen  \\\n",
            "       .1254   .1257 149.6  2402 69909       0          0 461.3 7512 .2155  .006241   .1254    .7125         1   .4418 13.96   \n",
            "    loss    lr  ltpb  ltps  ltrunc  ltrunclen   mrr  rank  total_train_updates  tpb   tps  \n",
            "   2.777 .0001 221.3  6441       0          0 .2886 7.466                 1014 2623 76349\n",
            "\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'exs': SumMetric(7801),\n",
              "  'accuracy': ExactMatchMetric(0.1236),\n",
              "  'f1': F1Metric(0.2123),\n",
              "  'bleu-4': BleuMetric(0.1237),\n",
              "  'hits@1': AverageMetric(0.1236),\n",
              "  'hits@5': AverageMetric(0.424),\n",
              "  'hits@10': AverageMetric(0.7057),\n",
              "  'hits@100': AverageMetric(1),\n",
              "  'clen': AverageMetric(153.2),\n",
              "  'ctrunc': AverageMetric(0),\n",
              "  'ctrunclen': AverageMetric(0),\n",
              "  'llen': AverageMetric(14.1),\n",
              "  'ltrunc': AverageMetric(0),\n",
              "  'ltrunclen': AverageMetric(0),\n",
              "  'loss': AverageMetric(2.794),\n",
              "  'rank': AverageMetric(7.616),\n",
              "  'mrr': AverageMetric(0.2829),\n",
              "  'exps': GlobalTimerMetric(436.5),\n",
              "  'ltpb': GlobalAverageMetric(223.6),\n",
              "  'ltps': GlobalTimerMetric(6156),\n",
              "  'ctpb': GlobalAverageMetric(2460),\n",
              "  'ctps': GlobalTimerMetric(6.772e+04),\n",
              "  'tpb': GlobalAverageMetric(2684),\n",
              "  'tps': GlobalTimerMetric(7.387e+04),\n",
              "  'lr': GlobalAverageMetric(0.0001),\n",
              "  'gpu_mem': GlobalAverageMetric(0.006239),\n",
              "  'total_train_updates': GlobalFixedMetric(1014)},\n",
              " {'exs': SumMetric(7512),\n",
              "  'accuracy': ExactMatchMetric(0.1254),\n",
              "  'f1': F1Metric(0.2155),\n",
              "  'bleu-4': BleuMetric(0.1257),\n",
              "  'hits@1': AverageMetric(0.1254),\n",
              "  'hits@5': AverageMetric(0.4418),\n",
              "  'hits@10': AverageMetric(0.7125),\n",
              "  'hits@100': AverageMetric(1),\n",
              "  'clen': AverageMetric(149.6),\n",
              "  'ctrunc': AverageMetric(0),\n",
              "  'ctrunclen': AverageMetric(0),\n",
              "  'llen': AverageMetric(13.96),\n",
              "  'ltrunc': AverageMetric(0),\n",
              "  'ltrunclen': AverageMetric(0),\n",
              "  'loss': AverageMetric(2.777),\n",
              "  'rank': AverageMetric(7.466),\n",
              "  'mrr': AverageMetric(0.2886),\n",
              "  'exps': GlobalTimerMetric(461.3),\n",
              "  'ltpb': GlobalAverageMetric(221.3),\n",
              "  'ltps': GlobalTimerMetric(6441),\n",
              "  'ctpb': GlobalAverageMetric(2402),\n",
              "  'ctps': GlobalTimerMetric(6.991e+04),\n",
              "  'tpb': GlobalAverageMetric(2623),\n",
              "  'tps': GlobalTimerMetric(7.635e+04),\n",
              "  'lr': GlobalAverageMetric(0.0001),\n",
              "  'gpu_mem': GlobalAverageMetric(0.006241),\n",
              "  'total_train_updates': GlobalFixedMetric(1014)})"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✨실습✨\n",
        "\n",
        "- parlai 모델 목록 중 bi-encoder를 찾아 fine-tuning으로 학습시키세요"
      ],
      "metadata": {
        "id": "3-SZib1fKH8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf from_pretrained_bi\n",
        "!mkdir -p from_pretrained_bi\n",
        "\n",
        "TrainModel.main(\n",
        "    task='personachat', \n",
        "    model='transformer/biencoder',\n",
        "    model_file='from_pretrained_bi/model',\n",
        "    \n",
        "    init_model='zoo:pretrained_transformers/bi_model_huge_reddit/model',\n",
        "    dict_file='zoo:pretrained_transformers/bi_model_huge_reddit/model.dict',\n",
        "    warmup_updates=100, lr_scheduler_patience=0,\n",
        "    lr_scheduler_decay=0.4, lr=5e-05, data_parallel=True,\n",
        "    history_size=20, label_truncate=72, text_truncate=360,\n",
        "    validation_metric='accuracy', validation_metric_mode='max',\n",
        "    candidates='batch', dict_tokenizer='bpe', dict_lower=True, optimizer='adamax',\n",
        "    output_scaling=0.06, variant='xlm', reduction_type='mean', share_encoders=False,\n",
        "    learn_positional_embeddings=True, n_layers=12, n_heads=12,\n",
        "    ffn_size=3072, attention_dropout=0.1, relu_dropout=0.0, dropout=0.1,\n",
        "    n_positions=1024, embedding_size=768, activation='gelu',\n",
        "    embeddings_scale=False, n_segments=2, learn_embeddings=True,\n",
        "    share_word_embeddings=False, dict_endtoken='__start__',\n",
        "    \n",
        "    max_train_time=60, validation_every_n_epochs=0.25, \n",
        "    batchsize=12, fp16=True,\n",
        ")"
      ],
      "metadata": {
        "id": "L4_k4FsCKH8p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bc3b81e-8ee7-4ec5-b60a-25d5b2896116"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "06:43:10 | building dictionary first...\n",
            "06:43:10 | No model with opt yet at: from_pretrained_bi/model(.opt)\n",
            "06:43:10 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: None,verbose: False,is_debug: False,datapath: /usr/local/lib/python3.8/dist-packages/data,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 60.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: -1,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: False,validation_every_n_epochs: 0.25,validation_max_exs: -1,short_final_eval: False,validation_patience: 10,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,load_from_checkpoint: True,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: -1,mutators: None,interactive_mode: False,fp16_impl: safe,force_fp16_tokens: False,adam_eps: 1e-08,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,invsqrt_lr_decay_gamma: -1,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None\u001b[0m\n",
            "06:43:10 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --single-turn False --rank-candidates True --candidates inline --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n",
            "06:43:10 | Using CUDA\n",
            "06:43:10 | loading dictionary from /usr/local/lib/python3.8/dist-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n",
            "06:43:11 | num words = 54944\n",
            "06:43:16 | Total parameters: 256,081,920 (256,081,920 trainable)\n",
            "06:43:16 | Loading existing model parameters from /usr/local/lib/python3.8/dist-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n",
            "06:43:22 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n",
            "06:43:22 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n",
            "06:43:22 | Opt:\n",
            "06:43:22 |     activation: gelu\n",
            "06:43:22 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "06:43:22 |     adam_eps: 1e-08\n",
            "06:43:22 |     add_p1_after_newln: False\n",
            "06:43:22 |     aggregate_micro: False\n",
            "06:43:22 |     allow_missing_init_opts: False\n",
            "06:43:22 |     attention_dropout: 0.1\n",
            "06:43:22 |     batchsize: 12\n",
            "06:43:22 |     betas: '(0.9, 0.999)'\n",
            "06:43:22 |     bpe_add_prefix_space: None\n",
            "06:43:22 |     bpe_debug: False\n",
            "06:43:22 |     bpe_dropout: None\n",
            "06:43:22 |     bpe_merge: None\n",
            "06:43:22 |     bpe_vocab: None\n",
            "06:43:22 |     candidates: batch\n",
            "06:43:22 |     cap_num_predictions: 100\n",
            "06:43:22 |     checkpoint_activations: False\n",
            "06:43:22 |     data_parallel: True\n",
            "06:43:22 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
            "06:43:22 |     datatype: train\n",
            "06:43:22 |     delimiter: '\\n'\n",
            "06:43:22 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "06:43:22 |     dict_endtoken: __start__\n",
            "06:43:22 |     dict_file: /usr/local/lib/python3.8/dist-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n",
            "06:43:22 |     dict_include_test: False\n",
            "06:43:22 |     dict_include_valid: False\n",
            "06:43:22 |     dict_initpath: None\n",
            "06:43:22 |     dict_language: english\n",
            "06:43:22 |     dict_loaded: True\n",
            "06:43:22 |     dict_lower: True\n",
            "06:43:22 |     dict_max_ngram_size: -1\n",
            "06:43:22 |     dict_maxexs: -1\n",
            "06:43:22 |     dict_maxtokens: -1\n",
            "06:43:22 |     dict_minfreq: 0\n",
            "06:43:22 |     dict_nulltoken: __null__\n",
            "06:43:22 |     dict_starttoken: __start__\n",
            "06:43:22 |     dict_textfields: text,labels\n",
            "06:43:22 |     dict_tokenizer: bpe\n",
            "06:43:22 |     dict_unktoken: __unk__\n",
            "06:43:22 |     display_examples: False\n",
            "06:43:22 |     download_path: None\n",
            "06:43:22 |     dropout: 0.1\n",
            "06:43:22 |     dynamic_batching: None\n",
            "06:43:22 |     embedding_projection: random\n",
            "06:43:22 |     embedding_size: 768\n",
            "06:43:22 |     embedding_type: random\n",
            "06:43:22 |     embeddings_scale: False\n",
            "06:43:22 |     encode_candidate_vecs: True\n",
            "06:43:22 |     encode_candidate_vecs_batchsize: 256\n",
            "06:43:22 |     eval_batchsize: None\n",
            "06:43:22 |     eval_candidates: inline\n",
            "06:43:22 |     eval_dynamic_batching: None\n",
            "06:43:22 |     evaltask: None\n",
            "06:43:22 |     ffn_size: 3072\n",
            "06:43:22 |     final_extra_opt: \n",
            "06:43:22 |     fixed_candidate_vecs: reuse\n",
            "06:43:22 |     fixed_candidates_path: None\n",
            "06:43:22 |     force_fp16_tokens: False\n",
            "06:43:22 |     fp16: True\n",
            "06:43:22 |     fp16_impl: safe\n",
            "06:43:22 |     gpu: -1\n",
            "06:43:22 |     gradient_clip: 0.1\n",
            "06:43:22 |     hide_labels: False\n",
            "06:43:22 |     history_add_global_end_token: None\n",
            "06:43:22 |     history_reversed: False\n",
            "06:43:22 |     history_size: 20\n",
            "06:43:22 |     ignore_bad_candidates: False\n",
            "06:43:22 |     image_cropsize: 224\n",
            "06:43:22 |     image_mode: raw\n",
            "06:43:22 |     image_size: 256\n",
            "06:43:22 |     inference: max\n",
            "06:43:22 |     init_model: /usr/local/lib/python3.8/dist-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n",
            "06:43:22 |     init_opt: None\n",
            "06:43:22 |     interactive_candidates: fixed\n",
            "06:43:22 |     interactive_mode: False\n",
            "06:43:22 |     invsqrt_lr_decay_gamma: -1\n",
            "06:43:22 |     is_debug: False\n",
            "06:43:22 |     label_truncate: 72\n",
            "06:43:22 |     learn_embeddings: True\n",
            "06:43:22 |     learn_positional_embeddings: True\n",
            "06:43:22 |     learningrate: 5e-05\n",
            "06:43:22 |     load_from_checkpoint: True\n",
            "06:43:22 |     log_every_n_secs: -1\n",
            "06:43:22 |     log_every_n_steps: 50\n",
            "06:43:22 |     log_keep_fields: all\n",
            "06:43:22 |     loglevel: info\n",
            "06:43:22 |     lr_scheduler: reduceonplateau\n",
            "06:43:22 |     lr_scheduler_decay: 0.4\n",
            "06:43:22 |     lr_scheduler_patience: 0\n",
            "06:43:22 |     max_train_steps: -1\n",
            "06:43:22 |     max_train_time: 60.0\n",
            "06:43:22 |     memory_attention: sqrt\n",
            "06:43:22 |     metrics: default\n",
            "06:43:22 |     model: transformer/biencoder\n",
            "06:43:22 |     model_file: from_pretrained_bi/model\n",
            "06:43:22 |     model_parallel: False\n",
            "06:43:22 |     momentum: 0\n",
            "06:43:22 |     multitask_weights: [1]\n",
            "06:43:22 |     mutators: None\n",
            "06:43:22 |     n_decoder_layers: -1\n",
            "06:43:22 |     n_encoder_layers: -1\n",
            "06:43:22 |     n_heads: 12\n",
            "06:43:22 |     n_layers: 12\n",
            "06:43:22 |     n_positions: 1024\n",
            "06:43:22 |     n_segments: 2\n",
            "06:43:22 |     nesterov: True\n",
            "06:43:22 |     no_cuda: False\n",
            "06:43:22 |     normalize_sent_emb: False\n",
            "06:43:22 |     num_epochs: -1\n",
            "06:43:22 |     num_workers: 0\n",
            "06:43:22 |     nus: (0.7,)\n",
            "06:43:22 |     optimizer: adamax\n",
            "06:43:22 |     output_scaling: 0.06\n",
            "06:43:22 |     override: \"{'task': 'personachat', 'model': 'transformer/biencoder', 'model_file': 'from_pretrained_bi/model', 'init_model': 'zoo:pretrained_transformers/bi_model_huge_reddit/model', 'dict_file': '/usr/local/lib/python3.8/dist-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict', 'warmup_updates': 100, 'lr_scheduler_patience': 0, 'lr_scheduler_decay': 0.4, 'learningrate': 5e-05, 'data_parallel': True, 'history_size': 20, 'label_truncate': 72, 'text_truncate': 360, 'validation_metric': 'accuracy', 'validation_metric_mode': 'max', 'candidates': 'batch', 'dict_tokenizer': 'bpe', 'dict_lower': True, 'optimizer': 'adamax', 'output_scaling': 0.06, 'variant': 'xlm', 'reduction_type': 'mean', 'share_encoders': False, 'learn_positional_embeddings': True, 'n_layers': 12, 'n_heads': 12, 'ffn_size': 3072, 'attention_dropout': 0.1, 'relu_dropout': 0.0, 'dropout': 0.1, 'n_positions': 1024, 'embedding_size': 768, 'activation': 'gelu', 'embeddings_scale': False, 'n_segments': 2, 'learn_embeddings': True, 'share_word_embeddings': False, 'dict_endtoken': '__start__', 'max_train_time': 60.0, 'validation_every_n_epochs': 0.25, 'batchsize': 12, 'fp16': True}\"\n",
            "06:43:22 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
            "06:43:22 |     person_tokens: False\n",
            "06:43:22 |     rank_candidates: True\n",
            "06:43:22 |     rank_top_k: -1\n",
            "06:43:22 |     reduction_type: mean\n",
            "06:43:22 |     relu_dropout: 0.0\n",
            "06:43:22 |     repeat_blocking_heuristic: True\n",
            "06:43:22 |     return_cand_scores: False\n",
            "06:43:22 |     save_after_valid: False\n",
            "06:43:22 |     save_every_n_secs: -1\n",
            "06:43:22 |     save_format: conversations\n",
            "06:43:22 |     share_encoders: False\n",
            "06:43:22 |     share_word_embeddings: False\n",
            "06:43:22 |     short_final_eval: False\n",
            "06:43:22 |     special_tok_lst: None\n",
            "06:43:22 |     split_lines: False\n",
            "06:43:22 |     starttime: Jan12_06-43\n",
            "06:43:22 |     task: personachat\n",
            "06:43:22 |     tensorboard_log: False\n",
            "06:43:22 |     tensorboard_logdir: None\n",
            "06:43:22 |     text_truncate: 360\n",
            "06:43:22 |     topk: 5\n",
            "06:43:22 |     train_predict: False\n",
            "06:43:22 |     truncate: 1024\n",
            "06:43:22 |     update_freq: 1\n",
            "06:43:22 |     use_memories: False\n",
            "06:43:22 |     use_reply: label\n",
            "06:43:22 |     validation_cutoff: 1.0\n",
            "06:43:22 |     validation_every_n_epochs: 0.25\n",
            "06:43:22 |     validation_every_n_secs: -1\n",
            "06:43:22 |     validation_every_n_steps: -1\n",
            "06:43:22 |     validation_max_exs: -1\n",
            "06:43:22 |     validation_metric: accuracy\n",
            "06:43:22 |     validation_metric_mode: max\n",
            "06:43:22 |     validation_patience: 10\n",
            "06:43:22 |     validation_share_agent: False\n",
            "06:43:22 |     variant: xlm\n",
            "06:43:22 |     verbose: False\n",
            "06:43:22 |     wandb_entity: None\n",
            "06:43:22 |     wandb_log: False\n",
            "06:43:22 |     wandb_name: None\n",
            "06:43:22 |     wandb_project: None\n",
            "06:43:22 |     warmup_rate: 0.0001\n",
            "06:43:22 |     warmup_updates: 100\n",
            "06:43:22 |     weight_decay: None\n",
            "06:43:22 |     world_logs: \n",
            "06:43:22 |     wrap_memory_encoder: False\n",
            "06:43:23 | creating task(s): personachat\n",
            "06:43:23 | loading fbdialog data: /usr/local/lib/python3.8/dist-packages/data/Persona-Chat/personachat/train_self_original.txt\n",
            "06:43:24 | training...\n",
            "06:43:24 | \u001b[33m[ Executing train mode with batch labels as set of candidates. ]\u001b[0m\n",
            "06:43:39 | time:15s total_exs:600 total_steps:50 epochs:0.01\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gpu_mem  llen      lr  ltpb  ltps  ltrunc  \\\n",
            "   143.2 .9800  1743  5768       0          0 39.72  600              4096    .4704 14.52 2.5e-05 174.3 576.8       0   \n",
            "    ltrunclen  mean_loss   mrr  rank  total_train_updates  tpb  tps  train_accuracy   ups  \n",
            "            0      2.356 .4846 3.913                   50 1917 6345           .2967 3.315\n",
            "\n",
            "06:43:55 | time:31s total_exs:1200 total_steps:100 epochs:0.02\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen       lr  ltpb  ltps  ltrunc  \\\n",
            "   148.3     1  1803  5641       0          0 37.54  600              2048  59.53    .4775 14.67 4.95e-05   176 550.6       0   \n",
            "    ltrunclen  mean_loss   mrr  rank  total_train_updates  tpb  tps  train_accuracy   ups  \n",
            "            0      1.544 .6536 2.693                  100 1979 6191           .5017 3.133\n",
            "\n",
            "06:44:11 | time:47s total_exs:1800 total_steps:150 epochs:0.03\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen       lr  ltpb  ltps  ltrunc  \\\n",
            "   143.9     1  1750  5681       0          0 38.95  600              2048  55.82    .4653 13.71 4.95e-05 164.5 533.9       0   \n",
            "    ltrunclen  mean_loss   mrr  rank  total_train_updates  tpb  tps  train_accuracy   ups  \n",
            "            0      1.264 .7373  2.12                  150 1915 6215           .6050 3.251\n",
            "\n",
            "06:44:24 | max_train_time elapsed:60.06977987289429s\n",
            "06:44:24 | Saving dictionary to from_pretrained_bi/model.dict\n",
            "06:44:34 | \u001b[33mOverriding opt[\"init_model\"] to zoo:pretrained_transformers/bi_model_huge_reddit/model (previously: /usr/local/lib/python3.8/dist-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model)\u001b[0m\n",
            "06:44:34 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: None,is_debug: False,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 60.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: -1,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: False,validation_every_n_epochs: 0.25,validation_max_exs: -1,short_final_eval: False,validation_patience: 10,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: -1,mutators: None,fp16_impl: safe,force_fp16_tokens: True,adam_eps: 1e-08,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,invsqrt_lr_decay_gamma: -1,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,dict_loaded: True,download_path: None,verbose: False,datapath: /usr/local/lib/python3.8/dist-packages/data,load_from_checkpoint: True,interactive_mode: False\u001b[0m\n",
            "06:44:34 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --single-turn False --candidates inline --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n",
            "06:44:34 | Using CUDA\n",
            "06:44:34 | loading dictionary from from_pretrained_bi/model.dict\n",
            "06:44:34 | num words = 54944\n",
            "06:44:38 | Total parameters: 256,081,920 (256,081,920 trainable)\n",
            "06:44:38 | Loading existing model parameters from from_pretrained_bi/model\n",
            "06:44:44 | creating task(s): personachat\n",
            "06:44:44 | loading fbdialog data: /usr/local/lib/python3.8/dist-packages/data/Persona-Chat/personachat/valid_self_original.txt\n",
            "06:44:44 | running eval: valid\n",
            "06:46:16 | eval completed in 91.52s\n",
            "06:46:16 | \u001b[1mvalid:\n",
            "    accuracy  bleu-4  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  gpu_mem  hits@1  hits@10  hits@100  hits@5  llen  \\\n",
            "       .5863   .5865 155.8  1882 13455       0          0 85.28 7801 .6308    .2944   .5863    .9644         1   .8876 14.35   \n",
            "    loss       lr  ltpb  ltps  ltrunc  ltrunclen   mrr  rank  total_train_updates  tpb   tps  \n",
            "     1.4 4.95e-05 171.2  1224       0          0 .7152 2.547                  194 2053 14679\n",
            "\u001b[0m\n",
            "06:46:16 | creating task(s): personachat\n",
            "06:46:16 | loading fbdialog data: /usr/local/lib/python3.8/dist-packages/data/Persona-Chat/personachat/test_self_original.txt\n",
            "06:46:16 | running eval: test\n",
            "06:47:42 | eval completed in 85.92s\n",
            "06:47:42 | \u001b[1mtest:\n",
            "    accuracy  bleu-4  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  gpu_mem  hits@1  hits@10  hits@100  hits@5  llen  \\\n",
            "       .5787   .5787 152.5  1839 13514       0          0 87.46 7512 .6261    .2944   .5787    .9621         1   .8841 14.24   \n",
            "    loss       lr  ltpb  ltps  ltrunc  ltrunclen   mrr  rank  total_train_updates  tpb   tps  \n",
            "   1.413 4.95e-05 169.5  1245       0          0 .7110 2.568                  194 2009 14759\n",
            "\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'exs': SumMetric(7801),\n",
              "  'accuracy': ExactMatchMetric(0.5863),\n",
              "  'f1': F1Metric(0.6308),\n",
              "  'bleu-4': BleuMetric(0.5865),\n",
              "  'hits@1': AverageMetric(0.5863),\n",
              "  'hits@5': AverageMetric(0.8876),\n",
              "  'hits@10': AverageMetric(0.9644),\n",
              "  'hits@100': AverageMetric(1),\n",
              "  'clen': AverageMetric(155.8),\n",
              "  'ctrunc': AverageMetric(0),\n",
              "  'ctrunclen': AverageMetric(0),\n",
              "  'llen': AverageMetric(14.35),\n",
              "  'ltrunc': AverageMetric(0),\n",
              "  'ltrunclen': AverageMetric(0),\n",
              "  'loss': AverageMetric(1.4),\n",
              "  'rank': AverageMetric(2.547),\n",
              "  'mrr': AverageMetric(0.7152),\n",
              "  'exps': GlobalTimerMetric(85.28),\n",
              "  'ltpb': GlobalAverageMetric(171.2),\n",
              "  'ltps': GlobalTimerMetric(1224),\n",
              "  'ctpb': GlobalAverageMetric(1882),\n",
              "  'ctps': GlobalTimerMetric(1.345e+04),\n",
              "  'tpb': GlobalAverageMetric(2053),\n",
              "  'tps': GlobalTimerMetric(1.468e+04),\n",
              "  'lr': GlobalAverageMetric(4.95e-05),\n",
              "  'gpu_mem': GlobalAverageMetric(0.2944),\n",
              "  'total_train_updates': GlobalFixedMetric(194)},\n",
              " {'exs': SumMetric(7512),\n",
              "  'accuracy': ExactMatchMetric(0.5787),\n",
              "  'f1': F1Metric(0.6261),\n",
              "  'bleu-4': BleuMetric(0.5787),\n",
              "  'hits@1': AverageMetric(0.5787),\n",
              "  'hits@5': AverageMetric(0.8841),\n",
              "  'hits@10': AverageMetric(0.9621),\n",
              "  'hits@100': AverageMetric(1),\n",
              "  'clen': AverageMetric(152.5),\n",
              "  'ctrunc': AverageMetric(0),\n",
              "  'ctrunclen': AverageMetric(0),\n",
              "  'llen': AverageMetric(14.24),\n",
              "  'ltrunc': AverageMetric(0),\n",
              "  'ltrunclen': AverageMetric(0),\n",
              "  'loss': AverageMetric(1.413),\n",
              "  'rank': AverageMetric(2.568),\n",
              "  'mrr': AverageMetric(0.711),\n",
              "  'exps': GlobalTimerMetric(87.46),\n",
              "  'ltpb': GlobalAverageMetric(169.5),\n",
              "  'ltps': GlobalTimerMetric(1245),\n",
              "  'ctpb': GlobalAverageMetric(1839),\n",
              "  'ctps': GlobalTimerMetric(1.351e+04),\n",
              "  'tpb': GlobalAverageMetric(2009),\n",
              "  'tps': GlobalTimerMetric(1.476e+04),\n",
              "  'lr': GlobalAverageMetric(4.95e-05),\n",
              "  'gpu_mem': GlobalAverageMetric(0.2944),\n",
              "  'total_train_updates': GlobalFixedMetric(194)})"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from parlai.scripts.interactive import Interactive\n",
        "\n",
        "Interactive.main(\n",
        "    model_file='from_pretrained_bi/model'\n",
        ")"
      ],
      "metadata": {
        "id": "I9jpaYlBE0y_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "606f8587-63e0-488d-8d00-52bba4afeee1"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "06:50:13 | 88.8% complete (58,340 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,341 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,342 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,343 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,344 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,345 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,346 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,347 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,348 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,349 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,350 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,351 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,352 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,353 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,354 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,355 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,356 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,357 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,358 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,359 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,360 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,361 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,362 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,363 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,364 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,365 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,366 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,367 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,368 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,369 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,370 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,371 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,372 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,373 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,374 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,375 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,376 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,377 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,378 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,379 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,380 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,381 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,382 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,383 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,384 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,385 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,386 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,387 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,388 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,389 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,390 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.8% complete (58,391 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,392 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,393 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,394 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,395 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,396 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,397 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,398 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,399 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,400 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,401 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,402 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,403 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,404 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,405 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,406 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,407 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,408 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,409 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,410 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,411 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,412 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,413 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,414 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,415 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,416 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,417 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,418 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,419 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,420 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,421 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,422 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,423 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,424 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,425 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,426 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,427 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,428 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,429 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,430 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,431 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,432 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,433 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,434 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,435 / 65,719), 0:02:28 elapsed, 0:00:19 eta\n",
            "06:50:13 | 88.9% complete (58,436 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 88.9% complete (58,437 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 88.9% complete (58,438 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 88.9% complete (58,439 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 88.9% complete (58,440 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 88.9% complete (58,441 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 88.9% complete (58,442 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 88.9% complete (58,443 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 88.9% complete (58,444 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 88.9% complete (58,445 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 88.9% complete (58,446 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 88.9% complete (58,447 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 88.9% complete (58,448 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 88.9% complete (58,449 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 88.9% complete (58,450 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 88.9% complete (58,451 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 88.9% complete (58,452 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 88.9% complete (58,453 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 88.9% complete (58,454 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 88.9% complete (58,455 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 88.9% complete (58,456 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 88.9% complete (58,457 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,458 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,459 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,460 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,461 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,462 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,463 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,464 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,465 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,466 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,467 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,468 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,469 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,470 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,471 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,472 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,473 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,474 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,475 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,476 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,477 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,478 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,479 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,480 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,481 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,482 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,483 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,484 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,485 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,486 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,487 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,488 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,489 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,490 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,491 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,492 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,493 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,494 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,495 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,496 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,497 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,498 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,499 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,500 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,501 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,502 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,503 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,504 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,505 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,506 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,507 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,508 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,509 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,510 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,511 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,512 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,513 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,514 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,515 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,516 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,517 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,518 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,519 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,520 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,521 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.0% complete (58,522 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,523 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,524 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,525 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,526 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,527 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,528 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,529 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,530 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,531 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,532 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,533 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,534 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,535 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,536 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,537 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,538 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,539 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,540 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,541 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,542 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,543 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,544 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,545 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,546 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,547 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,548 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,549 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,550 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,551 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,552 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,553 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,554 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,555 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,556 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,557 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,558 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,559 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,560 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,561 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,562 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,563 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,564 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,565 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,566 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,567 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,568 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,569 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,570 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,571 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,572 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,573 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,574 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,575 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,576 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,577 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,578 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,579 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,580 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,581 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,582 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,583 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,584 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,585 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,586 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,587 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.1% complete (58,588 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,589 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,590 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,591 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,592 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,593 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,594 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,595 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,596 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,597 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,598 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,599 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,600 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,601 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,602 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,603 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,604 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,605 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,606 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,607 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,608 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,609 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,610 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,611 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,612 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,613 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,614 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,615 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,616 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,617 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,618 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,619 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,620 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,621 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,622 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,623 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,624 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,625 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,626 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,627 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,628 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,629 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,630 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,631 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,632 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,633 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,634 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,635 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,636 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,637 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,638 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,639 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,640 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,641 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,642 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,643 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,644 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,645 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,646 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,647 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,648 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,649 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,650 / 65,719), 0:02:28 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,651 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,652 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,653 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.2% complete (58,654 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,655 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,656 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,657 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,658 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,659 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,660 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,661 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,662 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,663 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,664 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,665 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,666 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,667 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,668 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,669 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,670 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,671 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,672 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,673 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,674 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,675 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,676 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,677 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,678 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,679 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,680 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,681 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,682 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,683 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,684 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,685 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,686 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,687 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,688 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,689 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,690 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,691 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,692 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,693 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,694 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,695 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,696 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,697 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,698 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,699 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,700 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,701 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,702 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,703 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,704 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,705 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,706 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,707 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,708 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,709 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,710 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,711 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,712 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,713 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,714 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,715 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,716 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,717 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:13 | 89.3% complete (58,718 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.3% complete (58,719 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,720 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,721 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,722 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,723 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,724 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,725 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,726 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,727 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,728 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,729 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,730 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,731 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,732 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,733 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,734 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,735 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,736 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,737 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,738 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,739 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,740 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,741 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,742 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,743 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,744 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,745 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,746 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,747 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,748 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,749 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,750 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,751 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,752 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,753 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,754 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,755 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,756 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,757 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,758 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,759 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,760 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,761 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,762 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,763 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,764 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,765 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,766 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,767 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,768 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,769 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,770 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,771 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,772 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,773 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,774 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,775 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,776 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,777 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,778 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,779 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,780 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,781 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,782 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,783 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,784 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.4% complete (58,785 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,786 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,787 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,788 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,789 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,790 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,791 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,792 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,793 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,794 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,795 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,796 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,797 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,798 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,799 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,800 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,801 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,802 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,803 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,804 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,805 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,806 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,807 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,808 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,809 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,810 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,811 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,812 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,813 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,814 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,815 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,816 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,817 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,818 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,819 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,820 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,821 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,822 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,823 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,824 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,825 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,826 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,827 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,828 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,829 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,830 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,831 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,832 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,833 / 65,719), 0:02:29 elapsed, 0:00:18 eta\n",
            "06:50:14 | 89.5% complete (58,834 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.5% complete (58,835 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.5% complete (58,836 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.5% complete (58,837 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.5% complete (58,838 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.5% complete (58,839 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.5% complete (58,840 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.5% complete (58,841 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.5% complete (58,842 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.5% complete (58,843 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.5% complete (58,844 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.5% complete (58,845 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.5% complete (58,846 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.5% complete (58,847 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.5% complete (58,848 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.5% complete (58,849 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.5% complete (58,850 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.5% complete (58,851 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,852 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,853 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,854 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,855 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,856 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,857 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,858 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,859 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,860 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,861 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,862 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,863 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,864 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,865 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,866 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,867 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,868 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,869 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,870 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,871 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,872 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,873 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,874 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,875 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,876 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,877 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,878 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,879 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,880 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,881 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,882 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,883 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,884 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,885 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,886 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,887 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,888 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,889 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,890 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,891 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,892 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,893 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,894 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,895 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,896 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,897 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,898 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,899 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,900 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,901 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,902 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,903 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,904 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,905 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,906 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,907 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,908 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,909 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,910 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,911 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,912 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,913 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,914 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,915 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,916 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.6% complete (58,917 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,918 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,919 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,920 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,921 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,922 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,923 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,924 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,925 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,926 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,927 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,928 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,929 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,930 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,931 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,932 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,933 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,934 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,935 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,936 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,937 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,938 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,939 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,940 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,941 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,942 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,943 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,944 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,945 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,946 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,947 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,948 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,949 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,950 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,951 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,952 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,953 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,954 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,955 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,956 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,957 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,958 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,959 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,960 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,961 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,962 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,963 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,964 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,965 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,966 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,967 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,968 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,969 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,970 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,971 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,972 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,973 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,974 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,975 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,976 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,977 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,978 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,979 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,980 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,981 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.7% complete (58,982 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (58,983 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (58,984 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (58,985 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (58,986 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (58,987 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (58,988 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (58,989 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (58,990 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (58,991 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (58,992 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (58,993 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (58,994 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (58,995 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (58,996 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (58,997 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (58,998 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (58,999 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,000 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,001 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,002 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,003 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,004 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,005 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,006 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,007 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,008 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,009 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,010 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,011 / 65,719), 0:02:29 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,012 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,013 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,014 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,015 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,016 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,017 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,018 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,019 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,020 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,021 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,022 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,023 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,024 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,025 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,026 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,027 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,028 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,029 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,030 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,031 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,032 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,033 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,034 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,035 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,036 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,037 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,038 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,039 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,040 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,041 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,042 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,043 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,044 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,045 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:14 | 89.8% complete (59,046 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.8% complete (59,047 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.8% complete (59,048 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,049 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,050 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,051 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,052 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,053 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,054 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,055 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,056 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,057 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,058 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,059 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,060 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,061 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,062 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,063 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,064 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,065 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,066 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,067 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,068 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,069 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,070 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,071 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,072 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,073 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,074 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,075 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,076 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,077 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,078 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,079 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,080 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,081 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,082 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,083 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,084 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,085 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,086 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,087 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,088 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,089 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,090 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,091 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,092 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,093 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,094 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,095 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,096 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,097 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,098 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,099 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,100 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,101 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,102 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,103 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,104 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,105 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,106 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,107 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,108 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,109 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,110 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,111 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,112 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,113 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 89.9% complete (59,114 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,115 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,116 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,117 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,118 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,119 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,120 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,121 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,122 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,123 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,124 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,125 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,126 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,127 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,128 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,129 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,130 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,131 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,132 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,133 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,134 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,135 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,136 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,137 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,138 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,139 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,140 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,141 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,142 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,143 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,144 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,145 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,146 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,147 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,148 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,149 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,150 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,151 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,152 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,153 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,154 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,155 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,156 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,157 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,158 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,159 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,160 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,161 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,162 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,163 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,164 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,165 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,166 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,167 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,168 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,169 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,170 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,171 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,172 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,173 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,174 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,175 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,176 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,177 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,178 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.0% complete (59,179 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,180 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,181 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,182 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,183 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,184 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,185 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,186 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,187 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,188 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,189 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,190 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,191 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,192 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,193 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,194 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,195 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,196 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,197 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,198 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,199 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,200 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,201 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,202 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,203 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,204 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,205 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,206 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,207 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,208 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,209 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,210 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,211 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,212 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,213 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,214 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,215 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,216 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,217 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,218 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,219 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,220 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,221 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,222 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,223 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,224 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,225 / 65,719), 0:02:30 elapsed, 0:00:17 eta\n",
            "06:50:15 | 90.1% complete (59,226 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.1% complete (59,227 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.1% complete (59,228 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.1% complete (59,229 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.1% complete (59,230 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.1% complete (59,231 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.1% complete (59,232 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.1% complete (59,233 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.1% complete (59,234 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.1% complete (59,235 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.1% complete (59,236 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.1% complete (59,237 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.1% complete (59,238 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.1% complete (59,239 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.1% complete (59,240 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.1% complete (59,241 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.1% complete (59,242 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.1% complete (59,243 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.1% complete (59,244 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.1% complete (59,245 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,246 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,247 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,248 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,249 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,250 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,251 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,252 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,253 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,254 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,255 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,256 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,257 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,258 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,259 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,260 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,261 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,262 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,263 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,264 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,265 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,266 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,267 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,268 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,269 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,270 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,271 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,272 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,273 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,274 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,275 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,276 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,277 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,278 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,279 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,280 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,281 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,282 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,283 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,284 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,285 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,286 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,287 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,288 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,289 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,290 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,291 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,292 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,293 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,294 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,295 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,296 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,297 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,298 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,299 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,300 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,301 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,302 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,303 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,304 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,305 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,306 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,307 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,308 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,309 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,310 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.2% complete (59,311 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,312 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,313 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,314 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,315 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,316 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,317 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,318 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,319 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,320 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,321 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,322 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,323 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,324 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,325 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,326 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,327 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,328 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,329 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,330 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,331 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,332 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,333 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,334 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,335 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,336 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,337 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,338 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,339 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,340 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,341 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,342 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,343 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,344 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,345 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,346 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,347 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,348 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,349 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,350 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,351 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,352 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,353 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,354 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,355 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,356 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,357 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,358 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,359 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,360 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,361 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,362 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,363 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,364 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,365 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,366 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,367 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,368 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,369 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,370 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,371 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,372 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,373 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,374 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,375 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,376 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.3% complete (59,377 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,378 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,379 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,380 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,381 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,382 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,383 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,384 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,385 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,386 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,387 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,388 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,389 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,390 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,391 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,392 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,393 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,394 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,395 / 65,719), 0:02:30 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,396 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,397 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,398 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,399 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,400 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,401 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,402 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,403 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,404 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,405 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,406 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,407 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,408 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,409 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,410 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,411 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,412 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,413 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,414 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,415 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,416 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,417 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,418 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,419 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,420 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,421 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,422 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,423 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,424 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,425 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,426 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,427 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,428 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,429 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,430 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,431 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,432 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,433 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,434 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,435 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,436 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,437 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,438 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,439 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,440 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,441 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.4% complete (59,442 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:15 | 90.5% complete (59,443 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,444 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,445 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,446 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,447 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,448 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,449 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,450 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,451 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,452 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,453 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,454 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,455 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,456 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,457 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,458 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,459 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,460 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,461 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,462 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,463 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,464 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,465 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,466 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,467 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,468 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,469 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,470 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,471 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,472 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,473 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,474 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,475 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,476 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,477 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,478 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,479 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,480 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,481 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,482 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,483 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,484 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,485 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,486 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,487 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,488 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,489 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,490 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,491 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,492 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,493 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,494 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,495 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,496 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,497 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,498 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,499 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,500 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,501 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,502 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,503 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,504 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,505 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,506 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,507 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.5% complete (59,508 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,509 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,510 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,511 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,512 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,513 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,514 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,515 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,516 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,517 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,518 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,519 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,520 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,521 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,522 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,523 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,524 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,525 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,526 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,527 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,528 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,529 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,530 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,531 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,532 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,533 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,534 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,535 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,536 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,537 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,538 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,539 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,540 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,541 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,542 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,543 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,544 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,545 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,546 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,547 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,548 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,549 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,550 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,551 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,552 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,553 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,554 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,555 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,556 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,557 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,558 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,559 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,560 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,561 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,562 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,563 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,564 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,565 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,566 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,567 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,568 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,569 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,570 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,571 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,572 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,573 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.6% complete (59,574 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,575 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,576 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,577 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,578 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,579 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,580 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,581 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,582 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,583 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,584 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,585 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,586 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,587 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,588 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,589 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,590 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,591 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,592 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,593 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,594 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,595 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,596 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,597 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,598 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,599 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,600 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,601 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,602 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,603 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,604 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,605 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,606 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,607 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,608 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,609 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,610 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,611 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,612 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,613 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,614 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,615 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,616 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,617 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,618 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,619 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,620 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,621 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,622 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,623 / 65,719), 0:02:31 elapsed, 0:00:16 eta\n",
            "06:50:16 | 90.7% complete (59,624 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.7% complete (59,625 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.7% complete (59,626 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.7% complete (59,627 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.7% complete (59,628 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.7% complete (59,629 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.7% complete (59,630 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.7% complete (59,631 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.7% complete (59,632 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.7% complete (59,633 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.7% complete (59,634 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.7% complete (59,635 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.7% complete (59,636 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.7% complete (59,637 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.7% complete (59,638 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.7% complete (59,639 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,640 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,641 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,642 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,643 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,644 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,645 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,646 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,647 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,648 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,649 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,650 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,651 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,652 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,653 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,654 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,655 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,656 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,657 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,658 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,659 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,660 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,661 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,662 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,663 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,664 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,665 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,666 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,667 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,668 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,669 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,670 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,671 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,672 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,673 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,674 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,675 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,676 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,677 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,678 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,679 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,680 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,681 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,682 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,683 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,684 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,685 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,686 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,687 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,688 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,689 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,690 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,691 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,692 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,693 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,694 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,695 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,696 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,697 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,698 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,699 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,700 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,701 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,702 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,703 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,704 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.8% complete (59,705 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,706 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,707 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,708 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,709 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,710 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,711 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,712 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,713 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,714 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,715 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,716 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,717 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,718 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,719 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,720 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,721 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,722 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,723 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,724 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,725 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,726 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,727 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,728 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,729 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,730 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,731 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,732 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,733 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,734 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,735 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,736 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,737 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,738 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,739 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,740 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,741 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,742 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,743 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,744 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,745 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,746 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,747 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,748 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,749 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,750 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,751 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,752 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,753 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,754 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,755 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,756 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,757 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,758 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,759 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,760 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,761 / 65,719), 0:02:31 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,762 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,763 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,764 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,765 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,766 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,767 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,768 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,769 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,770 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:16 | 90.9% complete (59,771 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:16 | 91.0% complete (59,772 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:16 | 91.0% complete (59,773 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:16 | 91.0% complete (59,774 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:16 | 91.0% complete (59,775 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:16 | 91.0% complete (59,776 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:16 | 91.0% complete (59,777 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:16 | 91.0% complete (59,778 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:16 | 91.0% complete (59,779 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:16 | 91.0% complete (59,780 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:16 | 91.0% complete (59,781 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:16 | 91.0% complete (59,782 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:16 | 91.0% complete (59,783 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:16 | 91.0% complete (59,784 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:16 | 91.0% complete (59,785 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:16 | 91.0% complete (59,786 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:16 | 91.0% complete (59,787 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:16 | 91.0% complete (59,788 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:16 | 91.0% complete (59,789 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:16 | 91.0% complete (59,790 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:16 | 91.0% complete (59,791 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:16 | 91.0% complete (59,792 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:16 | 91.0% complete (59,793 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:16 | 91.0% complete (59,794 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:16 | 91.0% complete (59,795 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:16 | 91.0% complete (59,796 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:16 | 91.0% complete (59,797 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:16 | 91.0% complete (59,798 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:16 | 91.0% complete (59,799 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:16 | 91.0% complete (59,800 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.0% complete (59,801 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.0% complete (59,802 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.0% complete (59,803 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.0% complete (59,804 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.0% complete (59,805 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.0% complete (59,806 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.0% complete (59,807 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.0% complete (59,808 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.0% complete (59,809 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.0% complete (59,810 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.0% complete (59,811 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.0% complete (59,812 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.0% complete (59,813 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.0% complete (59,814 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.0% complete (59,815 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.0% complete (59,816 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.0% complete (59,817 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.0% complete (59,818 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.0% complete (59,819 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.0% complete (59,820 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.0% complete (59,821 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.0% complete (59,822 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.0% complete (59,823 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.0% complete (59,824 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.0% complete (59,825 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.0% complete (59,826 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.0% complete (59,827 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.0% complete (59,828 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.0% complete (59,829 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.0% complete (59,830 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.0% complete (59,831 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.0% complete (59,832 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.0% complete (59,833 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.0% complete (59,834 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.0% complete (59,835 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.0% complete (59,836 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.0% complete (59,837 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,838 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,839 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,840 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,841 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,842 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,843 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,844 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,845 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,846 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,847 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,848 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,849 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,850 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,851 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,852 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,853 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,854 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,855 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,856 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,857 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,858 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,859 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,860 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,861 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,862 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,863 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,864 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,865 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,866 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,867 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,868 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,869 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,870 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,871 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,872 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,873 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,874 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,875 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,876 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,877 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,878 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,879 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,880 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,881 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,882 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,883 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,884 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,885 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,886 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,887 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,888 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,889 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,890 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,891 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,892 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,893 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,894 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,895 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,896 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,897 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,898 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,899 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,900 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,901 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.1% complete (59,902 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,903 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,904 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,905 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,906 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,907 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,908 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,909 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,910 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,911 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,912 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,913 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,914 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,915 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,916 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,917 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,918 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,919 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,920 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,921 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,922 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,923 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,924 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,925 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,926 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,927 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,928 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,929 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,930 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,931 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,932 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,933 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,934 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,935 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,936 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,937 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,938 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,939 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,940 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,941 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,942 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,943 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,944 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,945 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,946 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,947 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,948 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,949 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,950 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,951 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,952 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,953 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,954 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,955 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,956 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,957 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,958 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,959 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,960 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,961 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,962 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,963 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,964 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,965 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,966 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,967 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.2% complete (59,968 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (59,969 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (59,970 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (59,971 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (59,972 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (59,973 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (59,974 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (59,975 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (59,976 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (59,977 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (59,978 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (59,979 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (59,980 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (59,981 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (59,982 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (59,983 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (59,984 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (59,985 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (59,986 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (59,987 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (59,988 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (59,989 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (59,990 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (59,991 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (59,992 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (59,993 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (59,994 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (59,995 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (59,996 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (59,997 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (59,998 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (59,999 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (60,000 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (60,001 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (60,002 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (60,003 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (60,004 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (60,005 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (60,006 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (60,007 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (60,008 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (60,009 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (60,010 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (60,011 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (60,012 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (60,013 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (60,014 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (60,015 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (60,016 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (60,017 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (60,018 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (60,019 / 65,719), 0:02:32 elapsed, 0:00:15 eta\n",
            "06:50:17 | 91.3% complete (60,020 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.3% complete (60,021 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.3% complete (60,022 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.3% complete (60,023 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.3% complete (60,024 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.3% complete (60,025 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.3% complete (60,026 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.3% complete (60,027 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.3% complete (60,028 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.3% complete (60,029 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.3% complete (60,030 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.3% complete (60,031 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.3% complete (60,032 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.3% complete (60,033 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.3% complete (60,034 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,035 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,036 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,037 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,038 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,039 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,040 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,041 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,042 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,043 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,044 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,045 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,046 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,047 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,048 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,049 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,050 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,051 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,052 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,053 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,054 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,055 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,056 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,057 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,058 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,059 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,060 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,061 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,062 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,063 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,064 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,065 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,066 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,067 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,068 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,069 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,070 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,071 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,072 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,073 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,074 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,075 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,076 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,077 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,078 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,079 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,080 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,081 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,082 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,083 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,084 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,085 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,086 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,087 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,088 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,089 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,090 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,091 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,092 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,093 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,094 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,095 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,096 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,097 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,098 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,099 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.4% complete (60,100 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,101 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,102 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,103 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,104 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,105 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,106 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,107 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,108 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,109 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,110 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,111 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,112 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,113 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,114 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,115 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,116 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,117 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,118 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,119 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,120 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,121 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,122 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,123 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,124 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,125 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,126 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,127 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,128 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,129 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,130 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,131 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,132 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,133 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,134 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,135 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,136 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,137 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,138 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,139 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,140 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,141 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,142 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,143 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,144 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,145 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,146 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,147 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,148 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,149 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,150 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,151 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,152 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,153 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,154 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,155 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,156 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,157 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,158 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,159 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,160 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,161 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,162 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,163 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,164 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.5% complete (60,165 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,166 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,167 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,168 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,169 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,170 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,171 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,172 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,173 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,174 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,175 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,176 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,177 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,178 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,179 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,180 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,181 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,182 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,183 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,184 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,185 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,186 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,187 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,188 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,189 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,190 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,191 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,192 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,193 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,194 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,195 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,196 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,197 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,198 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,199 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,200 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,201 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,202 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,203 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,204 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,205 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,206 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,207 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,208 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,209 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,210 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,211 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,212 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,213 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,214 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,215 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,216 / 65,719), 0:02:32 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,217 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,218 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,219 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,220 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,221 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,222 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,223 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,224 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,225 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,226 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,227 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,228 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,229 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,230 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.6% complete (60,231 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.7% complete (60,232 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.7% complete (60,233 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.7% complete (60,234 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.7% complete (60,235 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.7% complete (60,236 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.7% complete (60,237 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.7% complete (60,238 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.7% complete (60,239 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.7% complete (60,240 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.7% complete (60,241 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.7% complete (60,242 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.7% complete (60,243 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.7% complete (60,244 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.7% complete (60,245 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.7% complete (60,246 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.7% complete (60,247 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.7% complete (60,248 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.7% complete (60,249 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.7% complete (60,250 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.7% complete (60,251 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.7% complete (60,252 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.7% complete (60,253 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.7% complete (60,254 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.7% complete (60,255 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.7% complete (60,256 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.7% complete (60,257 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.7% complete (60,258 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.7% complete (60,259 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.7% complete (60,260 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:17 | 91.7% complete (60,261 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.7% complete (60,262 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.7% complete (60,263 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.7% complete (60,264 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.7% complete (60,265 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.7% complete (60,266 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.7% complete (60,267 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.7% complete (60,268 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.7% complete (60,269 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.7% complete (60,270 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.7% complete (60,271 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.7% complete (60,272 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.7% complete (60,273 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.7% complete (60,274 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.7% complete (60,275 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.7% complete (60,276 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.7% complete (60,277 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.7% complete (60,278 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.7% complete (60,279 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.7% complete (60,280 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.7% complete (60,281 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.7% complete (60,282 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.7% complete (60,283 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.7% complete (60,284 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.7% complete (60,285 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.7% complete (60,286 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.7% complete (60,287 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.7% complete (60,288 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.7% complete (60,289 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.7% complete (60,290 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.7% complete (60,291 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.7% complete (60,292 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.7% complete (60,293 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.7% complete (60,294 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.7% complete (60,295 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.7% complete (60,296 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.7% complete (60,297 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,298 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,299 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,300 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,301 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,302 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,303 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,304 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,305 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,306 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,307 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,308 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,309 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,310 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,311 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,312 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,313 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,314 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,315 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,316 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,317 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,318 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,319 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,320 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,321 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,322 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,323 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,324 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,325 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,326 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,327 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,328 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,329 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,330 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,331 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,332 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,333 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,334 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,335 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,336 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,337 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,338 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,339 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,340 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,341 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,342 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,343 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,344 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,345 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,346 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,347 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,348 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,349 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,350 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,351 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,352 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,353 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,354 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,355 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,356 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,357 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,358 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,359 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,360 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,361 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.8% complete (60,362 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,363 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,364 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,365 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,366 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,367 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,368 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,369 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,370 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,371 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,372 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,373 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,374 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,375 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,376 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,377 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,378 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,379 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,380 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,381 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,382 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,383 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,384 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,385 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,386 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,387 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,388 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,389 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,390 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,391 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,392 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,393 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,394 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,395 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,396 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,397 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,398 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,399 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,400 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,401 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,402 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,403 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,404 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,405 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,406 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,407 / 65,719), 0:02:33 elapsed, 0:00:14 eta\n",
            "06:50:18 | 91.9% complete (60,408 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 91.9% complete (60,409 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 91.9% complete (60,410 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 91.9% complete (60,411 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 91.9% complete (60,412 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 91.9% complete (60,413 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 91.9% complete (60,414 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 91.9% complete (60,415 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 91.9% complete (60,416 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 91.9% complete (60,417 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 91.9% complete (60,418 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 91.9% complete (60,419 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 91.9% complete (60,420 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 91.9% complete (60,421 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 91.9% complete (60,422 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 91.9% complete (60,423 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 91.9% complete (60,424 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 91.9% complete (60,425 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 91.9% complete (60,426 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 91.9% complete (60,427 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 91.9% complete (60,428 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,429 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,430 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,431 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,432 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,433 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,434 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,435 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,436 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,437 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,438 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,439 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,440 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,441 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,442 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,443 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,444 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,445 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,446 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,447 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,448 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,449 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,450 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,451 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,452 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,453 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,454 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,455 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,456 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,457 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,458 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,459 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,460 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,461 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,462 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,463 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,464 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,465 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,466 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,467 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,468 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,469 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,470 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,471 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,472 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,473 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,474 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,475 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,476 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,477 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,478 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,479 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,480 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,481 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,482 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,483 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,484 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,485 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,486 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,487 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,488 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,489 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,490 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,491 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,492 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,493 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.0% complete (60,494 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,495 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,496 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,497 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,498 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,499 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,500 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,501 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,502 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,503 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,504 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,505 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,506 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,507 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,508 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,509 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,510 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,511 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,512 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,513 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,514 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,515 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,516 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,517 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,518 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,519 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,520 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,521 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,522 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,523 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,524 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,525 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,526 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,527 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,528 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,529 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,530 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,531 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,532 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,533 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,534 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,535 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,536 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,537 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,538 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,539 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,540 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,541 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,542 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,543 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,544 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,545 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,546 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,547 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,548 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,549 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,550 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,551 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,552 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,553 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,554 / 65,719), 0:02:33 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,555 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,556 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,557 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,558 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,559 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.1% complete (60,560 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,561 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,562 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,563 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,564 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,565 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,566 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,567 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,568 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,569 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,570 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,571 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,572 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,573 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,574 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,575 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,576 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,577 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,578 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,579 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,580 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,581 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,582 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,583 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,584 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,585 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,586 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,587 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,588 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,589 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,590 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,591 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,592 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,593 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,594 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,595 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,596 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,597 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,598 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,599 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,600 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,601 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,602 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,603 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,604 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,605 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,606 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,607 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,608 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,609 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,610 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,611 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,612 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,613 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,614 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,615 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,616 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,617 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,618 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,619 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,620 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,621 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,622 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,623 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,624 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.2% complete (60,625 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.3% complete (60,626 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.3% complete (60,627 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.3% complete (60,628 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.3% complete (60,629 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:18 | 92.3% complete (60,630 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,631 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,632 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,633 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,634 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,635 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,636 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,637 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,638 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,639 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,640 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,641 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,642 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,643 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,644 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,645 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,646 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,647 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,648 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,649 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,650 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,651 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,652 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,653 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,654 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,655 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,656 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,657 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,658 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,659 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,660 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,661 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,662 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,663 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,664 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,665 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,666 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,667 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,668 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,669 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,670 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,671 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,672 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,673 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,674 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,675 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,676 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,677 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,678 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,679 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,680 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,681 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,682 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,683 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,684 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,685 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,686 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,687 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,688 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,689 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,690 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.3% complete (60,691 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,692 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,693 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,694 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,695 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,696 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,697 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,698 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,699 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,700 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,701 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,702 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,703 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,704 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,705 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,706 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,707 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,708 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,709 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,710 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,711 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,712 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,713 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,714 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,715 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,716 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,717 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,718 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,719 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,720 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,721 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,722 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,723 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,724 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,725 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,726 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,727 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,728 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,729 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,730 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,731 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,732 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,733 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,734 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,735 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,736 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,737 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,738 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,739 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,740 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,741 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,742 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,743 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,744 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,745 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,746 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,747 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,748 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,749 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,750 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,751 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,752 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,753 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,754 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,755 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,756 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.4% complete (60,757 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,758 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,759 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,760 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,761 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,762 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,763 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,764 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,765 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,766 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,767 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,768 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,769 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,770 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,771 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,772 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,773 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,774 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,775 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,776 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,777 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,778 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,779 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,780 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,781 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,782 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,783 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,784 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,785 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,786 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,787 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,788 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,789 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,790 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,791 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,792 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,793 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,794 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,795 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,796 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,797 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,798 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,799 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,800 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,801 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,802 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,803 / 65,719), 0:02:34 elapsed, 0:00:13 eta\n",
            "06:50:19 | 92.5% complete (60,804 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.5% complete (60,805 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.5% complete (60,806 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.5% complete (60,807 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.5% complete (60,808 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.5% complete (60,809 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.5% complete (60,810 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.5% complete (60,811 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.5% complete (60,812 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.5% complete (60,813 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.5% complete (60,814 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.5% complete (60,815 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.5% complete (60,816 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.5% complete (60,817 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.5% complete (60,818 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.5% complete (60,819 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.5% complete (60,820 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.5% complete (60,821 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.5% complete (60,822 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,823 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,824 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,825 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,826 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,827 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,828 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,829 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,830 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,831 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,832 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,833 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,834 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,835 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,836 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,837 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,838 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,839 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,840 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,841 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,842 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,843 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,844 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,845 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,846 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,847 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,848 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,849 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,850 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,851 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,852 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,853 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,854 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,855 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,856 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,857 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,858 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,859 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,860 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,861 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,862 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,863 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,864 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,865 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,866 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,867 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,868 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,869 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,870 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,871 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,872 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,873 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,874 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,875 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,876 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,877 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,878 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,879 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,880 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,881 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,882 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,883 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,884 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,885 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,886 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,887 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.6% complete (60,888 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,889 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,890 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,891 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,892 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,893 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,894 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,895 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,896 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,897 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,898 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,899 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,900 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,901 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,902 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,903 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,904 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,905 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,906 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,907 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,908 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,909 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,910 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,911 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,912 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,913 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,914 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,915 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,916 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,917 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,918 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,919 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,920 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,921 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,922 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,923 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,924 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,925 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,926 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,927 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,928 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,929 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,930 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,931 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,932 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,933 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,934 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,935 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,936 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,937 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,938 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,939 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,940 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,941 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,942 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,943 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,944 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,945 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,946 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,947 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,948 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,949 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,950 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,951 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,952 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,953 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.7% complete (60,954 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,955 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,956 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,957 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,958 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,959 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,960 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,961 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,962 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,963 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,964 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,965 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,966 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,967 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,968 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,969 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,970 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,971 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,972 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,973 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,974 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,975 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,976 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,977 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,978 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,979 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,980 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,981 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,982 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,983 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,984 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,985 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,986 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,987 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,988 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,989 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,990 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,991 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,992 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,993 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,994 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,995 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,996 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,997 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,998 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (60,999 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (61,000 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (61,001 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (61,002 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (61,003 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (61,004 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (61,005 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (61,006 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (61,007 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (61,008 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (61,009 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (61,010 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:19 | 92.8% complete (61,011 / 65,719), 0:02:34 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.8% complete (61,012 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.8% complete (61,013 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.8% complete (61,014 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.8% complete (61,015 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.8% complete (61,016 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.8% complete (61,017 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.8% complete (61,018 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.8% complete (61,019 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.8% complete (61,020 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,021 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,022 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,023 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,024 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,025 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,026 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,027 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,028 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,029 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,030 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,031 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,032 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,033 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,034 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,035 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,036 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,037 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,038 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,039 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,040 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,041 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,042 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,043 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,044 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,045 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,046 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,047 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,048 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,049 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,050 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,051 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,052 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,053 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,054 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,055 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,056 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,057 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,058 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,059 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,060 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,061 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,062 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,063 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,064 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,065 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,066 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,067 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,068 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,069 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,070 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,071 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,072 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,073 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,074 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,075 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,076 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,077 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,078 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,079 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,080 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,081 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,082 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,083 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,084 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 92.9% complete (61,085 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,086 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,087 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,088 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,089 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,090 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,091 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,092 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,093 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,094 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,095 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,096 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,097 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,098 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,099 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,100 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,101 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,102 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,103 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,104 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,105 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,106 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,107 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,108 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,109 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,110 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,111 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,112 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,113 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,114 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,115 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,116 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,117 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,118 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,119 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,120 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,121 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,122 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,123 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,124 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,125 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,126 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,127 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,128 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,129 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,130 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,131 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,132 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,133 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,134 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,135 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,136 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,137 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,138 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,139 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,140 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,141 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,142 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,143 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,144 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,145 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,146 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,147 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,148 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,149 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,150 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.0% complete (61,151 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.1% complete (61,152 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.1% complete (61,153 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.1% complete (61,154 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.1% complete (61,155 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.1% complete (61,156 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.1% complete (61,157 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.1% complete (61,158 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.1% complete (61,159 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.1% complete (61,160 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.1% complete (61,161 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.1% complete (61,162 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.1% complete (61,163 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.1% complete (61,164 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.1% complete (61,165 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.1% complete (61,166 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.1% complete (61,167 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.1% complete (61,168 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.1% complete (61,169 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.1% complete (61,170 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.1% complete (61,171 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.1% complete (61,172 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.1% complete (61,173 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.1% complete (61,174 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.1% complete (61,175 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.1% complete (61,176 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.1% complete (61,177 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.1% complete (61,178 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.1% complete (61,179 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.1% complete (61,180 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.1% complete (61,181 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.1% complete (61,182 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.1% complete (61,183 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.1% complete (61,184 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.1% complete (61,185 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.1% complete (61,186 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.1% complete (61,187 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.1% complete (61,188 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.1% complete (61,189 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.1% complete (61,190 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.1% complete (61,191 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.1% complete (61,192 / 65,719), 0:02:35 elapsed, 0:00:12 eta\n",
            "06:50:20 | 93.1% complete (61,193 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.1% complete (61,194 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.1% complete (61,195 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.1% complete (61,196 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.1% complete (61,197 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.1% complete (61,198 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.1% complete (61,199 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.1% complete (61,200 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.1% complete (61,201 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.1% complete (61,202 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.1% complete (61,203 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.1% complete (61,204 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.1% complete (61,205 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.1% complete (61,206 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.1% complete (61,207 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.1% complete (61,208 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.1% complete (61,209 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.1% complete (61,210 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.1% complete (61,211 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.1% complete (61,212 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.1% complete (61,213 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.1% complete (61,214 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.1% complete (61,215 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.1% complete (61,216 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.1% complete (61,217 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,218 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,219 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,220 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,221 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,222 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,223 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,224 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,225 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,226 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,227 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,228 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,229 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,230 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,231 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,232 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,233 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,234 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,235 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,236 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,237 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,238 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,239 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,240 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,241 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,242 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,243 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,244 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,245 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,246 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,247 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,248 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,249 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,250 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,251 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,252 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,253 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,254 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,255 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,256 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,257 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,258 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,259 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,260 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,261 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,262 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,263 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,264 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,265 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,266 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,267 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,268 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,269 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,270 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,271 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,272 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,273 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,274 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,275 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,276 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,277 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,278 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,279 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,280 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,281 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.2% complete (61,282 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,283 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,284 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,285 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,286 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,287 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,288 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,289 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,290 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,291 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,292 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,293 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,294 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,295 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,296 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,297 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,298 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,299 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,300 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,301 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,302 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,303 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,304 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,305 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,306 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,307 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,308 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,309 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,310 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,311 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,312 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,313 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,314 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,315 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,316 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,317 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,318 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,319 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,320 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,321 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,322 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,323 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,324 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,325 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,326 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,327 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,328 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,329 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,330 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,331 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,332 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,333 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,334 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,335 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,336 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,337 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,338 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,339 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,340 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,341 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,342 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,343 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,344 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,345 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,346 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,347 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.3% complete (61,348 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,349 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,350 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,351 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,352 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,353 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,354 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,355 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,356 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,357 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,358 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,359 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,360 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,361 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,362 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,363 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,364 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,365 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,366 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,367 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,368 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,369 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,370 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,371 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,372 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,373 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,374 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,375 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,376 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,377 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,378 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,379 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,380 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,381 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,382 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,383 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,384 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,385 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,386 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,387 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,388 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,389 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,390 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,391 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,392 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,393 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,394 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,395 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,396 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,397 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,398 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,399 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,400 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,401 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,402 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,403 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,404 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,405 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,406 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,407 / 65,719), 0:02:35 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,408 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,409 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,410 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,411 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,412 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,413 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.4% complete (61,414 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,415 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,416 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,417 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,418 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,419 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,420 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,421 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,422 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,423 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,424 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,425 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,426 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,427 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,428 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,429 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,430 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,431 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,432 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,433 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,434 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,435 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,436 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,437 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,438 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,439 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,440 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,441 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,442 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,443 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,444 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,445 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,446 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,447 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,448 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,449 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,450 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,451 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,452 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,453 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,454 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,455 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,456 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,457 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,458 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,459 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,460 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,461 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,462 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,463 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,464 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,465 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,466 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,467 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,468 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,469 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,470 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,471 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,472 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,473 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,474 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,475 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,476 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,477 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,478 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:20 | 93.5% complete (61,479 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.5% complete (61,480 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,481 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,482 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,483 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,484 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,485 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,486 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,487 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,488 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,489 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,490 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,491 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,492 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,493 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,494 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,495 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,496 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,497 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,498 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,499 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,500 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,501 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,502 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,503 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,504 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,505 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,506 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,507 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,508 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,509 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,510 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,511 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,512 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,513 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,514 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,515 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,516 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,517 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,518 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,519 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,520 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,521 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,522 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,523 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,524 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,525 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,526 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,527 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,528 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,529 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,530 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,531 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,532 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,533 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,534 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,535 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,536 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,537 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,538 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,539 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,540 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,541 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,542 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,543 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,544 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.6% complete (61,545 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,546 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,547 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,548 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,549 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,550 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,551 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,552 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,553 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,554 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,555 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,556 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,557 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,558 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,559 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,560 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,561 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,562 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,563 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,564 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,565 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,566 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,567 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,568 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,569 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,570 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,571 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,572 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,573 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,574 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,575 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,576 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,577 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,578 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,579 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,580 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,581 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,582 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,583 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,584 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,585 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,586 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,587 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,588 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,589 / 65,719), 0:02:36 elapsed, 0:00:11 eta\n",
            "06:50:21 | 93.7% complete (61,590 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.7% complete (61,591 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.7% complete (61,592 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.7% complete (61,593 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.7% complete (61,594 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.7% complete (61,595 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.7% complete (61,596 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.7% complete (61,597 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.7% complete (61,598 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.7% complete (61,599 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.7% complete (61,600 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.7% complete (61,601 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.7% complete (61,602 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.7% complete (61,603 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.7% complete (61,604 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.7% complete (61,605 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.7% complete (61,606 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.7% complete (61,607 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.7% complete (61,608 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.7% complete (61,609 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.7% complete (61,610 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.7% complete (61,611 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,612 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,613 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,614 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,615 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,616 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,617 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,618 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,619 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,620 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,621 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,622 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,623 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,624 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,625 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,626 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,627 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,628 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,629 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,630 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,631 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,632 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,633 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,634 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,635 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,636 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,637 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,638 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,639 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,640 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,641 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,642 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,643 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,644 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,645 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,646 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,647 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,648 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,649 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,650 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,651 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,652 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,653 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,654 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,655 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,656 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,657 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,658 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,659 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,660 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,661 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,662 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,663 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,664 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,665 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,666 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,667 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,668 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,669 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,670 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,671 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,672 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,673 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,674 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,675 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,676 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.8% complete (61,677 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,678 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,679 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,680 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,681 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,682 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,683 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,684 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,685 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,686 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,687 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,688 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,689 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,690 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,691 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,692 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,693 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,694 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,695 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,696 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,697 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,698 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,699 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,700 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,701 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,702 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,703 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,704 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,705 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,706 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,707 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,708 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,709 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,710 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,711 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,712 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,713 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,714 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,715 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,716 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,717 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,718 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,719 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,720 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,721 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,722 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,723 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,724 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,725 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,726 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,727 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,728 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,729 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,730 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,731 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,732 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,733 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,734 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,735 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,736 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,737 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,738 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,739 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,740 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,741 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,742 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 93.9% complete (61,743 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,744 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,745 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,746 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,747 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,748 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,749 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,750 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,751 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,752 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,753 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,754 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,755 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,756 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,757 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,758 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,759 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,760 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,761 / 65,719), 0:02:36 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,762 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,763 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,764 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,765 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,766 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,767 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,768 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,769 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,770 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,771 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,772 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,773 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,774 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,775 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,776 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,777 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,778 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,779 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,780 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,781 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,782 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,783 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,784 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,785 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,786 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,787 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,788 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,789 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,790 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,791 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,792 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,793 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,794 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,795 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,796 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,797 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,798 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,799 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,800 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,801 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,802 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:21 | 94.0% complete (61,803 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.0% complete (61,804 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.0% complete (61,805 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.0% complete (61,806 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.0% complete (61,807 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.0% complete (61,808 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,809 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,810 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,811 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,812 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,813 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,814 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,815 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,816 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,817 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,818 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,819 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,820 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,821 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,822 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,823 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,824 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,825 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,826 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,827 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,828 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,829 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,830 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,831 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,832 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,833 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,834 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,835 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,836 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,837 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,838 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,839 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,840 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,841 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,842 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,843 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,844 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,845 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,846 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,847 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,848 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,849 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,850 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,851 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,852 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,853 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,854 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,855 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,856 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,857 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,858 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,859 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,860 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,861 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,862 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,863 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,864 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,865 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,866 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,867 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,868 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,869 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,870 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,871 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,872 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,873 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.1% complete (61,874 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,875 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,876 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,877 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,878 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,879 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,880 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,881 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,882 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,883 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,884 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,885 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,886 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,887 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,888 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,889 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,890 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,891 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,892 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,893 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,894 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,895 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,896 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,897 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,898 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,899 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,900 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,901 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,902 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,903 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,904 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,905 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,906 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,907 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,908 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,909 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,910 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,911 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,912 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,913 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,914 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,915 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,916 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,917 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,918 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,919 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,920 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,921 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,922 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,923 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,924 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,925 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,926 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,927 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,928 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,929 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,930 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,931 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,932 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,933 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,934 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,935 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,936 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,937 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,938 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,939 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.2% complete (61,940 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.3% complete (61,941 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.3% complete (61,942 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.3% complete (61,943 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.3% complete (61,944 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.3% complete (61,945 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.3% complete (61,946 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.3% complete (61,947 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.3% complete (61,948 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.3% complete (61,949 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.3% complete (61,950 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.3% complete (61,951 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.3% complete (61,952 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.3% complete (61,953 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.3% complete (61,954 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.3% complete (61,955 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.3% complete (61,956 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.3% complete (61,957 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.3% complete (61,958 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.3% complete (61,959 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.3% complete (61,960 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.3% complete (61,961 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.3% complete (61,962 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.3% complete (61,963 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.3% complete (61,964 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.3% complete (61,965 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.3% complete (61,966 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.3% complete (61,967 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.3% complete (61,968 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.3% complete (61,969 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.3% complete (61,970 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.3% complete (61,971 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.3% complete (61,972 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.3% complete (61,973 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.3% complete (61,974 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.3% complete (61,975 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.3% complete (61,976 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.3% complete (61,977 / 65,719), 0:02:37 elapsed, 0:00:10 eta\n",
            "06:50:22 | 94.3% complete (61,978 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.3% complete (61,979 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.3% complete (61,980 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.3% complete (61,981 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.3% complete (61,982 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.3% complete (61,983 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.3% complete (61,984 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.3% complete (61,985 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.3% complete (61,986 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.3% complete (61,987 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.3% complete (61,988 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.3% complete (61,989 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.3% complete (61,990 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.3% complete (61,991 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.3% complete (61,992 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.3% complete (61,993 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.3% complete (61,994 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.3% complete (61,995 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.3% complete (61,996 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.3% complete (61,997 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.3% complete (61,998 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.3% complete (61,999 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.3% complete (62,000 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.3% complete (62,001 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.3% complete (62,002 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.3% complete (62,003 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.3% complete (62,004 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.3% complete (62,005 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,006 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,007 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,008 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,009 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,010 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,011 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,012 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,013 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,014 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,015 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,016 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,017 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,018 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,019 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,020 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,021 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,022 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,023 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,024 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,025 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,026 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,027 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,028 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,029 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,030 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,031 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,032 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,033 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,034 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,035 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,036 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,037 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,038 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,039 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,040 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,041 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,042 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,043 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,044 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,045 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,046 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,047 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,048 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,049 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,050 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,051 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,052 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,053 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,054 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,055 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,056 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,057 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,058 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,059 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,060 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,061 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,062 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,063 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,064 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,065 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,066 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,067 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,068 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,069 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,070 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.4% complete (62,071 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,072 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,073 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,074 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,075 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,076 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,077 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,078 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,079 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,080 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,081 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,082 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,083 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,084 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,085 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,086 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,087 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,088 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,089 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,090 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,091 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,092 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,093 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,094 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,095 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,096 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,097 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,098 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,099 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,100 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,101 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,102 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,103 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,104 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,105 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,106 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,107 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,108 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,109 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,110 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,111 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,112 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,113 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,114 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,115 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,116 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,117 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,118 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,119 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,120 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,121 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,122 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,123 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,124 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,125 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,126 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,127 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,128 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,129 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,130 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,131 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,132 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,133 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,134 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,135 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,136 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.5% complete (62,137 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,138 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,139 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,140 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,141 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,142 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,143 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,144 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,145 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,146 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,147 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,148 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,149 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,150 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,151 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,152 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,153 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,154 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,155 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,156 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,157 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,158 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,159 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,160 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,161 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,162 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,163 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,164 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,165 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,166 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,167 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,168 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,169 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,170 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,171 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,172 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,173 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,174 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,175 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,176 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,177 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,178 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,179 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,180 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,181 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,182 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,183 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,184 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,185 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,186 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,187 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,188 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,189 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,190 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,191 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,192 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,193 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,194 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,195 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,196 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,197 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,198 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,199 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,200 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,201 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,202 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.6% complete (62,203 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,204 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,205 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,206 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,207 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,208 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,209 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,210 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,211 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,212 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,213 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,214 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,215 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,216 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,217 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,218 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,219 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,220 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,221 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,222 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,223 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,224 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,225 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,226 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,227 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,228 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,229 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,230 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,231 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,232 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,233 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,234 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,235 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,236 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,237 / 65,719), 0:02:37 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,238 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,239 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,240 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,241 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,242 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,243 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,244 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,245 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,246 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,247 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,248 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,249 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,250 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,251 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,252 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,253 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,254 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,255 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,256 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,257 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,258 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,259 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,260 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:22 | 94.7% complete (62,261 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.7% complete (62,262 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.7% complete (62,263 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.7% complete (62,264 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.7% complete (62,265 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.7% complete (62,266 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.7% complete (62,267 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.7% complete (62,268 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,269 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,270 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,271 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,272 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,273 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,274 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,275 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,276 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,277 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,278 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,279 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,280 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,281 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,282 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,283 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,284 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,285 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,286 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,287 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,288 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,289 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,290 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,291 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,292 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,293 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,294 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,295 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,296 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,297 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,298 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,299 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,300 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,301 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,302 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,303 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,304 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,305 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,306 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,307 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,308 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,309 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,310 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,311 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,312 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,313 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,314 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,315 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,316 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,317 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,318 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,319 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,320 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,321 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,322 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,323 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,324 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,325 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,326 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,327 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,328 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,329 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,330 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,331 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,332 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,333 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.8% complete (62,334 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.9% complete (62,335 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.9% complete (62,336 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.9% complete (62,337 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.9% complete (62,338 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.9% complete (62,339 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.9% complete (62,340 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.9% complete (62,341 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.9% complete (62,342 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.9% complete (62,343 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.9% complete (62,344 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.9% complete (62,345 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.9% complete (62,346 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.9% complete (62,347 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.9% complete (62,348 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.9% complete (62,349 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.9% complete (62,350 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.9% complete (62,351 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.9% complete (62,352 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.9% complete (62,353 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.9% complete (62,354 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.9% complete (62,355 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.9% complete (62,356 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.9% complete (62,357 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.9% complete (62,358 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.9% complete (62,359 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.9% complete (62,360 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.9% complete (62,361 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.9% complete (62,362 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.9% complete (62,363 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.9% complete (62,364 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.9% complete (62,365 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.9% complete (62,366 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.9% complete (62,367 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.9% complete (62,368 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.9% complete (62,369 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.9% complete (62,370 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.9% complete (62,371 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.9% complete (62,372 / 65,719), 0:02:38 elapsed, 0:00:09 eta\n",
            "06:50:23 | 94.9% complete (62,373 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 94.9% complete (62,374 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 94.9% complete (62,375 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 94.9% complete (62,376 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 94.9% complete (62,377 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 94.9% complete (62,378 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 94.9% complete (62,379 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 94.9% complete (62,380 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 94.9% complete (62,381 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 94.9% complete (62,382 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 94.9% complete (62,383 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 94.9% complete (62,384 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 94.9% complete (62,385 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 94.9% complete (62,386 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 94.9% complete (62,387 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 94.9% complete (62,388 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 94.9% complete (62,389 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 94.9% complete (62,390 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 94.9% complete (62,391 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 94.9% complete (62,392 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 94.9% complete (62,393 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 94.9% complete (62,394 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 94.9% complete (62,395 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 94.9% complete (62,396 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 94.9% complete (62,397 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 94.9% complete (62,398 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 94.9% complete (62,399 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 94.9% complete (62,400 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,401 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,402 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,403 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,404 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,405 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,406 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,407 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,408 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,409 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,410 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,411 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,412 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,413 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,414 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,415 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,416 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,417 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,418 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,419 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,420 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,421 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,422 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,423 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,424 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,425 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,426 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,427 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,428 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,429 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,430 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,431 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,432 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,433 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,434 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,435 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,436 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,437 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,438 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,439 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,440 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,441 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,442 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,443 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,444 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,445 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,446 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,447 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,448 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,449 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,450 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,451 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,452 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,453 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,454 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,455 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,456 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,457 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,458 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,459 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,460 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,461 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,462 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,463 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,464 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.0% complete (62,465 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,466 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,467 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,468 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,469 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,470 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,471 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,472 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,473 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,474 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,475 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,476 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,477 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,478 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,479 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,480 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,481 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,482 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,483 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,484 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,485 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,486 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,487 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,488 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,489 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,490 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,491 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,492 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,493 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,494 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,495 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,496 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,497 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,498 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,499 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,500 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,501 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,502 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,503 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,504 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,505 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,506 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,507 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,508 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,509 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,510 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,511 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,512 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,513 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,514 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,515 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,516 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,517 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,518 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,519 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,520 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,521 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,522 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,523 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,524 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,525 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,526 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,527 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,528 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,529 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,530 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.1% complete (62,531 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,532 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,533 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,534 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,535 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,536 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,537 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,538 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,539 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,540 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,541 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,542 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,543 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,544 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,545 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,546 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,547 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,548 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,549 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,550 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,551 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,552 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,553 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,554 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,555 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,556 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,557 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,558 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,559 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,560 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,561 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,562 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,563 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,564 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,565 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,566 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,567 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,568 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,569 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,570 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,571 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,572 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,573 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,574 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,575 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,576 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,577 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,578 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,579 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,580 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,581 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,582 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,583 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,584 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,585 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,586 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,587 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,588 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,589 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,590 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,591 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,592 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,593 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,594 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,595 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,596 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.2% complete (62,597 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,598 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,599 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,600 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,601 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,602 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,603 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,604 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,605 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,606 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,607 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,608 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,609 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,610 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,611 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,612 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,613 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,614 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,615 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,616 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,617 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,618 / 65,719), 0:02:38 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,619 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,620 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,621 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,622 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,623 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,624 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,625 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,626 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,627 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,628 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,629 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,630 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,631 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,632 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,633 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,634 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,635 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,636 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,637 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,638 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,639 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,640 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,641 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,642 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,643 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,644 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,645 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,646 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,647 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,648 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,649 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,650 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,651 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,652 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,653 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,654 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,655 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,656 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,657 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,658 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,659 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,660 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,661 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,662 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.3% complete (62,663 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.4% complete (62,664 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.4% complete (62,665 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.4% complete (62,666 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.4% complete (62,667 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.4% complete (62,668 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.4% complete (62,669 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.4% complete (62,670 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.4% complete (62,671 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.4% complete (62,672 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.4% complete (62,673 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.4% complete (62,674 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.4% complete (62,675 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.4% complete (62,676 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.4% complete (62,677 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.4% complete (62,678 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.4% complete (62,679 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.4% complete (62,680 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.4% complete (62,681 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.4% complete (62,682 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.4% complete (62,683 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.4% complete (62,684 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.4% complete (62,685 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.4% complete (62,686 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.4% complete (62,687 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.4% complete (62,688 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.4% complete (62,689 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.4% complete (62,690 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:23 | 95.4% complete (62,691 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.4% complete (62,692 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.4% complete (62,693 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.4% complete (62,694 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.4% complete (62,695 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.4% complete (62,696 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.4% complete (62,697 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.4% complete (62,698 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.4% complete (62,699 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.4% complete (62,700 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.4% complete (62,701 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.4% complete (62,702 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.4% complete (62,703 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.4% complete (62,704 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.4% complete (62,705 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.4% complete (62,706 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.4% complete (62,707 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.4% complete (62,708 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.4% complete (62,709 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.4% complete (62,710 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.4% complete (62,711 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.4% complete (62,712 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.4% complete (62,713 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.4% complete (62,714 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.4% complete (62,715 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.4% complete (62,716 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.4% complete (62,717 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.4% complete (62,718 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.4% complete (62,719 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.4% complete (62,720 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.4% complete (62,721 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.4% complete (62,722 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.4% complete (62,723 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.4% complete (62,724 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.4% complete (62,725 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.4% complete (62,726 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.4% complete (62,727 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.4% complete (62,728 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.5% complete (62,729 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.5% complete (62,730 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.5% complete (62,731 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.5% complete (62,732 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.5% complete (62,733 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.5% complete (62,734 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.5% complete (62,735 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.5% complete (62,736 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.5% complete (62,737 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.5% complete (62,738 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.5% complete (62,739 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.5% complete (62,740 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.5% complete (62,741 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.5% complete (62,742 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.5% complete (62,743 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.5% complete (62,744 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.5% complete (62,745 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.5% complete (62,746 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.5% complete (62,747 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.5% complete (62,748 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.5% complete (62,749 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.5% complete (62,750 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.5% complete (62,751 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.5% complete (62,752 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.5% complete (62,753 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.5% complete (62,754 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.5% complete (62,755 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.5% complete (62,756 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.5% complete (62,757 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.5% complete (62,758 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.5% complete (62,759 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.5% complete (62,760 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.5% complete (62,761 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.5% complete (62,762 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.5% complete (62,763 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.5% complete (62,764 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.5% complete (62,765 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.5% complete (62,766 / 65,719), 0:02:39 elapsed, 0:00:08 eta\n",
            "06:50:24 | 95.5% complete (62,767 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.5% complete (62,768 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.5% complete (62,769 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.5% complete (62,770 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.5% complete (62,771 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.5% complete (62,772 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.5% complete (62,773 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.5% complete (62,774 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.5% complete (62,775 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.5% complete (62,776 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.5% complete (62,777 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.5% complete (62,778 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.5% complete (62,779 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.5% complete (62,780 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.5% complete (62,781 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.5% complete (62,782 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.5% complete (62,783 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.5% complete (62,784 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.5% complete (62,785 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.5% complete (62,786 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.5% complete (62,787 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.5% complete (62,788 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.5% complete (62,789 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.5% complete (62,790 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.5% complete (62,791 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.5% complete (62,792 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.5% complete (62,793 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.5% complete (62,794 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,795 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,796 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,797 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,798 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,799 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,800 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,801 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,802 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,803 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,804 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,805 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,806 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,807 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,808 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,809 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,810 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,811 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,812 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,813 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,814 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,815 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,816 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,817 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,818 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,819 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,820 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,821 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,822 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,823 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,824 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,825 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,826 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,827 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,828 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,829 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,830 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,831 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,832 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,833 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,834 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,835 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,836 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,837 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,838 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,839 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,840 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,841 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,842 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,843 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,844 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,845 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,846 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,847 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,848 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,849 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,850 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,851 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,852 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,853 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,854 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,855 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,856 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,857 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,858 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,859 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.6% complete (62,860 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,861 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,862 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,863 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,864 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,865 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,866 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,867 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,868 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,869 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,870 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,871 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,872 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,873 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,874 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,875 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,876 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,877 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,878 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,879 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,880 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,881 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,882 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,883 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,884 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,885 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,886 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,887 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,888 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,889 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,890 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,891 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,892 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,893 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,894 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,895 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,896 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,897 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,898 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,899 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,900 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,901 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,902 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,903 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,904 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,905 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,906 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,907 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,908 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,909 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,910 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,911 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,912 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,913 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,914 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,915 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,916 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,917 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,918 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,919 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,920 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,921 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,922 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,923 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,924 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.7% complete (62,925 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,926 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,927 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,928 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,929 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,930 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,931 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,932 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,933 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,934 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,935 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,936 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,937 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,938 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,939 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,940 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,941 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,942 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,943 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,944 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,945 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,946 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,947 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,948 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,949 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,950 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,951 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,952 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,953 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,954 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,955 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,956 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,957 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,958 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,959 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,960 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,961 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,962 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,963 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,964 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,965 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,966 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,967 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,968 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,969 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,970 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,971 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,972 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,973 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,974 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,975 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,976 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,977 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,978 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,979 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,980 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,981 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,982 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,983 / 65,719), 0:02:39 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,984 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,985 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,986 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,987 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,988 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,989 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,990 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.8% complete (62,991 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (62,992 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (62,993 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (62,994 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (62,995 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (62,996 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (62,997 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (62,998 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (62,999 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,000 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,001 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,002 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,003 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,004 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,005 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,006 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,007 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,008 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,009 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,010 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,011 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,012 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,013 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,014 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,015 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,016 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,017 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,018 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,019 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,020 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,021 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,022 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,023 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,024 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,025 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,026 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,027 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,028 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,029 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,030 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,031 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,032 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,033 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,034 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,035 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,036 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,037 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,038 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,039 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,040 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,041 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,042 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,043 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:24 | 95.9% complete (63,044 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 95.9% complete (63,045 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 95.9% complete (63,046 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 95.9% complete (63,047 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 95.9% complete (63,048 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 95.9% complete (63,049 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 95.9% complete (63,050 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 95.9% complete (63,051 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 95.9% complete (63,052 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 95.9% complete (63,053 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 95.9% complete (63,054 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 95.9% complete (63,055 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 95.9% complete (63,056 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 95.9% complete (63,057 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,058 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,059 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,060 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,061 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,062 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,063 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,064 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,065 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,066 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,067 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,068 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,069 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,070 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,071 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,072 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,073 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,074 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,075 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,076 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,077 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,078 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,079 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,080 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,081 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,082 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,083 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,084 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,085 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,086 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,087 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,088 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,089 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,090 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,091 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,092 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,093 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,094 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,095 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,096 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,097 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,098 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,099 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,100 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,101 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,102 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,103 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,104 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,105 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,106 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,107 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,108 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,109 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,110 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,111 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,112 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,113 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,114 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,115 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,116 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,117 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,118 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,119 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,120 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,121 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,122 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.0% complete (63,123 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.1% complete (63,124 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.1% complete (63,125 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.1% complete (63,126 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.1% complete (63,127 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.1% complete (63,128 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.1% complete (63,129 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.1% complete (63,130 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.1% complete (63,131 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.1% complete (63,132 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.1% complete (63,133 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.1% complete (63,134 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.1% complete (63,135 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.1% complete (63,136 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.1% complete (63,137 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.1% complete (63,138 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.1% complete (63,139 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.1% complete (63,140 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.1% complete (63,141 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.1% complete (63,142 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.1% complete (63,143 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.1% complete (63,144 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.1% complete (63,145 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.1% complete (63,146 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.1% complete (63,147 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.1% complete (63,148 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.1% complete (63,149 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.1% complete (63,150 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.1% complete (63,151 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.1% complete (63,152 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.1% complete (63,153 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.1% complete (63,154 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.1% complete (63,155 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.1% complete (63,156 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.1% complete (63,157 / 65,719), 0:02:40 elapsed, 0:00:07 eta\n",
            "06:50:25 | 96.1% complete (63,158 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.1% complete (63,159 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.1% complete (63,160 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.1% complete (63,161 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.1% complete (63,162 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.1% complete (63,163 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.1% complete (63,164 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.1% complete (63,165 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.1% complete (63,166 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.1% complete (63,167 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.1% complete (63,168 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.1% complete (63,169 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.1% complete (63,170 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.1% complete (63,171 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.1% complete (63,172 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.1% complete (63,173 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.1% complete (63,174 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.1% complete (63,175 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.1% complete (63,176 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.1% complete (63,177 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.1% complete (63,178 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.1% complete (63,179 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.1% complete (63,180 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.1% complete (63,181 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.1% complete (63,182 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.1% complete (63,183 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.1% complete (63,184 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.1% complete (63,185 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.1% complete (63,186 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.1% complete (63,187 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.1% complete (63,188 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,189 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,190 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,191 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,192 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,193 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,194 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,195 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,196 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,197 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,198 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,199 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,200 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,201 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,202 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,203 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,204 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,205 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,206 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,207 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,208 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,209 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,210 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,211 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,212 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,213 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,214 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,215 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,216 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,217 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,218 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,219 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,220 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,221 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,222 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,223 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,224 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,225 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,226 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,227 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,228 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,229 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,230 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,231 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,232 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,233 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,234 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,235 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,236 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,237 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,238 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,239 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,240 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,241 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,242 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,243 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,244 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,245 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,246 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,247 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,248 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,249 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,250 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,251 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,252 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,253 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.2% complete (63,254 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,255 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,256 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,257 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,258 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,259 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,260 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,261 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,262 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,263 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,264 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,265 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,266 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,267 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,268 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,269 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,270 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,271 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,272 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,273 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,274 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,275 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,276 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,277 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,278 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,279 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,280 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,281 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,282 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,283 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,284 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,285 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,286 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,287 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,288 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,289 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,290 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,291 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,292 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,293 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,294 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,295 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,296 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,297 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,298 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,299 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,300 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,301 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,302 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,303 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,304 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,305 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,306 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,307 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,308 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,309 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,310 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,311 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,312 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,313 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,314 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,315 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,316 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,317 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,318 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,319 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.3% complete (63,320 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.4% complete (63,321 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.4% complete (63,322 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.4% complete (63,323 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.4% complete (63,324 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.4% complete (63,325 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.4% complete (63,326 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.4% complete (63,327 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.4% complete (63,328 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.4% complete (63,329 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.4% complete (63,330 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.4% complete (63,331 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.4% complete (63,332 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.4% complete (63,333 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.4% complete (63,334 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:25 | 96.4% complete (63,335 / 65,719), 0:02:40 elapsed, 0:00:06 eta\n",
            "06:50:37 | Total parameters: 256,081,920 (256,081,920 trainable)\n",
            "06:50:38 | Loading existing model parameters from from_pretrained_bi/model\n",
            "06:50:48 | Loading fixed candidate set from from_pretrained_bi/model.cands-personachat.cands\n",
            "06:50:48 | Vectorizing fixed candidate set (126 batch(es) of up to 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 126/126 [00:06<00:00, 20.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "06:50:55 | Saving fixed candidate set vectors to from_pretrained_bi/model.model.cands-personachat.vecs\n",
            "06:50:55 | Encoding fixed candidates set from (251 batch(es) of up to 256) ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 251/251 [00:25<00:00,  9.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "06:51:21 | Saving fixed candidate set encodings to from_pretrained_bi/model.model.cands-personachat.encs\n",
            "06:51:23 | Opt:\n",
            "06:51:23 |     activation: gelu\n",
            "06:51:23 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "06:51:23 |     adam_eps: 1e-08\n",
            "06:51:23 |     add_p1_after_newln: False\n",
            "06:51:23 |     aggregate_micro: False\n",
            "06:51:23 |     allow_missing_init_opts: False\n",
            "06:51:23 |     attention_dropout: 0.1\n",
            "06:51:23 |     batchsize: 12\n",
            "06:51:23 |     betas: '[0.9, 0.999]'\n",
            "06:51:23 |     bpe_add_prefix_space: None\n",
            "06:51:23 |     bpe_debug: False\n",
            "06:51:23 |     bpe_dropout: None\n",
            "06:51:23 |     bpe_merge: None\n",
            "06:51:23 |     bpe_vocab: None\n",
            "06:51:23 |     candidates: batch\n",
            "06:51:23 |     cap_num_predictions: 100\n",
            "06:51:23 |     checkpoint_activations: False\n",
            "06:51:23 |     data_parallel: True\n",
            "06:51:23 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
            "06:51:23 |     datatype: train\n",
            "06:51:23 |     delimiter: '\\n'\n",
            "06:51:23 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "06:51:23 |     dict_endtoken: __start__\n",
            "06:51:23 |     dict_file: from_pretrained_bi/model.dict\n",
            "06:51:23 |     dict_include_test: False\n",
            "06:51:23 |     dict_include_valid: False\n",
            "06:51:23 |     dict_initpath: None\n",
            "06:51:23 |     dict_language: english\n",
            "06:51:23 |     dict_loaded: True\n",
            "06:51:23 |     dict_lower: True\n",
            "06:51:23 |     dict_max_ngram_size: -1\n",
            "06:51:23 |     dict_maxexs: -1\n",
            "06:51:23 |     dict_maxtokens: -1\n",
            "06:51:23 |     dict_minfreq: 0\n",
            "06:51:23 |     dict_nulltoken: __null__\n",
            "06:51:23 |     dict_starttoken: __start__\n",
            "06:51:23 |     dict_textfields: text,labels\n",
            "06:51:23 |     dict_tokenizer: bpe\n",
            "06:51:23 |     dict_unktoken: __unk__\n",
            "06:51:23 |     display_add_fields: \n",
            "06:51:23 |     display_examples: False\n",
            "06:51:23 |     display_prettify: False\n",
            "06:51:23 |     download_path: None\n",
            "06:51:23 |     dropout: 0.1\n",
            "06:51:23 |     dynamic_batching: None\n",
            "06:51:23 |     embedding_projection: random\n",
            "06:51:23 |     embedding_size: 768\n",
            "06:51:23 |     embedding_type: random\n",
            "06:51:23 |     embeddings_scale: False\n",
            "06:51:23 |     encode_candidate_vecs: True\n",
            "06:51:23 |     encode_candidate_vecs_batchsize: 256\n",
            "06:51:23 |     eval_batchsize: None\n",
            "06:51:23 |     eval_candidates: inline\n",
            "06:51:23 |     eval_dynamic_batching: None\n",
            "06:51:23 |     evaltask: None\n",
            "06:51:23 |     ffn_size: 3072\n",
            "06:51:23 |     final_extra_opt: \n",
            "06:51:23 |     fixed_candidate_vecs: reuse\n",
            "06:51:23 |     fixed_candidates_path: None\n",
            "06:51:23 |     force_fp16_tokens: True\n",
            "06:51:23 |     fp16: True\n",
            "06:51:23 |     fp16_impl: safe\n",
            "06:51:23 |     gpu: -1\n",
            "06:51:23 |     gradient_clip: 0.1\n",
            "06:51:23 |     hide_labels: False\n",
            "06:51:23 |     history_add_global_end_token: None\n",
            "06:51:23 |     history_reversed: False\n",
            "06:51:23 |     history_size: 20\n",
            "06:51:23 |     ignore_bad_candidates: False\n",
            "06:51:23 |     image_cropsize: 224\n",
            "06:51:23 |     image_mode: raw\n",
            "06:51:23 |     image_size: 256\n",
            "06:51:23 |     inference: max\n",
            "06:51:23 |     init_model: /usr/local/lib/python3.8/dist-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n",
            "06:51:23 |     init_opt: None\n",
            "06:51:23 |     interactive_candidates: fixed\n",
            "06:51:23 |     interactive_mode: True\n",
            "06:51:23 |     interactive_task: True\n",
            "06:51:23 |     invsqrt_lr_decay_gamma: -1\n",
            "06:51:23 |     is_debug: False\n",
            "06:51:23 |     label_truncate: 72\n",
            "06:51:23 |     learn_embeddings: True\n",
            "06:51:23 |     learn_positional_embeddings: True\n",
            "06:51:23 |     learningrate: 5e-05\n",
            "06:51:23 |     local_human_candidates_file: None\n",
            "06:51:23 |     log_every_n_secs: -1\n",
            "06:51:23 |     log_every_n_steps: 50\n",
            "06:51:23 |     log_keep_fields: all\n",
            "06:51:23 |     loglevel: info\n",
            "06:51:23 |     lr_scheduler: reduceonplateau\n",
            "06:51:23 |     lr_scheduler_decay: 0.4\n",
            "06:51:23 |     lr_scheduler_patience: 0\n",
            "06:51:23 |     max_train_steps: -1\n",
            "06:51:23 |     max_train_time: 60.0\n",
            "06:51:23 |     memory_attention: sqrt\n",
            "06:51:23 |     metrics: default\n",
            "06:51:23 |     model: transformer/biencoder\n",
            "06:51:23 |     model_file: from_pretrained_bi/model\n",
            "06:51:23 |     model_parallel: False\n",
            "06:51:23 |     momentum: 0\n",
            "06:51:23 |     multitask_weights: [1]\n",
            "06:51:23 |     mutators: None\n",
            "06:51:23 |     n_decoder_layers: -1\n",
            "06:51:23 |     n_encoder_layers: -1\n",
            "06:51:23 |     n_heads: 12\n",
            "06:51:23 |     n_layers: 12\n",
            "06:51:23 |     n_positions: 1024\n",
            "06:51:23 |     n_segments: 2\n",
            "06:51:23 |     nesterov: True\n",
            "06:51:23 |     no_cuda: False\n",
            "06:51:23 |     normalize_sent_emb: False\n",
            "06:51:23 |     num_epochs: -1\n",
            "06:51:23 |     num_workers: 0\n",
            "06:51:23 |     nus: [0.7]\n",
            "06:51:23 |     optimizer: adamax\n",
            "06:51:23 |     outfile: \n",
            "06:51:23 |     output_scaling: 0.06\n",
            "06:51:23 |     override: \"{'model_file': 'from_pretrained_bi/model'}\"\n",
            "06:51:23 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
            "06:51:23 |     person_tokens: False\n",
            "06:51:23 |     rank_candidates: True\n",
            "06:51:23 |     rank_top_k: -1\n",
            "06:51:23 |     reduction_type: mean\n",
            "06:51:23 |     relu_dropout: 0.0\n",
            "06:51:23 |     repeat_blocking_heuristic: True\n",
            "06:51:23 |     return_cand_scores: False\n",
            "06:51:23 |     save_after_valid: False\n",
            "06:51:23 |     save_every_n_secs: -1\n",
            "06:51:23 |     save_format: conversations\n",
            "06:51:23 |     share_encoders: False\n",
            "06:51:23 |     share_word_embeddings: False\n",
            "06:51:23 |     short_final_eval: False\n",
            "06:51:23 |     single_turn: False\n",
            "06:51:23 |     special_tok_lst: None\n",
            "06:51:23 |     split_lines: False\n",
            "06:51:23 |     starttime: Jan12_06-43\n",
            "06:51:23 |     task: personachat\n",
            "06:51:23 |     tensorboard_log: False\n",
            "06:51:23 |     tensorboard_logdir: None\n",
            "06:51:23 |     text_truncate: 360\n",
            "06:51:23 |     topk: 5\n",
            "06:51:23 |     train_predict: False\n",
            "06:51:23 |     truncate: 1024\n",
            "06:51:23 |     update_freq: 1\n",
            "06:51:23 |     use_memories: False\n",
            "06:51:23 |     use_reply: label\n",
            "06:51:23 |     validation_cutoff: 1.0\n",
            "06:51:23 |     validation_every_n_epochs: 0.25\n",
            "06:51:23 |     validation_every_n_secs: -1\n",
            "06:51:23 |     validation_every_n_steps: -1\n",
            "06:51:23 |     validation_max_exs: -1\n",
            "06:51:23 |     validation_metric: accuracy\n",
            "06:51:23 |     validation_metric_mode: max\n",
            "06:51:23 |     validation_patience: 10\n",
            "06:51:23 |     validation_share_agent: False\n",
            "06:51:23 |     variant: xlm\n",
            "06:51:23 |     verbose: False\n",
            "06:51:23 |     wandb_entity: None\n",
            "06:51:23 |     wandb_log: False\n",
            "06:51:23 |     wandb_name: None\n",
            "06:51:23 |     wandb_project: None\n",
            "06:51:23 |     warmup_rate: 0.0001\n",
            "06:51:23 |     warmup_updates: 100\n",
            "06:51:23 |     weight_decay: None\n",
            "06:51:23 |     world_logs: \n",
            "06:51:23 |     wrap_memory_encoder: False\n",
            "\u001b[1;31mEnter [DONE] if you want to end the episode, [EXIT] to quit.\u001b[0;0m\n",
            "06:51:24 | creating task(s): interactive\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m hi\n",
            "07:02:34 | \u001b[33m[ Executing eval mode with a common set of fixed candidates (n = 64119). ]\u001b[0m\n",
            "\u001b[0;34m[Biencoder]:\u001b[0;0m \u001b[1mhello\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m how old are you?\n",
            "\u001b[0;34m[Biencoder]:\u001b[0;0m \u001b[1mhow old are you ?\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m how are you?\n",
            "\u001b[0;34m[Biencoder]:\u001b[0;0m \u001b[1mjust how young are you ?\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m [Exit]\n",
            "\u001b[0;34m[Biencoder]:\u001b[0;0m \u001b[1moh how old are you now ?\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m [EXIT]\n",
            "CHAT DONE \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYuaSPWrw0Il"
      },
      "source": [
        "# 나만의 데이터셋 만들기\n",
        "\n",
        "이번에는 ParlAI를 이용해서 자신만의 데이터셋을 만들어보도록 하겠습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SgJi8XHwtph",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cc1bc27-12c2-459d-94f0-241fdfb32754"
      },
      "source": [
        "from parlai.core.teachers import register_teacher, DialogTeacher\n",
        "\n",
        "@register_teacher(\"my_teacher\")\n",
        "class MyTeacher(DialogTeacher):\n",
        "    def __init__(self, opt, shared=None):\n",
        "\n",
        "        # opt는 터미널에서 받는 argument입니다\n",
        "        # 데이터셋을 만들 때는 datafile을 설정해주어야합니다\n",
        "        # datafile은 데이터를 로드하는 경로를 의미합니다\n",
        "        # 오늘 실습에서는 (train/valid/test) + \".txt\"로 설정하도록 하겠습니다\n",
        "        opt['datafile'] = opt['datatype'].split(':')[0] + \".txt\"\n",
        "        super().__init__(opt, shared)\n",
        "    \n",
        "    def setup_data(self, datafile):\n",
        "        # 저희는 hardcoding 된 데이터를 사용할 것이지만 그래도 파일 경로를 출력하도록 하겠습니다\n",
        "        print(f\" ~~ Loading from {datafile} ~~ \")\n",
        "        \n",
        "        # setup_data에서는 ((text, label), new_episode) 형식의 튜플을 yield해야 합니다\n",
        "        # 데이터 타입은 이와 같습니다 ((str, str), bool)\n",
        "        \n",
        "        # 첫번째 에피소드\n",
        "        # 여기서 new_episode가 True인 것은 이번 에피소드의 첫 메시지임을 뜻합니다\n",
        "        yield ('Hello', 'Hi'), True\n",
        "        # 다음 턴에서는 에피소드가 계속되는 것을 표시해주기 위해서 new_episode를 False로 넘겨줍니다\n",
        "        yield ('How are you', 'I am fine'), False\n",
        "        yield (\"Let's say goodbye\", 'Goodbye!'), False\n",
        "        \n",
        "        # 두번째 에피소드입니다\n",
        "        # 새로운 에피소드의 시작이기 때문에 new_episode를 True로 지정해줍니다\n",
        "        yield (\"Hey\", \"hi there\"), True\n",
        "        yield (\"Deja vu?\", \"Deja vu!\"), False\n",
        "        yield (\"Last chance\", \"This is it\"), False\n",
        "\n",
        "        # 세번째 에피소드\n",
        "        yield (\"Hello\", \"Hi\"), True\n",
        "        yield (\"Where are you now?\", \"I'm in yonsei unversity\"), False\n",
        "        yield (\"Okay bye\", \"Bye\"), False\n",
        "        \n",
        "        \n",
        "DisplayData.main(task=\"my_teacher\")"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "07:03:19 | Opt:\n",
            "07:03:19 |     allow_missing_init_opts: False\n",
            "07:03:19 |     batchsize: 1\n",
            "07:03:19 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
            "07:03:19 |     datatype: train:ordered\n",
            "07:03:19 |     dict_class: None\n",
            "07:03:19 |     display_add_fields: \n",
            "07:03:19 |     download_path: None\n",
            "07:03:19 |     dynamic_batching: None\n",
            "07:03:19 |     hide_labels: False\n",
            "07:03:19 |     ignore_agent_reply: True\n",
            "07:03:19 |     image_cropsize: 224\n",
            "07:03:19 |     image_mode: raw\n",
            "07:03:19 |     image_size: 256\n",
            "07:03:19 |     init_model: None\n",
            "07:03:19 |     init_opt: None\n",
            "07:03:19 |     is_debug: False\n",
            "07:03:19 |     loglevel: info\n",
            "07:03:19 |     max_display_len: 1000\n",
            "07:03:19 |     model: None\n",
            "07:03:19 |     model_file: None\n",
            "07:03:19 |     multitask_weights: [1]\n",
            "07:03:19 |     mutators: None\n",
            "07:03:19 |     num_examples: 10\n",
            "07:03:19 |     override: \"{'task': 'my_teacher'}\"\n",
            "07:03:19 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
            "07:03:19 |     starttime: Jan12_07-03\n",
            "07:03:19 |     task: my_teacher\n",
            "07:03:19 |     verbose: False\n",
            "07:03:20 | creating task(s): my_teacher\n",
            " ~~ Loading from train.txt ~~ \n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher - - -\u001b[0;0m\n",
            "\u001b[0mHello\u001b[0;0m\n",
            "   \u001b[1;94mHi\u001b[0;0m\n",
            "\u001b[0mHow are you\u001b[0;0m\n",
            "   \u001b[1;94mI am fine\u001b[0;0m\n",
            "\u001b[0mLet's say goodbye\u001b[0;0m\n",
            "   \u001b[1;94mGoodbye!\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher - - -\u001b[0;0m\n",
            "\u001b[0mHey\u001b[0;0m\n",
            "   \u001b[1;94mhi there\u001b[0;0m\n",
            "\u001b[0mDeja vu?\u001b[0;0m\n",
            "   \u001b[1;94mDeja vu!\u001b[0;0m\n",
            "\u001b[0mLast chance\u001b[0;0m\n",
            "   \u001b[1;94mThis is it\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher - - -\u001b[0;0m\n",
            "\u001b[0mHello\u001b[0;0m\n",
            "   \u001b[1;94mHi\u001b[0;0m\n",
            "\u001b[0mWhere are you now?\u001b[0;0m\n",
            "   \u001b[1;94mI'm in yonsei unversity\u001b[0;0m\n",
            "\u001b[0mOkay bye\u001b[0;0m\n",
            "   \u001b[1;94mBye\u001b[0;0m\n",
            "07:03:20 | epoch done\n",
            "07:03:20 | loaded 3 episodes with a total of 9 examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✨실습✨\n",
        "\n",
        "- 에피소드가 다섯개인 데이터를 직접 만들어보세요"
      ],
      "metadata": {
        "id": "SeLOgyNv5pti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from parlai.core.teachers import register_teacher, DialogTeacher\n",
        "\n",
        "@register_teacher(\"my_teacher2\")\n",
        "class MyTeacher(DialogTeacher):\n",
        "    def __init__(self, opt, shared=None):\n",
        "\n",
        "        opt['datafile'] = opt['datatype'].split(':')[0] + \".txt\"\n",
        "        super().__init__(opt, shared)\n",
        "    \n",
        "    def setup_data(self, datafile):\n",
        "        print(f\" ~~ Loading from {datafile} ~~ \")\n",
        "        \n",
        "        yield ('Hello', 'Hi'), True\n",
        "        \n",
        "        \n",
        "DisplayData.main(task=\"my_teacher2\")"
      ],
      "metadata": {
        "id": "_29IgB1K5tX4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5c20162-5346-41de-d8d5-599dfa2afa84"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "07:03:20 | Opt:\n",
            "07:03:20 |     allow_missing_init_opts: False\n",
            "07:03:20 |     batchsize: 1\n",
            "07:03:20 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
            "07:03:20 |     datatype: train:ordered\n",
            "07:03:20 |     dict_class: None\n",
            "07:03:20 |     display_add_fields: \n",
            "07:03:20 |     download_path: None\n",
            "07:03:20 |     dynamic_batching: None\n",
            "07:03:20 |     hide_labels: False\n",
            "07:03:20 |     ignore_agent_reply: True\n",
            "07:03:20 |     image_cropsize: 224\n",
            "07:03:20 |     image_mode: raw\n",
            "07:03:20 |     image_size: 256\n",
            "07:03:20 |     init_model: None\n",
            "07:03:20 |     init_opt: None\n",
            "07:03:20 |     is_debug: False\n",
            "07:03:20 |     loglevel: info\n",
            "07:03:20 |     max_display_len: 1000\n",
            "07:03:20 |     model: None\n",
            "07:03:20 |     model_file: None\n",
            "07:03:20 |     multitask_weights: [1]\n",
            "07:03:20 |     mutators: None\n",
            "07:03:20 |     num_examples: 10\n",
            "07:03:20 |     override: \"{'task': 'my_teacher2'}\"\n",
            "07:03:20 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
            "07:03:20 |     starttime: Jan12_07-03\n",
            "07:03:20 |     task: my_teacher2\n",
            "07:03:20 |     verbose: False\n",
            "07:03:21 | creating task(s): my_teacher2\n",
            " ~~ Loading from train.txt ~~ \n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher2 - - -\u001b[0;0m\n",
            "\u001b[0mHello\u001b[0;0m\n",
            "   \u001b[1;94mHi\u001b[0;0m\n",
            "07:03:21 | epoch done\n",
            "07:03:21 | loaded 1 episodes with a total of 1 examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vvwxi6gXw8jU"
      },
      "source": [
        "이를 통해 ParlAI에서 데이터를 어떤 식으로 제공하는지 살펴볼 수 있었습니다. 실제 모델을 학습시킬 때는 데이터 파일을 불러와서 loop를 돌려 processed data에서 제공하는 tuple들을 yield 시킵니다.\n",
        "\n",
        "이제는 저희가 만든 task를 사용할 수 있습니다. 이전에 학습시킨 모델이 새로 만든 데이터에 어떻게 대응하는지 살펴봅시다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIyZQnxAw5HG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0560111-a771-48a2-b41c-32822864d9ed"
      },
      "source": [
        "DisplayModel.main(task='my_teacher', model_file='from_pretrained/model2',skip_generation=False)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "07:04:28 | \u001b[33mOverriding opt[\"task\"] to my_teacher (previously: empathetic_dialogues)\u001b[0m\n",
            "07:04:28 | \u001b[33mOverriding opt[\"skip_generation\"] to False (previously: True)\u001b[0m\n",
            "07:04:29 | Using CUDA\n",
            "07:04:29 | loading dictionary from from_pretrained/model2.dict\n",
            "07:04:29 | num words = 54944\n",
            "07:04:32 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "07:04:32 | Loading existing model params from from_pretrained/model2\n",
            "07:04:41 | creating task(s): my_teacher\n",
            " ~~ Loading from valid.txt ~~ \n",
            "07:04:41 | Opt:\n",
            "07:04:41 |     activation: gelu\n",
            "07:04:41 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "07:04:41 |     adam_eps: 1e-08\n",
            "07:04:41 |     add_p1_after_newln: False\n",
            "07:04:41 |     aggregate_micro: False\n",
            "07:04:41 |     allow_missing_init_opts: False\n",
            "07:04:41 |     attention_dropout: 0.0\n",
            "07:04:41 |     batchsize: 12\n",
            "07:04:41 |     beam_block_full_context: True\n",
            "07:04:41 |     beam_block_list_filename: None\n",
            "07:04:41 |     beam_block_ngram: -1\n",
            "07:04:41 |     beam_context_block_ngram: -1\n",
            "07:04:41 |     beam_delay: 30\n",
            "07:04:41 |     beam_length_penalty: 0.65\n",
            "07:04:41 |     beam_min_length: 1\n",
            "07:04:41 |     beam_size: 1\n",
            "07:04:41 |     betas: '[0.9, 0.999]'\n",
            "07:04:41 |     bpe_add_prefix_space: None\n",
            "07:04:41 |     bpe_debug: False\n",
            "07:04:41 |     bpe_dropout: None\n",
            "07:04:41 |     bpe_merge: None\n",
            "07:04:41 |     bpe_vocab: None\n",
            "07:04:41 |     checkpoint_activations: False\n",
            "07:04:41 |     compute_tokenized_bleu: False\n",
            "07:04:41 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
            "07:04:41 |     datatype: train\n",
            "07:04:41 |     delimiter: '\\n'\n",
            "07:04:41 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "07:04:41 |     dict_endtoken: __end__\n",
            "07:04:41 |     dict_file: from_pretrained/model2.dict\n",
            "07:04:41 |     dict_include_test: False\n",
            "07:04:41 |     dict_include_valid: False\n",
            "07:04:41 |     dict_initpath: None\n",
            "07:04:41 |     dict_language: english\n",
            "07:04:41 |     dict_loaded: True\n",
            "07:04:41 |     dict_lower: True\n",
            "07:04:41 |     dict_max_ngram_size: -1\n",
            "07:04:41 |     dict_maxexs: -1\n",
            "07:04:41 |     dict_maxtokens: -1\n",
            "07:04:41 |     dict_minfreq: 0\n",
            "07:04:41 |     dict_nulltoken: __null__\n",
            "07:04:41 |     dict_starttoken: __start__\n",
            "07:04:41 |     dict_textfields: text,labels\n",
            "07:04:41 |     dict_tokenizer: bpe\n",
            "07:04:41 |     dict_unktoken: __unk__\n",
            "07:04:41 |     display_add_fields: \n",
            "07:04:41 |     display_examples: False\n",
            "07:04:41 |     download_path: None\n",
            "07:04:41 |     dropout: 0.0\n",
            "07:04:41 |     dynamic_batching: full\n",
            "07:04:41 |     embedding_projection: random\n",
            "07:04:41 |     embedding_size: 512\n",
            "07:04:41 |     embedding_type: random\n",
            "07:04:41 |     embeddings_scale: True\n",
            "07:04:41 |     eval_batchsize: None\n",
            "07:04:41 |     eval_dynamic_batching: None\n",
            "07:04:41 |     evaltask: None\n",
            "07:04:41 |     ffn_size: 2048\n",
            "07:04:41 |     final_extra_opt: \n",
            "07:04:41 |     force_fp16_tokens: True\n",
            "07:04:41 |     fp16: True\n",
            "07:04:41 |     fp16_impl: mem_efficient\n",
            "07:04:41 |     gpu: -1\n",
            "07:04:41 |     gradient_clip: 0.1\n",
            "07:04:41 |     hide_labels: False\n",
            "07:04:41 |     history_add_global_end_token: None\n",
            "07:04:41 |     history_reversed: False\n",
            "07:04:41 |     history_size: -1\n",
            "07:04:41 |     image_cropsize: 224\n",
            "07:04:41 |     image_mode: raw\n",
            "07:04:41 |     image_size: 256\n",
            "07:04:41 |     inference: greedy\n",
            "07:04:41 |     init_model: /usr/local/lib/python3.8/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "07:04:41 |     init_opt: None\n",
            "07:04:41 |     interactive_mode: False\n",
            "07:04:41 |     invsqrt_lr_decay_gamma: -1\n",
            "07:04:41 |     is_debug: False\n",
            "07:04:41 |     label_truncate: 128\n",
            "07:04:41 |     learn_positional_embeddings: True\n",
            "07:04:41 |     learningrate: 1e-05\n",
            "07:04:41 |     log_every_n_secs: -1\n",
            "07:04:41 |     log_every_n_steps: 50\n",
            "07:04:41 |     log_keep_fields: all\n",
            "07:04:41 |     loglevel: info\n",
            "07:04:41 |     lr_scheduler: reduceonplateau\n",
            "07:04:41 |     lr_scheduler_decay: 0.5\n",
            "07:04:41 |     lr_scheduler_patience: 3\n",
            "07:04:41 |     max_train_steps: -1\n",
            "07:04:41 |     max_train_time: 60.0\n",
            "07:04:41 |     metrics: default\n",
            "07:04:41 |     model: transformer/generator\n",
            "07:04:41 |     model_file: from_pretrained/model2\n",
            "07:04:41 |     model_parallel: False\n",
            "07:04:41 |     momentum: 0\n",
            "07:04:41 |     multitask_weights: [1]\n",
            "07:04:41 |     mutators: None\n",
            "07:04:41 |     n_decoder_layers: -1\n",
            "07:04:41 |     n_encoder_layers: -1\n",
            "07:04:41 |     n_heads: 16\n",
            "07:04:41 |     n_layers: 8\n",
            "07:04:41 |     n_positions: 512\n",
            "07:04:41 |     n_segments: 0\n",
            "07:04:41 |     nesterov: True\n",
            "07:04:41 |     no_cuda: False\n",
            "07:04:41 |     num_epochs: -1\n",
            "07:04:41 |     num_examples: 10\n",
            "07:04:41 |     num_workers: 0\n",
            "07:04:41 |     nus: [0.7]\n",
            "07:04:41 |     optimizer: mem_eff_adam\n",
            "07:04:41 |     output_scaling: 1.0\n",
            "07:04:41 |     override: \"{'task': 'my_teacher', 'model_file': 'from_pretrained/model2', 'skip_generation': False}\"\n",
            "07:04:41 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
            "07:04:41 |     person_tokens: False\n",
            "07:04:41 |     rank_candidates: False\n",
            "07:04:41 |     relu_dropout: 0.0\n",
            "07:04:41 |     save_after_valid: False\n",
            "07:04:41 |     save_every_n_secs: -1\n",
            "07:04:41 |     save_format: conversations\n",
            "07:04:41 |     share_word_embeddings: True\n",
            "07:04:41 |     short_final_eval: False\n",
            "07:04:41 |     skip_generation: False\n",
            "07:04:41 |     special_tok_lst: None\n",
            "07:04:41 |     split_lines: False\n",
            "07:04:41 |     starttime: Jan12_05-41\n",
            "07:04:41 |     task: my_teacher\n",
            "07:04:41 |     temperature: 1.0\n",
            "07:04:41 |     tensorboard_log: False\n",
            "07:04:41 |     tensorboard_logdir: None\n",
            "07:04:41 |     text_truncate: 512\n",
            "07:04:41 |     topk: 10\n",
            "07:04:41 |     topp: 0.9\n",
            "07:04:41 |     train_experiencer_only: False\n",
            "07:04:41 |     truncate: -1\n",
            "07:04:41 |     update_freq: 1\n",
            "07:04:41 |     use_reply: label\n",
            "07:04:41 |     validation_cutoff: 1.0\n",
            "07:04:41 |     validation_every_n_epochs: 0.25\n",
            "07:04:41 |     validation_every_n_secs: -1\n",
            "07:04:41 |     validation_every_n_steps: -1\n",
            "07:04:41 |     validation_max_exs: -1\n",
            "07:04:41 |     validation_metric: ppl\n",
            "07:04:41 |     validation_metric_mode: None\n",
            "07:04:41 |     validation_patience: 10\n",
            "07:04:41 |     validation_share_agent: False\n",
            "07:04:41 |     variant: xlm\n",
            "07:04:41 |     verbose: False\n",
            "07:04:41 |     wandb_entity: None\n",
            "07:04:41 |     wandb_log: False\n",
            "07:04:41 |     wandb_name: None\n",
            "07:04:41 |     wandb_project: None\n",
            "07:04:41 |     warmup_rate: 0.0001\n",
            "07:04:41 |     warmup_updates: 100\n",
            "07:04:41 |     weight_decay: None\n",
            "07:04:41 |     world_logs: \n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
            "\u001b[0mHello\u001b[0;0m\n",
            "\u001b[1;94m    labels: Hi\u001b[0;0m\n",
            "\u001b[0;95m     model: hi\u001b[0;0m\n",
            "\u001b[0mHow are you\u001b[0;0m\n",
            "\u001b[1;94m    labels: I am fine\u001b[0;0m\n",
            "\u001b[0;95m     model: i ' m good , how are you ?\u001b[0;0m\n",
            "\u001b[0mLet's say goodbye\u001b[0;0m\n",
            "\u001b[1;94m    labels: Goodbye!\u001b[0;0m\n",
            "\u001b[0;95m     model: i am fine\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
            "\u001b[0mHey\u001b[0;0m\n",
            "\u001b[1;94m    labels: hi there\u001b[0;0m\n",
            "\u001b[0;95m     model: hey\u001b[0;0m\n",
            "\u001b[0mDeja vu?\u001b[0;0m\n",
            "\u001b[1;94m    labels: Deja vu!\u001b[0;0m\n",
            "\u001b[0;95m     model: i ' ve just been in this place before\u001b[0;0m\n",
            "\u001b[0mLast chance\u001b[0;0m\n",
            "\u001b[1;94m    labels: This is it\u001b[0;0m\n",
            "\u001b[0;95m     model: i ' ve just been in this place before\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
            "\u001b[0mHello\u001b[0;0m\n",
            "\u001b[1;94m    labels: Hi\u001b[0;0m\n",
            "\u001b[0;95m     model: hi\u001b[0;0m\n",
            "\u001b[0mWhere are you now?\u001b[0;0m\n",
            "\u001b[1;94m    labels: I'm in yonsei unversity\u001b[0;0m\n",
            "\u001b[0;95m     model: i ' m in the middle of a long road trip .\u001b[0;0m\n",
            "\u001b[0mOkay bye\u001b[0;0m\n",
            "\u001b[1;94m    labels: Bye\u001b[0;0m\n",
            "\u001b[0;95m     model: i ' m sorry\u001b[0;0m\n",
            "07:04:43 | epoch done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✨실습✨\n",
        "\n",
        "- 학습시킨 모델이 어떻게 대응하는지 display_model로 살펴보세요\n",
        "- 학습시킨 모델이 직접 만든 데이터에 평가했을때 성능이 어느정도 나오는지 살펴보세요"
      ],
      "metadata": {
        "id": "bnR_JdEE57N5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DisplayModel.main(task='my_teacher', model_file='from_pretrained/model2', skip_generation=False)"
      ],
      "metadata": {
        "id": "wik361Zj6K_q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6682332-40ff-4a82-fa44-50663f90a9e0"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "07:05:35 | \u001b[33mOverriding opt[\"task\"] to my_teacher (previously: empathetic_dialogues)\u001b[0m\n",
            "07:05:35 | \u001b[33mOverriding opt[\"skip_generation\"] to False (previously: True)\u001b[0m\n",
            "07:05:35 | Using CUDA\n",
            "07:05:35 | loading dictionary from from_pretrained/model2.dict\n",
            "07:05:36 | num words = 54944\n",
            "07:05:37 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "07:05:37 | Loading existing model params from from_pretrained/model2\n",
            "07:05:42 | creating task(s): my_teacher\n",
            " ~~ Loading from valid.txt ~~ \n",
            "07:05:42 | Opt:\n",
            "07:05:42 |     activation: gelu\n",
            "07:05:42 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "07:05:42 |     adam_eps: 1e-08\n",
            "07:05:42 |     add_p1_after_newln: False\n",
            "07:05:42 |     aggregate_micro: False\n",
            "07:05:42 |     allow_missing_init_opts: False\n",
            "07:05:42 |     attention_dropout: 0.0\n",
            "07:05:42 |     batchsize: 12\n",
            "07:05:42 |     beam_block_full_context: True\n",
            "07:05:42 |     beam_block_list_filename: None\n",
            "07:05:42 |     beam_block_ngram: -1\n",
            "07:05:42 |     beam_context_block_ngram: -1\n",
            "07:05:42 |     beam_delay: 30\n",
            "07:05:42 |     beam_length_penalty: 0.65\n",
            "07:05:42 |     beam_min_length: 1\n",
            "07:05:42 |     beam_size: 1\n",
            "07:05:42 |     betas: '[0.9, 0.999]'\n",
            "07:05:42 |     bpe_add_prefix_space: None\n",
            "07:05:42 |     bpe_debug: False\n",
            "07:05:42 |     bpe_dropout: None\n",
            "07:05:42 |     bpe_merge: None\n",
            "07:05:42 |     bpe_vocab: None\n",
            "07:05:42 |     checkpoint_activations: False\n",
            "07:05:42 |     compute_tokenized_bleu: False\n",
            "07:05:42 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
            "07:05:42 |     datatype: train\n",
            "07:05:42 |     delimiter: '\\n'\n",
            "07:05:42 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "07:05:42 |     dict_endtoken: __end__\n",
            "07:05:42 |     dict_file: from_pretrained/model2.dict\n",
            "07:05:42 |     dict_include_test: False\n",
            "07:05:42 |     dict_include_valid: False\n",
            "07:05:42 |     dict_initpath: None\n",
            "07:05:42 |     dict_language: english\n",
            "07:05:42 |     dict_loaded: True\n",
            "07:05:42 |     dict_lower: True\n",
            "07:05:42 |     dict_max_ngram_size: -1\n",
            "07:05:42 |     dict_maxexs: -1\n",
            "07:05:42 |     dict_maxtokens: -1\n",
            "07:05:42 |     dict_minfreq: 0\n",
            "07:05:42 |     dict_nulltoken: __null__\n",
            "07:05:42 |     dict_starttoken: __start__\n",
            "07:05:42 |     dict_textfields: text,labels\n",
            "07:05:42 |     dict_tokenizer: bpe\n",
            "07:05:42 |     dict_unktoken: __unk__\n",
            "07:05:42 |     display_add_fields: \n",
            "07:05:42 |     display_examples: False\n",
            "07:05:42 |     download_path: None\n",
            "07:05:42 |     dropout: 0.0\n",
            "07:05:42 |     dynamic_batching: full\n",
            "07:05:42 |     embedding_projection: random\n",
            "07:05:42 |     embedding_size: 512\n",
            "07:05:42 |     embedding_type: random\n",
            "07:05:42 |     embeddings_scale: True\n",
            "07:05:42 |     eval_batchsize: None\n",
            "07:05:42 |     eval_dynamic_batching: None\n",
            "07:05:42 |     evaltask: None\n",
            "07:05:42 |     ffn_size: 2048\n",
            "07:05:42 |     final_extra_opt: \n",
            "07:05:42 |     force_fp16_tokens: True\n",
            "07:05:42 |     fp16: True\n",
            "07:05:42 |     fp16_impl: mem_efficient\n",
            "07:05:42 |     gpu: -1\n",
            "07:05:42 |     gradient_clip: 0.1\n",
            "07:05:42 |     hide_labels: False\n",
            "07:05:42 |     history_add_global_end_token: None\n",
            "07:05:42 |     history_reversed: False\n",
            "07:05:42 |     history_size: -1\n",
            "07:05:42 |     image_cropsize: 224\n",
            "07:05:42 |     image_mode: raw\n",
            "07:05:42 |     image_size: 256\n",
            "07:05:42 |     inference: greedy\n",
            "07:05:42 |     init_model: /usr/local/lib/python3.8/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "07:05:42 |     init_opt: None\n",
            "07:05:42 |     interactive_mode: False\n",
            "07:05:42 |     invsqrt_lr_decay_gamma: -1\n",
            "07:05:42 |     is_debug: False\n",
            "07:05:42 |     label_truncate: 128\n",
            "07:05:42 |     learn_positional_embeddings: True\n",
            "07:05:42 |     learningrate: 1e-05\n",
            "07:05:42 |     log_every_n_secs: -1\n",
            "07:05:42 |     log_every_n_steps: 50\n",
            "07:05:42 |     log_keep_fields: all\n",
            "07:05:42 |     loglevel: info\n",
            "07:05:42 |     lr_scheduler: reduceonplateau\n",
            "07:05:42 |     lr_scheduler_decay: 0.5\n",
            "07:05:42 |     lr_scheduler_patience: 3\n",
            "07:05:42 |     max_train_steps: -1\n",
            "07:05:42 |     max_train_time: 60.0\n",
            "07:05:42 |     metrics: default\n",
            "07:05:42 |     model: transformer/generator\n",
            "07:05:42 |     model_file: from_pretrained/model2\n",
            "07:05:42 |     model_parallel: False\n",
            "07:05:42 |     momentum: 0\n",
            "07:05:42 |     multitask_weights: [1]\n",
            "07:05:42 |     mutators: None\n",
            "07:05:42 |     n_decoder_layers: -1\n",
            "07:05:42 |     n_encoder_layers: -1\n",
            "07:05:42 |     n_heads: 16\n",
            "07:05:42 |     n_layers: 8\n",
            "07:05:42 |     n_positions: 512\n",
            "07:05:42 |     n_segments: 0\n",
            "07:05:42 |     nesterov: True\n",
            "07:05:42 |     no_cuda: False\n",
            "07:05:42 |     num_epochs: -1\n",
            "07:05:42 |     num_examples: 10\n",
            "07:05:42 |     num_workers: 0\n",
            "07:05:42 |     nus: [0.7]\n",
            "07:05:42 |     optimizer: mem_eff_adam\n",
            "07:05:42 |     output_scaling: 1.0\n",
            "07:05:42 |     override: \"{'task': 'my_teacher', 'model_file': 'from_pretrained/model2', 'skip_generation': False}\"\n",
            "07:05:42 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
            "07:05:42 |     person_tokens: False\n",
            "07:05:42 |     rank_candidates: False\n",
            "07:05:42 |     relu_dropout: 0.0\n",
            "07:05:42 |     save_after_valid: False\n",
            "07:05:42 |     save_every_n_secs: -1\n",
            "07:05:42 |     save_format: conversations\n",
            "07:05:42 |     share_word_embeddings: True\n",
            "07:05:42 |     short_final_eval: False\n",
            "07:05:42 |     skip_generation: False\n",
            "07:05:42 |     special_tok_lst: None\n",
            "07:05:42 |     split_lines: False\n",
            "07:05:42 |     starttime: Jan12_05-41\n",
            "07:05:42 |     task: my_teacher\n",
            "07:05:42 |     temperature: 1.0\n",
            "07:05:42 |     tensorboard_log: False\n",
            "07:05:42 |     tensorboard_logdir: None\n",
            "07:05:42 |     text_truncate: 512\n",
            "07:05:42 |     topk: 10\n",
            "07:05:42 |     topp: 0.9\n",
            "07:05:42 |     train_experiencer_only: False\n",
            "07:05:42 |     truncate: -1\n",
            "07:05:42 |     update_freq: 1\n",
            "07:05:42 |     use_reply: label\n",
            "07:05:42 |     validation_cutoff: 1.0\n",
            "07:05:42 |     validation_every_n_epochs: 0.25\n",
            "07:05:42 |     validation_every_n_secs: -1\n",
            "07:05:42 |     validation_every_n_steps: -1\n",
            "07:05:42 |     validation_max_exs: -1\n",
            "07:05:42 |     validation_metric: ppl\n",
            "07:05:42 |     validation_metric_mode: None\n",
            "07:05:42 |     validation_patience: 10\n",
            "07:05:42 |     validation_share_agent: False\n",
            "07:05:42 |     variant: xlm\n",
            "07:05:42 |     verbose: False\n",
            "07:05:42 |     wandb_entity: None\n",
            "07:05:42 |     wandb_log: False\n",
            "07:05:42 |     wandb_name: None\n",
            "07:05:42 |     wandb_project: None\n",
            "07:05:42 |     warmup_rate: 0.0001\n",
            "07:05:42 |     warmup_updates: 100\n",
            "07:05:42 |     weight_decay: None\n",
            "07:05:42 |     world_logs: \n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
            "\u001b[0mHello\u001b[0;0m\n",
            "\u001b[1;94m    labels: Hi\u001b[0;0m\n",
            "\u001b[0;95m     model: hi\u001b[0;0m\n",
            "\u001b[0mHow are you\u001b[0;0m\n",
            "\u001b[1;94m    labels: I am fine\u001b[0;0m\n",
            "\u001b[0;95m     model: i ' m good , how are you ?\u001b[0;0m\n",
            "\u001b[0mLet's say goodbye\u001b[0;0m\n",
            "\u001b[1;94m    labels: Goodbye!\u001b[0;0m\n",
            "\u001b[0;95m     model: i am fine\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
            "\u001b[0mHey\u001b[0;0m\n",
            "\u001b[1;94m    labels: hi there\u001b[0;0m\n",
            "\u001b[0;95m     model: hey\u001b[0;0m\n",
            "\u001b[0mDeja vu?\u001b[0;0m\n",
            "\u001b[1;94m    labels: Deja vu!\u001b[0;0m\n",
            "\u001b[0;95m     model: i ' ve just been in this place before\u001b[0;0m\n",
            "\u001b[0mLast chance\u001b[0;0m\n",
            "\u001b[1;94m    labels: This is it\u001b[0;0m\n",
            "\u001b[0;95m     model: i ' ve just been in this place before\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
            "\u001b[0mHello\u001b[0;0m\n",
            "\u001b[1;94m    labels: Hi\u001b[0;0m\n",
            "\u001b[0;95m     model: hi\u001b[0;0m\n",
            "\u001b[0mWhere are you now?\u001b[0;0m\n",
            "\u001b[1;94m    labels: I'm in yonsei unversity\u001b[0;0m\n",
            "\u001b[0;95m     model: i ' m in the middle of a long road trip .\u001b[0;0m\n",
            "\u001b[0mOkay bye\u001b[0;0m\n",
            "\u001b[1;94m    labels: Bye\u001b[0;0m\n",
            "\u001b[0;95m     model: i ' m sorry\u001b[0;0m\n",
            "07:05:44 | epoch done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from parlai.scripts.eval_model import EvalModel\n",
        "\n",
        "EvalModel.main(\n",
        "    task='empathetic_dialogues',\n",
        "    model_file='from_pretrained/model2',\n",
        "    num_examples=2,\n",
        "    skip_generation=False,\n",
        "    datatype='test',\n",
        "    metrics='bleu,ppl'\n",
        ")"
      ],
      "metadata": {
        "id": "HVkPZl9G6Oyn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db935efc-594a-4e30-b0cc-9fe0b8051697"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "07:06:01 | \u001b[33mOverriding opt[\"datatype\"] to test (previously: train)\u001b[0m\n",
            "07:06:01 | \u001b[33mOverriding opt[\"skip_generation\"] to False (previously: True)\u001b[0m\n",
            "07:06:01 | \u001b[33mOverriding opt[\"metrics\"] to bleu,ppl (previously: default)\u001b[0m\n",
            "07:06:01 | Using CUDA\n",
            "07:06:01 | loading dictionary from from_pretrained/model2.dict\n",
            "07:06:01 | num words = 54944\n",
            "07:06:03 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "07:06:03 | Loading existing model params from from_pretrained/model2\n",
            "07:06:05 | Opt:\n",
            "07:06:05 |     activation: gelu\n",
            "07:06:05 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "07:06:05 |     adam_eps: 1e-08\n",
            "07:06:05 |     add_p1_after_newln: False\n",
            "07:06:05 |     aggregate_micro: False\n",
            "07:06:05 |     allow_missing_init_opts: False\n",
            "07:06:05 |     area_under_curve_class: None\n",
            "07:06:05 |     area_under_curve_digits: -1\n",
            "07:06:05 |     attention_dropout: 0.0\n",
            "07:06:05 |     batchsize: 12\n",
            "07:06:05 |     beam_block_full_context: True\n",
            "07:06:05 |     beam_block_list_filename: None\n",
            "07:06:05 |     beam_block_ngram: -1\n",
            "07:06:05 |     beam_context_block_ngram: -1\n",
            "07:06:05 |     beam_delay: 30\n",
            "07:06:05 |     beam_length_penalty: 0.65\n",
            "07:06:05 |     beam_min_length: 1\n",
            "07:06:05 |     beam_size: 1\n",
            "07:06:05 |     betas: '[0.9, 0.999]'\n",
            "07:06:05 |     bpe_add_prefix_space: None\n",
            "07:06:05 |     bpe_debug: False\n",
            "07:06:05 |     bpe_dropout: None\n",
            "07:06:05 |     bpe_merge: None\n",
            "07:06:05 |     bpe_vocab: None\n",
            "07:06:05 |     checkpoint_activations: False\n",
            "07:06:05 |     compute_tokenized_bleu: False\n",
            "07:06:05 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
            "07:06:05 |     datatype: test\n",
            "07:06:05 |     delimiter: '\\n'\n",
            "07:06:05 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "07:06:05 |     dict_endtoken: __end__\n",
            "07:06:05 |     dict_file: from_pretrained/model2.dict\n",
            "07:06:05 |     dict_include_test: False\n",
            "07:06:05 |     dict_include_valid: False\n",
            "07:06:05 |     dict_initpath: None\n",
            "07:06:05 |     dict_language: english\n",
            "07:06:05 |     dict_loaded: True\n",
            "07:06:05 |     dict_lower: True\n",
            "07:06:05 |     dict_max_ngram_size: -1\n",
            "07:06:05 |     dict_maxexs: -1\n",
            "07:06:05 |     dict_maxtokens: -1\n",
            "07:06:05 |     dict_minfreq: 0\n",
            "07:06:05 |     dict_nulltoken: __null__\n",
            "07:06:05 |     dict_starttoken: __start__\n",
            "07:06:05 |     dict_textfields: text,labels\n",
            "07:06:05 |     dict_tokenizer: bpe\n",
            "07:06:05 |     dict_unktoken: __unk__\n",
            "07:06:05 |     display_examples: False\n",
            "07:06:05 |     download_path: None\n",
            "07:06:05 |     dropout: 0.0\n",
            "07:06:05 |     dynamic_batching: full\n",
            "07:06:05 |     embedding_projection: random\n",
            "07:06:05 |     embedding_size: 512\n",
            "07:06:05 |     embedding_type: random\n",
            "07:06:05 |     embeddings_scale: True\n",
            "07:06:05 |     eval_batchsize: None\n",
            "07:06:05 |     eval_dynamic_batching: None\n",
            "07:06:05 |     evaltask: None\n",
            "07:06:05 |     ffn_size: 2048\n",
            "07:06:05 |     final_extra_opt: \n",
            "07:06:05 |     force_fp16_tokens: True\n",
            "07:06:05 |     fp16: True\n",
            "07:06:05 |     fp16_impl: mem_efficient\n",
            "07:06:05 |     gpu: -1\n",
            "07:06:05 |     gradient_clip: 0.1\n",
            "07:06:05 |     hide_labels: False\n",
            "07:06:05 |     history_add_global_end_token: None\n",
            "07:06:05 |     history_reversed: False\n",
            "07:06:05 |     history_size: -1\n",
            "07:06:05 |     image_cropsize: 224\n",
            "07:06:05 |     image_mode: raw\n",
            "07:06:05 |     image_size: 256\n",
            "07:06:05 |     inference: greedy\n",
            "07:06:05 |     init_model: /usr/local/lib/python3.8/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "07:06:05 |     init_opt: None\n",
            "07:06:05 |     interactive_mode: False\n",
            "07:06:05 |     invsqrt_lr_decay_gamma: -1\n",
            "07:06:05 |     is_debug: False\n",
            "07:06:05 |     label_truncate: 128\n",
            "07:06:05 |     learn_positional_embeddings: True\n",
            "07:06:05 |     learningrate: 1e-05\n",
            "07:06:05 |     log_every_n_secs: -1\n",
            "07:06:05 |     log_every_n_steps: 50\n",
            "07:06:05 |     log_keep_fields: all\n",
            "07:06:05 |     loglevel: info\n",
            "07:06:05 |     lr_scheduler: reduceonplateau\n",
            "07:06:05 |     lr_scheduler_decay: 0.5\n",
            "07:06:05 |     lr_scheduler_patience: 3\n",
            "07:06:05 |     max_train_steps: -1\n",
            "07:06:05 |     max_train_time: 60.0\n",
            "07:06:05 |     metrics: bleu,ppl\n",
            "07:06:05 |     model: transformer/generator\n",
            "07:06:05 |     model_file: from_pretrained/model2\n",
            "07:06:05 |     model_parallel: False\n",
            "07:06:05 |     momentum: 0\n",
            "07:06:05 |     multitask_weights: [1]\n",
            "07:06:05 |     mutators: None\n",
            "07:06:05 |     n_decoder_layers: -1\n",
            "07:06:05 |     n_encoder_layers: -1\n",
            "07:06:05 |     n_heads: 16\n",
            "07:06:05 |     n_layers: 8\n",
            "07:06:05 |     n_positions: 512\n",
            "07:06:05 |     n_segments: 0\n",
            "07:06:05 |     nesterov: True\n",
            "07:06:05 |     no_cuda: False\n",
            "07:06:05 |     num_epochs: -1\n",
            "07:06:05 |     num_examples: 2\n",
            "07:06:05 |     num_workers: 0\n",
            "07:06:05 |     nus: [0.7]\n",
            "07:06:05 |     optimizer: mem_eff_adam\n",
            "07:06:05 |     output_scaling: 1.0\n",
            "07:06:05 |     override: \"{'datatype': 'test', 'task': 'empathetic_dialogues', 'model_file': 'from_pretrained/model2', 'num_examples': 2, 'skip_generation': False, 'metrics': 'bleu,ppl'}\"\n",
            "07:06:05 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
            "07:06:05 |     person_tokens: False\n",
            "07:06:05 |     rank_candidates: False\n",
            "07:06:05 |     relu_dropout: 0.0\n",
            "07:06:05 |     report_filename: \n",
            "07:06:05 |     save_after_valid: False\n",
            "07:06:05 |     save_every_n_secs: -1\n",
            "07:06:05 |     save_format: conversations\n",
            "07:06:05 |     share_word_embeddings: True\n",
            "07:06:05 |     short_final_eval: False\n",
            "07:06:05 |     skip_generation: False\n",
            "07:06:05 |     special_tok_lst: None\n",
            "07:06:05 |     split_lines: False\n",
            "07:06:05 |     starttime: Jan12_05-41\n",
            "07:06:05 |     task: empathetic_dialogues\n",
            "07:06:05 |     temperature: 1.0\n",
            "07:06:05 |     tensorboard_log: False\n",
            "07:06:05 |     tensorboard_logdir: None\n",
            "07:06:05 |     text_truncate: 512\n",
            "07:06:05 |     topk: 10\n",
            "07:06:05 |     topp: 0.9\n",
            "07:06:05 |     train_experiencer_only: False\n",
            "07:06:05 |     truncate: -1\n",
            "07:06:05 |     update_freq: 1\n",
            "07:06:05 |     use_reply: label\n",
            "07:06:05 |     validation_cutoff: 1.0\n",
            "07:06:05 |     validation_every_n_epochs: 0.25\n",
            "07:06:05 |     validation_every_n_secs: -1\n",
            "07:06:05 |     validation_every_n_steps: -1\n",
            "07:06:05 |     validation_max_exs: -1\n",
            "07:06:05 |     validation_metric: ppl\n",
            "07:06:05 |     validation_metric_mode: None\n",
            "07:06:05 |     validation_patience: 10\n",
            "07:06:05 |     validation_share_agent: False\n",
            "07:06:05 |     variant: xlm\n",
            "07:06:05 |     verbose: False\n",
            "07:06:05 |     wandb_entity: None\n",
            "07:06:05 |     wandb_log: False\n",
            "07:06:05 |     wandb_name: None\n",
            "07:06:05 |     wandb_project: None\n",
            "07:06:05 |     warmup_rate: 0.0001\n",
            "07:06:05 |     warmup_updates: 100\n",
            "07:06:05 |     weight_decay: None\n",
            "07:06:05 |     world_logs: \n",
            "07:06:06 | Evaluating task empathetic_dialogues using datatype test.\n",
            "07:06:06 | creating task(s): empathetic_dialogues\n",
            "07:06:07 | \u001b[1mReport for empathetic_dialogues:\n",
            "    accuracy  bleu-1    bleu-2    bleu-3    bleu-4  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  gen_n_toks  gpu_mem  \\\n",
            "           0  .08013 8.355e-08 8.734e-10 9.158e-11    56    56   252       0          0 4.499    2 .1005        18.5    .2944   \n",
            "    llen  loss  ltpb  ltps  ltrunc  ltrunclen  ppl  token_acc  token_em  tpb   tps  \n",
            "      11 1.866    11 49.51       0          0 6.46      .6818         0   67 301.6\u001b[0m\n",
            "07:06:07 | Finished evaluating tasks ['empathetic_dialogues'] using datatype test\n",
            "    accuracy  bleu-1    bleu-2    bleu-3    bleu-4  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  gen_n_toks  gpu_mem  \\\n",
            "           0  .08013 8.355e-08 8.734e-10 9.158e-11    56    56   252       0          0 4.499    2 .1005        18.5    .2944   \n",
            "    llen  loss  ltpb  ltps  ltrunc  ltrunclen  ppl  token_acc  token_em  tpb   tps  \n",
            "      11 1.866    11 49.51       0          0 6.46      .6818         0   67 301.6\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'exs': SumMetric(2),\n",
              " 'accuracy': ExactMatchMetric(0),\n",
              " 'f1': F1Metric(0.1005),\n",
              " 'bleu-1': BleuMetric(0.08013),\n",
              " 'bleu-2': BleuMetric(8.355e-08),\n",
              " 'bleu-3': BleuMetric(8.734e-10),\n",
              " 'bleu-4': BleuMetric(9.158e-11),\n",
              " 'clen': AverageMetric(56),\n",
              " 'ctrunc': AverageMetric(0),\n",
              " 'ctrunclen': AverageMetric(0),\n",
              " 'llen': AverageMetric(11),\n",
              " 'ltrunc': AverageMetric(0),\n",
              " 'ltrunclen': AverageMetric(0),\n",
              " 'loss': AverageMetric(1.866),\n",
              " 'ppl': PPLMetric(6.46),\n",
              " 'token_acc': AverageMetric(0.6818),\n",
              " 'token_em': AverageMetric(0),\n",
              " 'gen_n_toks': AverageMetric(18.5),\n",
              " 'exps': GlobalTimerMetric(4.499),\n",
              " 'ltpb': GlobalAverageMetric(11),\n",
              " 'ltps': GlobalTimerMetric(49.51),\n",
              " 'ctpb': GlobalAverageMetric(56),\n",
              " 'ctps': GlobalTimerMetric(252),\n",
              " 'tpb': GlobalAverageMetric(67),\n",
              " 'tps': GlobalTimerMetric(301.6),\n",
              " 'gpu_mem': GlobalAverageMetric(0.2944)}"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJj7Lhs00oOB"
      },
      "source": [
        "# 나만의 모델 만들기\n",
        "\n",
        "매우 간단한 모델을 만들어보도록 하겠습니다. 이 모델은 \"hello X, my name is Y\"라고 대답하며 X는 input에 따라 달라집니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pykhtFDrxCPo"
      },
      "source": [
        "from parlai.core.agents import register_agent, Agent\n",
        "\n",
        "@register_agent(\"hello\")\n",
        "class HelloAgent(Agent):\n",
        "    @classmethod\n",
        "    def add_cmdline_args(cls, parser, partial_opt):\n",
        "        parser.add_argument('--name', type=str, default='Alice', help=\"The agent's name.\")\n",
        "        return parser\n",
        "        \n",
        "    def __init__(self, opt, shared=None):\n",
        "        # 아까 데이터셋을 만들때와 비슷하게 opt와 shared memory가 존재합니다\n",
        "        super().__init__(opt, shared)\n",
        "        self.id = 'HelloAgent'\n",
        "        self.name = opt['name']\n",
        "    \n",
        "    def observe(self, observation):\n",
        "        # 유저의 input에서 마지막 단어를 가져옵니다\n",
        "        words = observation.get('text', '').split()\n",
        "        if words:\n",
        "            self.last_word = words[-1]\n",
        "        else:\n",
        "            self.last_word = \"stranger!\"\n",
        "    \n",
        "    def act(self):\n",
        "        # 항상 이와 같은 문자열을 return하도록 합니다\n",
        "        return {\n",
        "            'id': self.id,\n",
        "            'text': f\"Hello {self.last_word}, I'm {self.name}\",\n",
        "        }"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1SZmy_s0sGd"
      },
      "source": [
        "이 모델이 어떻게 작동하는지 살펴봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcS1UIFH0pb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6443a9ff-b79d-48c6-ad44-225884c323d8"
      },
      "source": [
        "DisplayModel.main(task='my_teacher', model='hello')"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "07:06:21 | creating task(s): my_teacher\n",
            " ~~ Loading from valid.txt ~~ \n",
            "07:06:21 | Opt:\n",
            "07:06:21 |     allow_missing_init_opts: False\n",
            "07:06:21 |     batchsize: 1\n",
            "07:06:21 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
            "07:06:21 |     datatype: valid\n",
            "07:06:21 |     dict_class: None\n",
            "07:06:21 |     display_add_fields: \n",
            "07:06:21 |     download_path: None\n",
            "07:06:21 |     dynamic_batching: None\n",
            "07:06:21 |     hide_labels: False\n",
            "07:06:21 |     image_cropsize: 224\n",
            "07:06:21 |     image_mode: raw\n",
            "07:06:21 |     image_size: 256\n",
            "07:06:21 |     init_model: None\n",
            "07:06:21 |     init_opt: None\n",
            "07:06:21 |     is_debug: False\n",
            "07:06:21 |     loglevel: info\n",
            "07:06:21 |     model: hello\n",
            "07:06:21 |     model_file: None\n",
            "07:06:21 |     multitask_weights: [1]\n",
            "07:06:21 |     mutators: None\n",
            "07:06:21 |     name: Alice\n",
            "07:06:21 |     num_examples: 10\n",
            "07:06:21 |     override: \"{'task': 'my_teacher', 'model': 'hello'}\"\n",
            "07:06:21 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
            "07:06:21 |     starttime: Jan12_07-06\n",
            "07:06:21 |     task: my_teacher\n",
            "07:06:21 |     verbose: False\n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
            "\u001b[0mHello\u001b[0;0m\n",
            "\u001b[1;94m    labels: Hi\u001b[0;0m\n",
            "\u001b[0;95m     model: Hello Hello, I'm Alice\u001b[0;0m\n",
            "\u001b[0mHow are you\u001b[0;0m\n",
            "\u001b[1;94m    labels: I am fine\u001b[0;0m\n",
            "\u001b[0;95m     model: Hello you, I'm Alice\u001b[0;0m\n",
            "\u001b[0mLet's say goodbye\u001b[0;0m\n",
            "\u001b[1;94m    labels: Goodbye!\u001b[0;0m\n",
            "\u001b[0;95m     model: Hello goodbye, I'm Alice\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
            "\u001b[0mHey\u001b[0;0m\n",
            "\u001b[1;94m    labels: hi there\u001b[0;0m\n",
            "\u001b[0;95m     model: Hello Hey, I'm Alice\u001b[0;0m\n",
            "\u001b[0mDeja vu?\u001b[0;0m\n",
            "\u001b[1;94m    labels: Deja vu!\u001b[0;0m\n",
            "\u001b[0;95m     model: Hello vu?, I'm Alice\u001b[0;0m\n",
            "\u001b[0mLast chance\u001b[0;0m\n",
            "\u001b[1;94m    labels: This is it\u001b[0;0m\n",
            "\u001b[0;95m     model: Hello chance, I'm Alice\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
            "\u001b[0mHello\u001b[0;0m\n",
            "\u001b[1;94m    labels: Hi\u001b[0;0m\n",
            "\u001b[0;95m     model: Hello Hello, I'm Alice\u001b[0;0m\n",
            "\u001b[0mWhere are you now?\u001b[0;0m\n",
            "\u001b[1;94m    labels: I'm in yonsei unversity\u001b[0;0m\n",
            "\u001b[0;95m     model: Hello now?, I'm Alice\u001b[0;0m\n",
            "\u001b[0mOkay bye\u001b[0;0m\n",
            "\u001b[1;94m    labels: Bye\u001b[0;0m\n",
            "\u001b[0;95m     model: Hello bye, I'm Alice\u001b[0;0m\n",
            "07:06:21 | epoch done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmvcRSGS0wQE"
      },
      "source": [
        "이 모델이 어떻게 유저가 보내는 말을 읽고, 본인의 이름을 이야기하는지 눈치채셨나요? 이제 모델에게 이름을 붙여주고 대화해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xd5CaG00tv6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb7e5066-b377-4a82-e95f-f77530f6fac2"
      },
      "source": [
        "Interactive.main(model='hello', name='Bob')"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "07:06:26 | Opt:\n",
            "07:06:26 |     allow_missing_init_opts: False\n",
            "07:06:26 |     batchsize: 1\n",
            "07:06:26 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
            "07:06:26 |     datatype: train\n",
            "07:06:26 |     dict_class: None\n",
            "07:06:26 |     display_add_fields: \n",
            "07:06:26 |     display_examples: False\n",
            "07:06:26 |     display_prettify: False\n",
            "07:06:26 |     download_path: None\n",
            "07:06:26 |     dynamic_batching: None\n",
            "07:06:26 |     hide_labels: False\n",
            "07:06:26 |     image_cropsize: 224\n",
            "07:06:26 |     image_mode: raw\n",
            "07:06:26 |     image_size: 256\n",
            "07:06:26 |     init_model: None\n",
            "07:06:26 |     init_opt: None\n",
            "07:06:26 |     interactive_mode: True\n",
            "07:06:26 |     interactive_task: True\n",
            "07:06:26 |     is_debug: False\n",
            "07:06:26 |     local_human_candidates_file: None\n",
            "07:06:26 |     log_keep_fields: all\n",
            "07:06:26 |     loglevel: info\n",
            "07:06:26 |     model: hello\n",
            "07:06:26 |     model_file: None\n",
            "07:06:26 |     multitask_weights: [1]\n",
            "07:06:26 |     name: Bob\n",
            "07:06:26 |     outfile: \n",
            "07:06:26 |     override: \"{'model': 'hello', 'name': 'Bob'}\"\n",
            "07:06:26 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
            "07:06:26 |     save_format: conversations\n",
            "07:06:26 |     single_turn: False\n",
            "07:06:26 |     starttime: Jan12_07-06\n",
            "07:06:26 |     task: interactive\n",
            "07:06:26 |     verbose: False\n",
            "\u001b[1;31mEnter [DONE] if you want to end the episode, [EXIT] to quit.\u001b[0;0m\n",
            "07:06:26 | creating task(s): interactive\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m hi\n",
            "\u001b[0;34m[HelloAgent]:\u001b[0;0m \u001b[1mHello hi, I'm Bob\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m how are you?\n",
            "\u001b[0;34m[HelloAgent]:\u001b[0;0m \u001b[1mHello you?, I'm Bob\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m hi, i'm jim\n",
            "\u001b[0;34m[HelloAgent]:\u001b[0;0m \u001b[1mHello jim, I'm Bob\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m how are you?\n",
            "\u001b[0;34m[HelloAgent]:\u001b[0;0m \u001b[1mHello you?, I'm Bob\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m [EXIT]\n",
            "CHAT DONE \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✨실습✨\n",
        "\n",
        "- 유저 Input의 첫번째 단어를 이름으로 인식하는 모델을 만들어보세요"
      ],
      "metadata": {
        "id": "cJj6guDR62-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from parlai.core.agents import register_agent, Agent\n",
        "\n",
        "@register_agent(\"hello\")\n",
        "class HelloAgent(Agent):\n",
        "    @classmethod\n",
        "    def add_cmdline_args(cls, parser, partial_opt):\n",
        "        parser.add_argument('--name', type=str, default='Alice', help=\"The agent's name.\")\n",
        "        return parser\n",
        "        \n",
        "    def __init__(self, opt, shared=None):\n",
        "        # 아까 데이터셋을 만들때와 비슷하게 opt와 shared memory가 존재합니다\n",
        "        super().__init__(opt, shared)\n",
        "        self.id = 'HelloAgent'\n",
        "        self.name = opt['name']\n",
        "    \n",
        "    def observe(self, observation):\n",
        "        # 유저의 input에서 마지막 단어를 가져옵니다\n",
        "        words = observation.get('text', '').split()\n",
        "        if words:\n",
        "            self.last_word = words[-1]\n",
        "        else:\n",
        "            self.last_word = \"stranger!\"\n",
        "    \n",
        "    def act(self):\n",
        "        # 항상 이와 같은 문자열을 return하도록 합니다\n",
        "        return {\n",
        "            'id': self.id,\n",
        "            'text': f\"Hello {self.last_word}, I'm {self.name}\",\n",
        "        }"
      ],
      "metadata": {
        "id": "wl14zrO57MpR"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aBbhKTO1DEE"
      },
      "source": [
        "## 심화 - 나만의 뉴럴 네트워크 모델 만들기\n",
        "\n",
        "위에서 만들었던 모델은 굉장히 단순하고, 조금의 동작밖에 할 수 없습니다. 더 많은 동작을 할 수 있는 모델을 만들기 위해 본인만의 뉴럴 네트워크 모델을 만들어봅시다. 아래의 모델 코드는 ParlAI에서 제공하는 [ExampleSeq2Seq](https://github.com/facebookresearch/ParlAI/blob/master/parlai/agents/examples/seq2seq.py) 코드입니다. 단순한 RNN 모델이며 기계 번역(Machine Translation) 모델과 같이 학습됩니다. 모델에 대한 설명은 너무 길기 때문에 [TorchGeneratorAgent tutorial](https://parl.ai/docs/tutorial_torch_generator_agent.html) 문서를 참고해주시길 바랍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVrZh-T903wh"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import parlai.core.torch_generator_agent as tga\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Example encoder, consisting of an embedding layer and a 1-layer LSTM with the\n",
        "    specified hidden size.\n",
        "    Pay particular attention to the ``forward`` output.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embeddings, hidden_size):\n",
        "        \"\"\"\n",
        "        Initialization.\n",
        "        Arguments here can be used to provide hyperparameters.\n",
        "        \"\"\"\n",
        "        # ParlAI에서 제공하는 nn.Modules를 반드시 super로 호출 해야합니다\n",
        "        super().__init__()\n",
        "\n",
        "        self.embeddings = embeddings\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=hidden_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=1,\n",
        "            batch_first=True,\n",
        "        )\n",
        "\n",
        "    def forward(self, input_tokens):\n",
        "        \"\"\"\n",
        "        Perform the forward pass for the encoder.\n",
        "        Input *must* be input_tokens, which are the context tokens given\n",
        "        as a matrix of lookup IDs.\n",
        "        :param input_tokens:\n",
        "            Input tokens as a bsz x seqlen LongTensor.\n",
        "            Likely will contain padding.\n",
        "        :return:\n",
        "            You can return anything you like; it is will be passed verbatim\n",
        "            into the decoder for conditioning. However, it should be something\n",
        "            you can easily manipulate in ``reorder_encoder_states``.\n",
        "            This particular implementation returns the hidden and cell states from the\n",
        "            LSTM.\n",
        "        \"\"\"\n",
        "        embedded = self.embeddings(input_tokens)\n",
        "        _output, hidden = self.lstm(embedded)\n",
        "        return hidden\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Basic example decoder, consisting of an embedding layer and a 1-layer LSTM with the\n",
        "    specified hidden size. Decoder allows for incremental decoding by ingesting the\n",
        "    current incremental state on each forward pass.\n",
        "    Pay particular note to the ``forward``.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embeddings, hidden_size):\n",
        "        \"\"\"\n",
        "        Initialization.\n",
        "        Arguments here can be used to provide hyperparameters.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.embeddings = embeddings\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=hidden_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=1,\n",
        "            batch_first=True,\n",
        "        )\n",
        "\n",
        "    def forward(self, input, encoder_state, incr_state=None):\n",
        "        \"\"\"\n",
        "        Run forward pass.\n",
        "        :param input:\n",
        "            The currently generated tokens from the decoder.\n",
        "        :param encoder_state:\n",
        "            The output from the encoder module.\n",
        "        :parm incr_state:\n",
        "            The previous hidden state of the decoder.\n",
        "        \"\"\"\n",
        "        embedded = self.embeddings(input)\n",
        "        if incr_state is None:\n",
        "            # 첫번째로 불러올 때에는 decoder에 아무런 값이 없으므로 \n",
        "            # encoder의 값을 넣어주어 decoder 값을 설정해줍니다\n",
        "            state = encoder_state\n",
        "        else:\n",
        "            # 값이 존재하는 경우에는 그대로 사용합니다\n",
        "            state = incr_state\n",
        "\n",
        "        # 새로운 output과 decoder의 incremental state를 받습니다\n",
        "        output, incr_state = self.lstm(embedded, state)\n",
        "\n",
        "        return output, incr_state\n",
        "\n",
        "\n",
        "class ExampleModel(tga.TorchGeneratorModel):\n",
        "    \"\"\"\n",
        "    ExampleModel implements the abstract methods of TorchGeneratorModel to define how to\n",
        "    re-order encoder states and decoder incremental states.\n",
        "    It also instantiates the embedding table, encoder, and decoder, and defines the\n",
        "    final output layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dictionary, hidden_size=1024):\n",
        "        super().__init__(\n",
        "            padding_idx=dictionary[dictionary.null_token],\n",
        "            start_idx=dictionary[dictionary.start_token],\n",
        "            end_idx=dictionary[dictionary.end_token],\n",
        "            unknown_idx=dictionary[dictionary.unk_token],\n",
        "        )\n",
        "        self.embeddings = nn.Embedding(len(dictionary), hidden_size)\n",
        "        self.encoder = Encoder(self.embeddings, hidden_size)\n",
        "        self.decoder = Decoder(self.embeddings, hidden_size)\n",
        "\n",
        "    def output(self, decoder_output):\n",
        "        \"\"\"\n",
        "        Perform the final output -> logits transformation.\n",
        "        \"\"\"\n",
        "        return F.linear(decoder_output, self.embeddings.weight)\n",
        "\n",
        "    def reorder_encoder_states(self, encoder_states, indices):\n",
        "        \"\"\"\n",
        "        Reorder the encoder states to select only the given batch indices.\n",
        "        Since encoder_state can be arbitrary, you must implement this yourself.\n",
        "        Typically you will just want to index select on the batch dimension.\n",
        "        \"\"\"\n",
        "        h, c = encoder_states\n",
        "        return h[:, indices, :], c[:, indices, :]\n",
        "\n",
        "    def reorder_decoder_incremental_state(self, incr_state, indices):\n",
        "        \"\"\"\n",
        "        Reorder the decoder states to select only the given batch indices.\n",
        "        This method can be a stub which always returns None; this will result in the\n",
        "        decoder doing a complete forward pass for every single token, making generation\n",
        "        O(n^2). However, if any state can be cached, then this method should be\n",
        "        implemented to reduce the generation complexity to O(n).\n",
        "        \"\"\"\n",
        "        h, c = incr_state\n",
        "        return h[:, indices, :], c[:, indices, :]\n",
        "\n",
        "\n",
        "@register_agent(\"my_first_lstm\")\n",
        "class Seq2seqAgent(tga.TorchGeneratorAgent):\n",
        "    \"\"\"\n",
        "    Example agent.\n",
        "    Implements the interface for TorchGeneratorAgent. The minimum requirement is that it\n",
        "    implements ``build_model``, but we will want to include additional command line\n",
        "    parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def add_cmdline_args(cls, argparser, partial_opt):\n",
        "        \"\"\"\n",
        "        Add CLI arguments.\n",
        "        \"\"\"\n",
        "        # TorchGeneratorAgent의 argument들을 추가해줍니다\n",
        "        super().add_cmdline_args(argparser)\n",
        "\n",
        "        # 이 모델만을 위한 argument도 추가합니다\n",
        "        group = argparser.add_argument_group('Example TGA Agent')\n",
        "        group.add_argument(\n",
        "            '-hid', '--hidden-size', type=int, default=1024, help='Hidden size.'\n",
        "        )\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"\n",
        "        Construct the model.\n",
        "        \"\"\"\n",
        "\n",
        "        model = ExampleModel(self.dict, self.opt['hidden_size'])\n",
        "        # 추가적으로 원한다면 pre-trained embedding을 원하는 source에서 복사해와서 initialize할 수 있습니다\n",
        "        # example: GloVe, fastText, etc.\n",
        "        self._copy_embeddings(model.embeddings.weight, self.opt['embedding_type'])\n",
        "        return model"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfR9w_Hm1HHY"
      },
      "source": [
        "이제 새로운 뉴럴 네트워크 모델을 학습할 준비가 되었습니다.\n",
        "앞에서 만들었던 toy task에 학습시켜봅시다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJMXpogz1E-_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2ae7f9b-ef86-4cdc-9980-44f3435d0f13"
      },
      "source": [
        "!rm -rf my_first_lstm\n",
        "!mkdir -p my_first_lstm\n",
        "\n",
        "TrainModel.main(\n",
        "    model='my_first_lstm',\n",
        "    model_file='my_first_lstm/model',\n",
        "    task='my_teacher',\n",
        "    batchsize=1,\n",
        "    validation_every_n_secs=10,\n",
        "    max_train_time=60,\n",
        ")"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "07:07:27 | building dictionary first...\n",
            "07:07:27 | Opt:\n",
            "07:07:27 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "07:07:27 |     adam_eps: 1e-08\n",
            "07:07:27 |     add_p1_after_newln: False\n",
            "07:07:27 |     aggregate_micro: False\n",
            "07:07:27 |     allow_missing_init_opts: False\n",
            "07:07:27 |     batchsize: 1\n",
            "07:07:27 |     beam_block_full_context: True\n",
            "07:07:27 |     beam_block_list_filename: None\n",
            "07:07:27 |     beam_block_ngram: -1\n",
            "07:07:27 |     beam_context_block_ngram: -1\n",
            "07:07:27 |     beam_delay: 30\n",
            "07:07:27 |     beam_length_penalty: 0.65\n",
            "07:07:27 |     beam_min_length: 1\n",
            "07:07:27 |     beam_size: 1\n",
            "07:07:27 |     betas: '(0.9, 0.999)'\n",
            "07:07:27 |     bpe_add_prefix_space: None\n",
            "07:07:27 |     bpe_debug: False\n",
            "07:07:27 |     bpe_dropout: None\n",
            "07:07:27 |     bpe_merge: None\n",
            "07:07:27 |     bpe_vocab: None\n",
            "07:07:27 |     compute_tokenized_bleu: False\n",
            "07:07:27 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
            "07:07:27 |     datatype: train\n",
            "07:07:27 |     delimiter: '\\n'\n",
            "07:07:27 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "07:07:27 |     dict_endtoken: __end__\n",
            "07:07:27 |     dict_file: my_first_lstm/model.dict\n",
            "07:07:27 |     dict_include_test: False\n",
            "07:07:27 |     dict_include_valid: False\n",
            "07:07:27 |     dict_initpath: None\n",
            "07:07:27 |     dict_language: english\n",
            "07:07:27 |     dict_loaded: False\n",
            "07:07:27 |     dict_lower: False\n",
            "07:07:27 |     dict_max_ngram_size: -1\n",
            "07:07:27 |     dict_maxexs: -1\n",
            "07:07:27 |     dict_maxtokens: -1\n",
            "07:07:27 |     dict_minfreq: 0\n",
            "07:07:27 |     dict_nulltoken: __null__\n",
            "07:07:27 |     dict_starttoken: __start__\n",
            "07:07:27 |     dict_textfields: text,labels\n",
            "07:07:27 |     dict_tokenizer: re\n",
            "07:07:27 |     dict_unktoken: __unk__\n",
            "07:07:27 |     display_examples: False\n",
            "07:07:27 |     download_path: None\n",
            "07:07:27 |     dynamic_batching: None\n",
            "07:07:27 |     embedding_projection: random\n",
            "07:07:27 |     embedding_type: random\n",
            "07:07:27 |     eval_batchsize: None\n",
            "07:07:27 |     eval_dynamic_batching: None\n",
            "07:07:27 |     evaltask: None\n",
            "07:07:27 |     final_extra_opt: \n",
            "07:07:27 |     force_fp16_tokens: False\n",
            "07:07:27 |     fp16: False\n",
            "07:07:27 |     fp16_impl: safe\n",
            "07:07:27 |     gpu: -1\n",
            "07:07:27 |     gradient_clip: 0.1\n",
            "07:07:27 |     hidden_size: 1024\n",
            "07:07:27 |     hide_labels: False\n",
            "07:07:27 |     history_add_global_end_token: None\n",
            "07:07:27 |     history_reversed: False\n",
            "07:07:27 |     history_size: -1\n",
            "07:07:27 |     image_cropsize: 224\n",
            "07:07:27 |     image_mode: no_image_model\n",
            "07:07:27 |     image_size: 256\n",
            "07:07:27 |     inference: greedy\n",
            "07:07:27 |     init_model: None\n",
            "07:07:27 |     init_opt: None\n",
            "07:07:27 |     interactive_mode: False\n",
            "07:07:27 |     invsqrt_lr_decay_gamma: -1\n",
            "07:07:27 |     is_debug: False\n",
            "07:07:27 |     label_truncate: None\n",
            "07:07:27 |     learningrate: 1\n",
            "07:07:27 |     load_from_checkpoint: True\n",
            "07:07:27 |     log_every_n_secs: -1\n",
            "07:07:27 |     log_every_n_steps: 50\n",
            "07:07:27 |     log_keep_fields: all\n",
            "07:07:27 |     loglevel: info\n",
            "07:07:27 |     lr_scheduler: reduceonplateau\n",
            "07:07:27 |     lr_scheduler_decay: 0.5\n",
            "07:07:27 |     lr_scheduler_patience: 3\n",
            "07:07:27 |     max_train_steps: -1\n",
            "07:07:27 |     max_train_time: 60.0\n",
            "07:07:27 |     metrics: default\n",
            "07:07:27 |     model: my_first_lstm\n",
            "07:07:27 |     model_file: my_first_lstm/model\n",
            "07:07:27 |     momentum: 0\n",
            "07:07:27 |     multitask_weights: [1]\n",
            "07:07:27 |     mutators: None\n",
            "07:07:27 |     nesterov: True\n",
            "07:07:27 |     no_cuda: False\n",
            "07:07:27 |     num_epochs: -1\n",
            "07:07:27 |     num_workers: 0\n",
            "07:07:27 |     nus: (0.7,)\n",
            "07:07:27 |     optimizer: sgd\n",
            "07:07:27 |     override: \"{'model': 'my_first_lstm', 'model_file': 'my_first_lstm/model', 'task': 'my_teacher', 'batchsize': 1, 'validation_every_n_secs': 10.0, 'max_train_time': 60.0}\"\n",
            "07:07:27 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
            "07:07:27 |     person_tokens: False\n",
            "07:07:27 |     rank_candidates: False\n",
            "07:07:27 |     save_after_valid: False\n",
            "07:07:27 |     save_every_n_secs: -1\n",
            "07:07:27 |     save_format: conversations\n",
            "07:07:27 |     short_final_eval: False\n",
            "07:07:27 |     skip_generation: False\n",
            "07:07:27 |     special_tok_lst: None\n",
            "07:07:27 |     split_lines: False\n",
            "07:07:27 |     starttime: Jan12_07-07\n",
            "07:07:27 |     task: my_teacher\n",
            "07:07:27 |     temperature: 1.0\n",
            "07:07:27 |     tensorboard_log: False\n",
            "07:07:27 |     tensorboard_logdir: None\n",
            "07:07:27 |     text_truncate: None\n",
            "07:07:27 |     topk: 10\n",
            "07:07:27 |     topp: 0.9\n",
            "07:07:27 |     truncate: -1\n",
            "07:07:27 |     update_freq: 1\n",
            "07:07:27 |     use_reply: label\n",
            "07:07:27 |     validation_cutoff: 1.0\n",
            "07:07:27 |     validation_every_n_epochs: -1\n",
            "07:07:27 |     validation_every_n_secs: 10.0\n",
            "07:07:27 |     validation_every_n_steps: -1\n",
            "07:07:27 |     validation_max_exs: -1\n",
            "07:07:27 |     validation_metric: accuracy\n",
            "07:07:27 |     validation_metric_mode: None\n",
            "07:07:27 |     validation_patience: 10\n",
            "07:07:27 |     validation_share_agent: False\n",
            "07:07:27 |     verbose: False\n",
            "07:07:27 |     wandb_entity: None\n",
            "07:07:27 |     wandb_log: False\n",
            "07:07:27 |     wandb_name: None\n",
            "07:07:27 |     wandb_project: None\n",
            "07:07:27 |     warmup_rate: 0.0001\n",
            "07:07:27 |     warmup_updates: -1\n",
            "07:07:27 |     weight_decay: None\n",
            "07:07:27 |     world_logs: \n",
            "07:07:28 | creating task(s): my_teacher\n",
            " ~~ Loading from train.txt ~~ \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building dictionary: 100%|██████████| 9.00/9.00 [00:00<00:00, 4.08kex/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ~~ Loading from train.txt ~~ \n",
            "07:07:28 | Saving dictionary to my_first_lstm/model.dict\n",
            "07:07:28 | dictionary built with 39 tokens in 0.0s\n",
            "07:07:28 | No model with opt yet at: my_first_lstm/model(.opt)\n",
            "07:07:28 | Using CUDA\n",
            "07:07:28 | loading dictionary from my_first_lstm/model.dict\n",
            "07:07:28 | num words = 39\n",
            "07:07:28 | Total parameters: 16,833,536 (16,833,536 trainable)\n",
            "07:07:28 | Opt:\n",
            "07:07:28 |     adafactor_eps: '(1e-30, 0.001)'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "07:07:28 |     adam_eps: 1e-08\n",
            "07:07:28 |     add_p1_after_newln: False\n",
            "07:07:28 |     aggregate_micro: False\n",
            "07:07:28 |     allow_missing_init_opts: False\n",
            "07:07:28 |     batchsize: 1\n",
            "07:07:28 |     beam_block_full_context: True\n",
            "07:07:28 |     beam_block_list_filename: None\n",
            "07:07:28 |     beam_block_ngram: -1\n",
            "07:07:28 |     beam_context_block_ngram: -1\n",
            "07:07:28 |     beam_delay: 30\n",
            "07:07:28 |     beam_length_penalty: 0.65\n",
            "07:07:28 |     beam_min_length: 1\n",
            "07:07:28 |     beam_size: 1\n",
            "07:07:28 |     betas: '(0.9, 0.999)'\n",
            "07:07:28 |     bpe_add_prefix_space: None\n",
            "07:07:28 |     bpe_debug: False\n",
            "07:07:28 |     bpe_dropout: None\n",
            "07:07:28 |     bpe_merge: None\n",
            "07:07:28 |     bpe_vocab: None\n",
            "07:07:28 |     compute_tokenized_bleu: False\n",
            "07:07:28 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
            "07:07:28 |     datatype: train\n",
            "07:07:28 |     delimiter: '\\n'\n",
            "07:07:28 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "07:07:28 |     dict_endtoken: __end__\n",
            "07:07:28 |     dict_file: my_first_lstm/model.dict\n",
            "07:07:28 |     dict_include_test: False\n",
            "07:07:28 |     dict_include_valid: False\n",
            "07:07:28 |     dict_initpath: None\n",
            "07:07:28 |     dict_language: english\n",
            "07:07:28 |     dict_loaded: True\n",
            "07:07:28 |     dict_lower: False\n",
            "07:07:28 |     dict_max_ngram_size: -1\n",
            "07:07:28 |     dict_maxexs: -1\n",
            "07:07:28 |     dict_maxtokens: -1\n",
            "07:07:28 |     dict_minfreq: 0\n",
            "07:07:28 |     dict_nulltoken: __null__\n",
            "07:07:28 |     dict_starttoken: __start__\n",
            "07:07:28 |     dict_textfields: text,labels\n",
            "07:07:28 |     dict_tokenizer: re\n",
            "07:07:28 |     dict_unktoken: __unk__\n",
            "07:07:28 |     display_examples: False\n",
            "07:07:28 |     download_path: None\n",
            "07:07:28 |     dynamic_batching: None\n",
            "07:07:28 |     embedding_projection: random\n",
            "07:07:28 |     embedding_type: random\n",
            "07:07:28 |     eval_batchsize: None\n",
            "07:07:28 |     eval_dynamic_batching: None\n",
            "07:07:28 |     evaltask: None\n",
            "07:07:28 |     final_extra_opt: \n",
            "07:07:28 |     force_fp16_tokens: False\n",
            "07:07:28 |     fp16: False\n",
            "07:07:28 |     fp16_impl: safe\n",
            "07:07:28 |     gpu: -1\n",
            "07:07:28 |     gradient_clip: 0.1\n",
            "07:07:28 |     hidden_size: 1024\n",
            "07:07:28 |     hide_labels: False\n",
            "07:07:28 |     history_add_global_end_token: None\n",
            "07:07:28 |     history_reversed: False\n",
            "07:07:28 |     history_size: -1\n",
            "07:07:28 |     image_cropsize: 224\n",
            "07:07:28 |     image_mode: raw\n",
            "07:07:28 |     image_size: 256\n",
            "07:07:28 |     inference: greedy\n",
            "07:07:28 |     init_model: None\n",
            "07:07:28 |     init_opt: None\n",
            "07:07:28 |     interactive_mode: False\n",
            "07:07:28 |     invsqrt_lr_decay_gamma: -1\n",
            "07:07:28 |     is_debug: False\n",
            "07:07:28 |     label_truncate: None\n",
            "07:07:28 |     learningrate: 1\n",
            "07:07:28 |     load_from_checkpoint: True\n",
            "07:07:28 |     log_every_n_secs: -1\n",
            "07:07:28 |     log_every_n_steps: 50\n",
            "07:07:28 |     log_keep_fields: all\n",
            "07:07:28 |     loglevel: info\n",
            "07:07:28 |     lr_scheduler: reduceonplateau\n",
            "07:07:28 |     lr_scheduler_decay: 0.5\n",
            "07:07:28 |     lr_scheduler_patience: 3\n",
            "07:07:28 |     max_train_steps: -1\n",
            "07:07:28 |     max_train_time: 60.0\n",
            "07:07:28 |     metrics: default\n",
            "07:07:28 |     model: my_first_lstm\n",
            "07:07:28 |     model_file: my_first_lstm/model\n",
            "07:07:28 |     momentum: 0\n",
            "07:07:28 |     multitask_weights: [1]\n",
            "07:07:28 |     mutators: None\n",
            "07:07:28 |     nesterov: True\n",
            "07:07:28 |     no_cuda: False\n",
            "07:07:28 |     num_epochs: -1\n",
            "07:07:28 |     num_workers: 0\n",
            "07:07:28 |     nus: (0.7,)\n",
            "07:07:28 |     optimizer: sgd\n",
            "07:07:28 |     override: \"{'model': 'my_first_lstm', 'model_file': 'my_first_lstm/model', 'task': 'my_teacher', 'batchsize': 1, 'validation_every_n_secs': 10.0, 'max_train_time': 60.0}\"\n",
            "07:07:28 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
            "07:07:28 |     person_tokens: False\n",
            "07:07:28 |     rank_candidates: False\n",
            "07:07:28 |     save_after_valid: False\n",
            "07:07:28 |     save_every_n_secs: -1\n",
            "07:07:28 |     save_format: conversations\n",
            "07:07:28 |     short_final_eval: False\n",
            "07:07:28 |     skip_generation: False\n",
            "07:07:28 |     special_tok_lst: None\n",
            "07:07:28 |     split_lines: False\n",
            "07:07:28 |     starttime: Jan12_07-07\n",
            "07:07:28 |     task: my_teacher\n",
            "07:07:28 |     temperature: 1.0\n",
            "07:07:28 |     tensorboard_log: False\n",
            "07:07:28 |     tensorboard_logdir: None\n",
            "07:07:28 |     text_truncate: None\n",
            "07:07:28 |     topk: 10\n",
            "07:07:28 |     topp: 0.9\n",
            "07:07:28 |     truncate: -1\n",
            "07:07:28 |     update_freq: 1\n",
            "07:07:28 |     use_reply: label\n",
            "07:07:28 |     validation_cutoff: 1.0\n",
            "07:07:28 |     validation_every_n_epochs: -1\n",
            "07:07:28 |     validation_every_n_secs: 10.0\n",
            "07:07:28 |     validation_every_n_steps: -1\n",
            "07:07:28 |     validation_max_exs: -1\n",
            "07:07:28 |     validation_metric: accuracy\n",
            "07:07:28 |     validation_metric_mode: None\n",
            "07:07:28 |     validation_patience: 10\n",
            "07:07:28 |     validation_share_agent: False\n",
            "07:07:28 |     verbose: False\n",
            "07:07:28 |     wandb_entity: None\n",
            "07:07:28 |     wandb_log: False\n",
            "07:07:28 |     wandb_name: None\n",
            "07:07:28 |     wandb_project: None\n",
            "07:07:28 |     warmup_rate: 0.0001\n",
            "07:07:28 |     warmup_updates: -1\n",
            "07:07:28 |     weight_decay: None\n",
            "07:07:28 |     world_logs: \n",
            "07:07:29 | creating task(s): my_teacher\n",
            " ~~ Loading from train.txt ~~ \n",
            "07:07:29 | training...\n",
            "07:07:29 | time:0s total_exs:50 total_steps:50 epochs:5.56\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "    8.68 .6600  8.68   903       0          0   104   50  69.71   .01428  3.36 2.862   1  3.36 349.5       0          0 17.49   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .6845     .4800                   50 12.04 1253 104.1\n",
            "\n",
            "07:07:30 | time:1s total_exs:100 total_steps:100 epochs:11.11\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen   loss  lr  ltpb  ltps  ltrunc  ltrunclen  \\\n",
            "    8.88 .2000  8.88  1131       0          0 127.4   50  5.915   .01428  3.34 .09246   1  3.34 425.5       0          0   \n",
            "     ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "   1.097      .9820     .9400                  100 12.22 1557 127.5\n",
            "\n",
            "07:07:30 | time:1s total_exs:150 total_steps:150 epochs:16.67\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen   loss  lr  ltpb  ltps  ltrunc  ltrunclen  \\\n",
            "    8.48 .1400  8.48  1099       0          0 129.6   50  3.121   .01428  3.42 .02096   1  3.42 443.2       0          0   \n",
            "     ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "   1.021      .9883     .9600                  150 11.9 1542 129.7\n",
            "\n",
            "07:07:31 | time:2s total_exs:200 total_steps:200 epochs:22.22\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs    gnorm  gpu_mem  llen      loss  lr  ltpb  ltps  ltrunc  ltrunclen  \\\n",
            "    8.66     0  8.66  1105       0          0 127.5   50 .0006291   .01428  3.54 3.625e-06   1  3.54 451.5       0          0   \n",
            "    ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "      1          1         1                  200 12.2 1556 127.6\n",
            "\n",
            "07:07:31 | time:2s total_exs:250 total_steps:250 epochs:27.78\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs    gnorm  gpu_mem  llen      loss  lr  ltpb  ltps  ltrunc  ltrunclen  \\\n",
            "    8.42     0  8.42  1095       0          0   130   50 .0002845   .01428  3.32 1.396e-06   1  3.32 431.6       0          0   \n",
            "    ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "      1          1         1                  250 11.74 1526 130.1\n",
            "\n",
            "07:07:31 | time:3s total_exs:300 total_steps:300 epochs:33.33\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs    gnorm  gpu_mem  llen     loss  lr  ltpb  ltps  ltrunc  ltrunclen  \\\n",
            "    9.04     0  9.04  1158       0          0 128.1   50 .0002807   .01429   3.5 1.53e-06   1   3.5 448.5       0          0   \n",
            "    ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "      1          1         1                  300 12.54 1607 128.3\n",
            "\n",
            "07:07:32 | time:3s total_exs:350 total_steps:350 epochs:38.89\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs    gnorm  gpu_mem  llen     loss  lr  ltpb  ltps  ltrunc  ltrunclen  \\\n",
            "    8.58     0  8.58  1101       0          0 128.3   50 .0002169   .01428  3.42 1.16e-06   1  3.42 438.8       0          0   \n",
            "    ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "      1          1         1                  350   12 1540 128.4\n",
            "\n",
            "07:07:32 | time:3s total_exs:400 total_steps:400 epochs:44.44\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs    gnorm  gpu_mem  llen      loss  lr  ltpb  ltps  ltrunc  ltrunclen  \\\n",
            "    8.78     0  8.78  1129       0          0 128.6   50 .0001716   .01428   3.5 9.571e-07   1   3.5 450.1       0          0   \n",
            "    ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "      1          1         1                  400 12.28 1579 128.7\n",
            "\n",
            "07:07:33 | time:4s total_exs:450 total_steps:450 epochs:50.00\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs    gnorm  gpu_mem  llen      loss  lr  ltpb  ltps  ltrunc  ltrunclen  \\\n",
            "    8.78     0  8.78  1099       0          0 125.1   50 .0001773   .01429   3.4 8.857e-07   1   3.4 425.4       0          0   \n",
            "    ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "      1          1         1                  450 12.18 1524 125.2\n",
            "\n",
            "07:07:33 | time:4s total_exs:500 total_steps:500 epochs:55.56\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs    gnorm  gpu_mem  llen      loss  lr  ltpb  ltps  ltrunc  ltrunclen  \\\n",
            "    8.72     0  8.72  1107       0          0 126.9   50 .0001302   .01428  3.54 7.321e-07   1  3.54 449.2       0          0   \n",
            "    ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "      1          1         1                  500 12.26 1556  127\n",
            "\n",
            "07:07:33 | time:5s total_exs:550 total_steps:550 epochs:61.11\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs    gnorm  gpu_mem  llen      loss  lr  ltpb  ltps  ltrunc  ltrunclen  \\\n",
            "     8.7     0   8.7  1128       0          0 129.6   50 .0001346   .01429  3.44 6.958e-07   1  3.44 445.9       0          0   \n",
            "    ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "      1          1         1                  550 12.14 1574 129.7\n",
            "\n",
            "07:07:34 | time:5s total_exs:600 total_steps:600 epochs:66.67\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs    gnorm  gpu_mem  llen      loss  lr  ltpb  ltps  ltrunc  ltrunclen  \\\n",
            "    8.78     0  8.78  1131       0          0 128.8   50 .0001283   .01428  3.42 6.288e-07   1  3.42 440.6       0          0   \n",
            "    ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "      1          1         1                  600 12.2 1572 128.9\n",
            "\n",
            "07:07:34 | time:5s total_exs:650 total_steps:650 epochs:72.22\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs    gnorm  gpu_mem  llen      loss  lr  ltpb  ltps  ltrunc  ltrunclen  \\\n",
            "    8.54     0  8.54  1108       0          0 129.8   50 .0001284   .01429  3.38 6.906e-07   1  3.38 438.7       0          0   \n",
            "    ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "      1          1         1                  650 11.92 1547 129.9\n",
            "\n",
            "07:07:35 | time:6s total_exs:700 total_steps:700 epochs:77.78\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs     gnorm  gpu_mem  llen      loss  lr  ltpb  ltps  ltrunc  \\\n",
            "    8.92     0  8.92  1146       0          0 128.4   50 8.721e-05   .01429  3.52 5.134e-07   1  3.52 452.1       0   \n",
            "    ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "            0    1          1         1                  700 12.44 1598 128.6\n",
            "\n",
            "07:07:35 | time:6s total_exs:750 total_steps:750 epochs:83.33\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs    gnorm  gpu_mem  llen      loss  lr  ltpb  ltps  ltrunc  ltrunclen  \\\n",
            "    8.86     0  8.86  1135       0          0 128.1   50 .0001269   .01428  3.26 6.136e-07   1  3.26 417.7       0          0   \n",
            "    ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "      1          1         1                  750 12.12 1553 128.2\n",
            "\n",
            "07:07:35 | time:7s total_exs:800 total_steps:800 epochs:88.89\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs     gnorm  gpu_mem  llen      loss  lr  ltpb  ltps  ltrunc  \\\n",
            "    8.56     0  8.56  1112       0          0 129.9   50 9.518e-05   .01429   3.4 5.189e-07   1   3.4 441.6       0   \n",
            "    ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "            0    1          1         1                  800 11.96 1554  130\n",
            "\n",
            "07:07:36 | time:7s total_exs:850 total_steps:850 epochs:94.44\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs    gnorm  gpu_mem  llen      loss  lr  ltpb  ltps  ltrunc  ltrunclen  \\\n",
            "    8.62     0  8.62  1101       0          0 127.7   50 7.32e-05   .01428  3.46 3.983e-07   1  3.46 441.9       0          0   \n",
            "    ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "      1          1         1                  850 12.08 1543 127.8\n",
            "\n",
            "07:07:36 | time:7s total_exs:900 total_steps:900 epochs:100.00\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs     gnorm  gpu_mem  llen      loss  lr  ltpb  ltps  ltrunc  \\\n",
            "    8.52     0  8.52  1115       0          0 130.9   50 7.708e-05   .01428  3.34 3.541e-07   1  3.34 437.2       0   \n",
            "    ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "            0    1          1         1                  900 11.86 1552  131\n",
            "\n",
            "07:07:37 | time:8s total_exs:950 total_steps:950 epochs:105.56\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs     gnorm  gpu_mem  llen      loss  lr  ltpb  ltps  ltrunc  \\\n",
            "     8.3     0   8.3  1061       0          0 127.8   50 7.233e-05   .01428  3.32 3.655e-07   1  3.32 424.3       0   \n",
            "    ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "            0    1          1         1                  950 11.62 1485 127.9\n",
            "\n",
            "07:07:37 | time:8s total_exs:1000 total_steps:1000 epochs:111.11\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs     gnorm  gpu_mem  llen      loss  lr  ltpb  ltps  ltrunc  \\\n",
            "    8.62     0  8.62  1114       0          0 129.2   50 6.877e-05   .01428  3.48 3.809e-07   1  3.48 449.6       0   \n",
            "    ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "            0    1          1         1                 1000 12.1 1563 129.3\n",
            "\n",
            "07:07:38 | time:9s total_exs:1050 total_steps:1050 epochs:116.67\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs     gnorm  gpu_mem  llen      loss  lr  ltpb  ltps  ltrunc  \\\n",
            "    8.98     0  8.98  1148       0          0 127.8   50 7.936e-05   .01429   3.4 4.341e-07   1   3.4 434.6       0   \n",
            "    ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "            0    1          1         1                 1050 12.38 1583 127.9\n",
            "\n",
            "07:07:38 | time:9s total_exs:1100 total_steps:1100 epochs:122.22\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs     gnorm  gpu_mem  llen      loss  lr  ltpb  ltps  ltrunc  \\\n",
            "     8.7     0   8.7  1114       0          0   128   50 8.029e-05   .01428  3.32 4.359e-07   1  3.32 425.1       0   \n",
            "    ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "            0    1          1         1                 1100 12.02 1539 128.2\n",
            "\n",
            "07:07:38 | time:9s total_exs:1150 total_steps:1150 epochs:127.78\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs     gnorm  gpu_mem  llen      loss  lr  ltpb  ltps  ltrunc  \\\n",
            "     8.8     0   8.8  1132       0          0 128.6   50 5.787e-05   .01429   3.5 3.277e-07   1   3.5 450.3       0   \n",
            "    ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
            "            0    1          1         1                 1150 12.3 1583 128.7\n",
            "\n",
            "07:07:39 | time:10s total_exs:1200 total_steps:1200 epochs:133.33\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs     gnorm  gpu_mem  llen      loss  lr  ltpb  ltps  ltrunc  \\\n",
            "    8.76     0  8.76  1132       0          0 129.2   50 6.125e-05   .01429  3.46 3.094e-07   1  3.46   447       0   \n",
            "    ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "            0    1          1         1                 1200 12.22 1579 129.3\n",
            "\n",
            "07:07:39 | time:10s total_exs:1230 total_steps:1230 epochs:136.67\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs     gnorm  gpu_mem  llen      loss  lr  ltpb  ltps  ltrunc  \\\n",
            "     8.6     0   8.6  1075       0          0   125   30 5.581e-05   .01429 3.467 2.969e-07   1 3.467 433.4       0   \n",
            "    ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "            0    1          1         1                 1230 12.07 1509 125.2\n",
            "\n",
            "07:07:39 | creating task(s): my_teacher\n",
            " ~~ Loading from valid.txt ~~ \n",
            "07:07:39 | running eval: valid\n",
            "07:07:39 | eval completed in 0.10s\n",
            "07:07:39 | \u001b[1mvalid:\n",
            "    accuracy  bleu-4  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  gen_n_toks  gpu_mem  llen      loss  lr  ltpb  ltps  \\\n",
            "           1   .1113 8.667 8.667 885.7       0          0 102.1    9   1       4.444  .009438 3.444 2.999e-07   1 3.444 351.9   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps  \n",
            "         0          0    1          1         1                 1230 12.11 1238\n",
            "\u001b[0m\n",
            "07:07:39 | \u001b[1;32mnew best accuracy: 1\u001b[0m\n",
            "07:07:39 | saving best valid model: my_first_lstm/model\n",
            "07:07:39 | task solved! stopping.\n",
            "07:07:39 | Using CUDA\n",
            "07:07:39 | loading dictionary from my_first_lstm/model.dict\n",
            "07:07:39 | num words = 39\n",
            "07:07:39 | Total parameters: 16,833,536 (16,833,536 trainable)\n",
            "07:07:39 | Loading existing model params from my_first_lstm/model\n",
            "07:07:39 | creating task(s): my_teacher\n",
            " ~~ Loading from valid.txt ~~ \n",
            "07:07:39 | running eval: valid\n",
            "07:07:40 | eval completed in 0.09s\n",
            "07:07:40 | \u001b[1mvalid:\n",
            "    accuracy  bleu-4  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  gen_n_toks  gpu_mem  llen      loss  lr  ltpb  ltps  \\\n",
            "           1   .1113 8.667 8.667  1101       0          0 126.8    9   1       4.444  .005188 3.444 2.999e-07   1 3.444 437.5   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps  \n",
            "         0          0    1          1         1                 1230 12.11 1539\n",
            "\u001b[0m\n",
            "07:07:40 | creating task(s): my_teacher\n",
            " ~~ Loading from test.txt ~~ \n",
            "07:07:40 | running eval: test\n",
            "07:07:40 | eval completed in 0.09s\n",
            "07:07:40 | \u001b[1mtest:\n",
            "    accuracy  bleu-4  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  gen_n_toks  gpu_mem  llen      loss  lr  ltpb  ltps  \\\n",
            "           1   .1113 8.667 8.667 969.7       0          0 111.7    9   1       4.444  .005188 3.444 2.999e-07   1 3.444 385.2   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps  \n",
            "         0          0    1          1         1                 1230 12.11 1355\n",
            "\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'exs': SumMetric(9),\n",
              "  'accuracy': ExactMatchMetric(1),\n",
              "  'f1': F1Metric(1),\n",
              "  'bleu-4': BleuMetric(0.1113),\n",
              "  'clen': AverageMetric(8.667),\n",
              "  'ctrunc': AverageMetric(0),\n",
              "  'ctrunclen': AverageMetric(0),\n",
              "  'llen': AverageMetric(3.444),\n",
              "  'ltrunc': AverageMetric(0),\n",
              "  'ltrunclen': AverageMetric(0),\n",
              "  'loss': AverageMetric(2.999e-07),\n",
              "  'ppl': PPLMetric(1),\n",
              "  'token_acc': AverageMetric(1),\n",
              "  'token_em': AverageMetric(1),\n",
              "  'gen_n_toks': AverageMetric(4.444),\n",
              "  'exps': GlobalTimerMetric(126.8),\n",
              "  'ltpb': GlobalAverageMetric(3.444),\n",
              "  'ltps': GlobalTimerMetric(437.5),\n",
              "  'ctpb': GlobalAverageMetric(8.667),\n",
              "  'ctps': GlobalTimerMetric(1101),\n",
              "  'tpb': GlobalAverageMetric(12.11),\n",
              "  'tps': GlobalTimerMetric(1539),\n",
              "  'lr': GlobalAverageMetric(1),\n",
              "  'gpu_mem': GlobalAverageMetric(0.005188),\n",
              "  'total_train_updates': GlobalFixedMetric(1230)},\n",
              " {'exs': SumMetric(9),\n",
              "  'accuracy': ExactMatchMetric(1),\n",
              "  'f1': F1Metric(1),\n",
              "  'bleu-4': BleuMetric(0.1113),\n",
              "  'clen': AverageMetric(8.667),\n",
              "  'ctrunc': AverageMetric(0),\n",
              "  'ctrunclen': AverageMetric(0),\n",
              "  'llen': AverageMetric(3.444),\n",
              "  'ltrunc': AverageMetric(0),\n",
              "  'ltrunclen': AverageMetric(0),\n",
              "  'loss': AverageMetric(2.999e-07),\n",
              "  'ppl': PPLMetric(1),\n",
              "  'token_acc': AverageMetric(1),\n",
              "  'token_em': AverageMetric(1),\n",
              "  'gen_n_toks': AverageMetric(4.444),\n",
              "  'exps': GlobalTimerMetric(111.7),\n",
              "  'ltpb': GlobalAverageMetric(3.444),\n",
              "  'ltps': GlobalTimerMetric(385.2),\n",
              "  'ctpb': GlobalAverageMetric(8.667),\n",
              "  'ctps': GlobalTimerMetric(969.7),\n",
              "  'tpb': GlobalAverageMetric(12.11),\n",
              "  'tps': GlobalTimerMetric(1355),\n",
              "  'lr': GlobalAverageMetric(1),\n",
              "  'gpu_mem': GlobalAverageMetric(0.005188),\n",
              "  'total_train_updates': GlobalFixedMetric(1230)})"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hHrruVd1KnK"
      },
      "source": [
        "어떻게 작동하는지 살펴봅시다. 데이터가 적으므로 아마 데이터를 완벽하게 외웠을 것이라고 생각됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shqFpdrE1Iif",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f33b0425-0764-4d0b-de2d-2e316bbb6a25"
      },
      "source": [
        "DisplayModel.main(model_file='my_first_lstm/model', task='my_teacher')"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "07:07:46 | Using CUDA\n",
            "07:07:46 | loading dictionary from my_first_lstm/model.dict\n",
            "07:07:46 | num words = 39\n",
            "07:07:47 | Total parameters: 16,833,536 (16,833,536 trainable)\n",
            "07:07:47 | Loading existing model params from my_first_lstm/model\n",
            "07:07:47 | creating task(s): my_teacher\n",
            " ~~ Loading from valid.txt ~~ \n",
            "07:07:47 | Opt:\n",
            "07:07:47 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "07:07:47 |     adam_eps: 1e-08\n",
            "07:07:47 |     add_p1_after_newln: False\n",
            "07:07:47 |     aggregate_micro: False\n",
            "07:07:47 |     allow_missing_init_opts: False\n",
            "07:07:47 |     batchsize: 1\n",
            "07:07:47 |     beam_block_full_context: True\n",
            "07:07:47 |     beam_block_list_filename: None\n",
            "07:07:47 |     beam_block_ngram: -1\n",
            "07:07:47 |     beam_context_block_ngram: -1\n",
            "07:07:47 |     beam_delay: 30\n",
            "07:07:47 |     beam_length_penalty: 0.65\n",
            "07:07:47 |     beam_min_length: 1\n",
            "07:07:47 |     beam_size: 1\n",
            "07:07:47 |     betas: '[0.9, 0.999]'\n",
            "07:07:47 |     bpe_add_prefix_space: None\n",
            "07:07:47 |     bpe_debug: False\n",
            "07:07:47 |     bpe_dropout: None\n",
            "07:07:47 |     bpe_merge: None\n",
            "07:07:47 |     bpe_vocab: None\n",
            "07:07:47 |     compute_tokenized_bleu: False\n",
            "07:07:47 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
            "07:07:47 |     datatype: train\n",
            "07:07:47 |     delimiter: '\\n'\n",
            "07:07:47 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "07:07:47 |     dict_endtoken: __end__\n",
            "07:07:47 |     dict_file: my_first_lstm/model.dict\n",
            "07:07:47 |     dict_include_test: False\n",
            "07:07:47 |     dict_include_valid: False\n",
            "07:07:47 |     dict_initpath: None\n",
            "07:07:47 |     dict_language: english\n",
            "07:07:47 |     dict_loaded: True\n",
            "07:07:47 |     dict_lower: False\n",
            "07:07:47 |     dict_max_ngram_size: -1\n",
            "07:07:47 |     dict_maxexs: -1\n",
            "07:07:47 |     dict_maxtokens: -1\n",
            "07:07:47 |     dict_minfreq: 0\n",
            "07:07:47 |     dict_nulltoken: __null__\n",
            "07:07:47 |     dict_starttoken: __start__\n",
            "07:07:47 |     dict_textfields: text,labels\n",
            "07:07:47 |     dict_tokenizer: re\n",
            "07:07:47 |     dict_unktoken: __unk__\n",
            "07:07:47 |     display_add_fields: \n",
            "07:07:47 |     display_examples: False\n",
            "07:07:47 |     download_path: None\n",
            "07:07:47 |     dynamic_batching: None\n",
            "07:07:47 |     embedding_projection: random\n",
            "07:07:47 |     embedding_type: random\n",
            "07:07:47 |     eval_batchsize: None\n",
            "07:07:47 |     eval_dynamic_batching: None\n",
            "07:07:47 |     evaltask: None\n",
            "07:07:47 |     final_extra_opt: \n",
            "07:07:47 |     force_fp16_tokens: False\n",
            "07:07:47 |     fp16: False\n",
            "07:07:47 |     fp16_impl: safe\n",
            "07:07:47 |     gpu: -1\n",
            "07:07:47 |     gradient_clip: 0.1\n",
            "07:07:47 |     hidden_size: 1024\n",
            "07:07:47 |     hide_labels: False\n",
            "07:07:47 |     history_add_global_end_token: None\n",
            "07:07:47 |     history_reversed: False\n",
            "07:07:47 |     history_size: -1\n",
            "07:07:47 |     image_cropsize: 224\n",
            "07:07:47 |     image_mode: raw\n",
            "07:07:47 |     image_size: 256\n",
            "07:07:47 |     inference: greedy\n",
            "07:07:47 |     init_model: None\n",
            "07:07:47 |     init_opt: None\n",
            "07:07:47 |     interactive_mode: False\n",
            "07:07:47 |     invsqrt_lr_decay_gamma: -1\n",
            "07:07:47 |     is_debug: False\n",
            "07:07:47 |     label_truncate: None\n",
            "07:07:47 |     learningrate: 1\n",
            "07:07:47 |     log_every_n_secs: -1\n",
            "07:07:47 |     log_every_n_steps: 50\n",
            "07:07:47 |     log_keep_fields: all\n",
            "07:07:47 |     loglevel: info\n",
            "07:07:47 |     lr_scheduler: reduceonplateau\n",
            "07:07:47 |     lr_scheduler_decay: 0.5\n",
            "07:07:47 |     lr_scheduler_patience: 3\n",
            "07:07:47 |     max_train_steps: -1\n",
            "07:07:47 |     max_train_time: 60.0\n",
            "07:07:47 |     metrics: default\n",
            "07:07:47 |     model: my_first_lstm\n",
            "07:07:47 |     model_file: my_first_lstm/model\n",
            "07:07:47 |     momentum: 0\n",
            "07:07:47 |     multitask_weights: [1]\n",
            "07:07:47 |     mutators: None\n",
            "07:07:47 |     nesterov: True\n",
            "07:07:47 |     no_cuda: False\n",
            "07:07:47 |     num_epochs: -1\n",
            "07:07:47 |     num_examples: 10\n",
            "07:07:47 |     num_workers: 0\n",
            "07:07:47 |     nus: [0.7]\n",
            "07:07:47 |     optimizer: sgd\n",
            "07:07:47 |     override: \"{'model_file': 'my_first_lstm/model', 'task': 'my_teacher'}\"\n",
            "07:07:47 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
            "07:07:47 |     person_tokens: False\n",
            "07:07:47 |     rank_candidates: False\n",
            "07:07:47 |     save_after_valid: False\n",
            "07:07:47 |     save_every_n_secs: -1\n",
            "07:07:47 |     save_format: conversations\n",
            "07:07:47 |     short_final_eval: False\n",
            "07:07:47 |     skip_generation: False\n",
            "07:07:47 |     special_tok_lst: None\n",
            "07:07:47 |     split_lines: False\n",
            "07:07:47 |     starttime: Jan12_07-07\n",
            "07:07:47 |     task: my_teacher\n",
            "07:07:47 |     temperature: 1.0\n",
            "07:07:47 |     tensorboard_log: False\n",
            "07:07:47 |     tensorboard_logdir: None\n",
            "07:07:47 |     text_truncate: None\n",
            "07:07:47 |     topk: 10\n",
            "07:07:47 |     topp: 0.9\n",
            "07:07:47 |     truncate: -1\n",
            "07:07:47 |     update_freq: 1\n",
            "07:07:47 |     use_reply: label\n",
            "07:07:47 |     validation_cutoff: 1.0\n",
            "07:07:47 |     validation_every_n_epochs: -1\n",
            "07:07:47 |     validation_every_n_secs: 10.0\n",
            "07:07:47 |     validation_every_n_steps: -1\n",
            "07:07:47 |     validation_max_exs: -1\n",
            "07:07:47 |     validation_metric: accuracy\n",
            "07:07:47 |     validation_metric_mode: None\n",
            "07:07:47 |     validation_patience: 10\n",
            "07:07:47 |     validation_share_agent: False\n",
            "07:07:47 |     verbose: False\n",
            "07:07:47 |     wandb_entity: None\n",
            "07:07:47 |     wandb_log: False\n",
            "07:07:47 |     wandb_name: None\n",
            "07:07:47 |     wandb_project: None\n",
            "07:07:47 |     warmup_rate: 0.0001\n",
            "07:07:47 |     warmup_updates: -1\n",
            "07:07:47 |     weight_decay: None\n",
            "07:07:47 |     world_logs: \n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
            "\u001b[0mHello\u001b[0;0m\n",
            "\u001b[1;94m    labels: Hi\u001b[0;0m\n",
            "\u001b[0;95m     model: Hi\u001b[0;0m\n",
            "\u001b[0mHow are you\u001b[0;0m\n",
            "\u001b[1;94m    labels: I am fine\u001b[0;0m\n",
            "\u001b[0;95m     model: I am fine\u001b[0;0m\n",
            "\u001b[0mLet's say goodbye\u001b[0;0m\n",
            "\u001b[1;94m    labels: Goodbye!\u001b[0;0m\n",
            "\u001b[0;95m     model: Goodbye !\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
            "\u001b[0mHey\u001b[0;0m\n",
            "\u001b[1;94m    labels: hi there\u001b[0;0m\n",
            "\u001b[0;95m     model: hi there\u001b[0;0m\n",
            "\u001b[0mDeja vu?\u001b[0;0m\n",
            "\u001b[1;94m    labels: Deja vu!\u001b[0;0m\n",
            "\u001b[0;95m     model: Deja vu !\u001b[0;0m\n",
            "\u001b[0mLast chance\u001b[0;0m\n",
            "\u001b[1;94m    labels: This is it\u001b[0;0m\n",
            "\u001b[0;95m     model: This is it\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
            "\u001b[0mHello\u001b[0;0m\n",
            "\u001b[1;94m    labels: Hi\u001b[0;0m\n",
            "\u001b[0;95m     model: Hi\u001b[0;0m\n",
            "\u001b[0mWhere are you now?\u001b[0;0m\n",
            "\u001b[1;94m    labels: I'm in yonsei unversity\u001b[0;0m\n",
            "\u001b[0;95m     model: I ' m in yonsei unversity\u001b[0;0m\n",
            "\u001b[0mOkay bye\u001b[0;0m\n",
            "\u001b[1;94m    labels: Bye\u001b[0;0m\n",
            "\u001b[0;95m     model: Bye\u001b[0;0m\n",
            "07:07:48 | epoch done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yd8sFoIrzl3F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}